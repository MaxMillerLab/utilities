{
  "collection_time": "2025-08-01T12:57:19.344670+00:00",
  "organization": "MaxMillerLab",
  "projects": [
    {
      "closed": false,
      "fields": {
        "totalCount": 13
      },
      "id": "PVT_kwHOAkGERM4A4Ygb",
      "items": {
        "totalCount": 2
      },
      "number": 14,
      "owner": {
        "login": "MaxMillerLab",
        "type": "User"
      },
      "public": false,
      "readme": "",
      "shortDescription": "",
      "title": "Census",
      "url": "https://github.com/users/MaxMillerLab/projects/14"
    },
    {
      "closed": false,
      "fields": {
        "totalCount": 15
      },
      "id": "PVT_kwHOAkGERM4A4SLl",
      "items": {
        "totalCount": 1
      },
      "number": 13,
      "owner": {
        "login": "MaxMillerLab",
        "type": "User"
      },
      "public": false,
      "readme": "",
      "shortDescription": "",
      "title": "Lab Manual Wiki",
      "url": "https://github.com/users/MaxMillerLab/projects/13"
    },
    {
      "closed": false,
      "fields": {
        "totalCount": 15
      },
      "id": "PVT_kwHOAkGERM4A3Q10",
      "items": {
        "totalCount": 7
      },
      "number": 12,
      "owner": {
        "login": "MaxMillerLab",
        "type": "User"
      },
      "public": false,
      "readme": "",
      "shortDescription": "",
      "title": "Propaganda",
      "url": "https://github.com/users/MaxMillerLab/projects/12"
    },
    {
      "closed": false,
      "fields": {
        "totalCount": 16
      },
      "id": "PVT_kwHOAkGERM4Ajyz0",
      "items": {
        "totalCount": 39
      },
      "number": 11,
      "owner": {
        "login": "MaxMillerLab",
        "type": "User"
      },
      "public": false,
      "readme": "",
      "shortDescription": "",
      "title": "Bill Probability and Impact",
      "url": "https://github.com/users/MaxMillerLab/projects/11"
    },
    {
      "closed": false,
      "fields": {
        "totalCount": 16
      },
      "id": "PVT_kwHOAkGERM4AjyzJ",
      "items": {
        "totalCount": 30
      },
      "number": 10,
      "owner": {
        "login": "MaxMillerLab",
        "type": "User"
      },
      "public": false,
      "readme": "",
      "shortDescription": "",
      "title": "Health",
      "url": "https://github.com/users/MaxMillerLab/projects/10"
    },
    {
      "closed": false,
      "fields": {
        "totalCount": 16
      },
      "id": "PVT_kwHOAkGERM4AjyvT",
      "items": {
        "totalCount": 6
      },
      "number": 9,
      "owner": {
        "login": "MaxMillerLab",
        "type": "User"
      },
      "public": false,
      "readme": "",
      "shortDescription": "",
      "title": "Inventory",
      "url": "https://github.com/users/MaxMillerLab/projects/9"
    },
    {
      "closed": false,
      "fields": {
        "totalCount": 16
      },
      "id": "PVT_kwHOAkGERM4AjyuX",
      "items": {
        "totalCount": 28
      },
      "number": 8,
      "owner": {
        "login": "MaxMillerLab",
        "type": "User"
      },
      "public": false,
      "readme": "",
      "shortDescription": "",
      "title": "Colonial Hedge",
      "url": "https://github.com/users/MaxMillerLab/projects/8"
    },
    {
      "closed": false,
      "fields": {
        "totalCount": 16
      },
      "id": "PVT_kwHOAkGERM4Ajyqj",
      "items": {
        "totalCount": 12
      },
      "number": 7,
      "owner": {
        "login": "MaxMillerLab",
        "type": "User"
      },
      "public": false,
      "readme": "",
      "shortDescription": "",
      "title": "Who values democracy?",
      "url": "https://github.com/users/MaxMillerLab/projects/7"
    },
    {
      "closed": false,
      "fields": {
        "totalCount": 15
      },
      "id": "PVT_kwHOAkGERM4Ajyjr",
      "items": {
        "totalCount": 7
      },
      "number": 6,
      "owner": {
        "login": "MaxMillerLab",
        "type": "User"
      },
      "public": false,
      "readme": "",
      "shortDescription": "",
      "title": "Social Security II",
      "url": "https://github.com/users/MaxMillerLab/projects/6"
    },
    {
      "closed": false,
      "fields": {
        "totalCount": 15
      },
      "id": "PVT_kwHOAkGERM4AjyjE",
      "items": {
        "totalCount": 8
      },
      "number": 5,
      "owner": {
        "login": "MaxMillerLab",
        "type": "User"
      },
      "public": false,
      "readme": "",
      "shortDescription": "",
      "title": "Social Security and Inequality",
      "url": "https://github.com/users/MaxMillerLab/projects/5"
    },
    {
      "closed": false,
      "fields": {
        "totalCount": 16
      },
      "id": "PVT_kwHOAkGERM4Ajw1d",
      "items": {
        "totalCount": 27
      },
      "number": 4,
      "owner": {
        "login": "MaxMillerLab",
        "type": "User"
      },
      "public": false,
      "readme": "",
      "shortDescription": "",
      "title": "PEPs",
      "url": "https://github.com/users/MaxMillerLab/projects/4"
    },
    {
      "closed": false,
      "fields": {
        "totalCount": 15
      },
      "id": "PVT_kwHOAkGERM4Ajwvr",
      "items": {
        "totalCount": 95
      },
      "number": 3,
      "owner": {
        "login": "MaxMillerLab",
        "type": "User"
      },
      "public": false,
      "readme": "",
      "shortDescription": "This project measures country risk in a new way. First, it uses investment bank analyst reports to create a text-based measure of country risk. Second, it uses an experimental survey to elicit perceptions and preferences over risk.",
      "title": "Emerging markets risk",
      "url": "https://github.com/users/MaxMillerLab/projects/3"
    },
    {
      "closed": false,
      "fields": {
        "totalCount": 14
      },
      "id": "PVT_kwHOAkGERM4AjpRh",
      "items": {
        "totalCount": 116
      },
      "number": 2,
      "owner": {
        "login": "MaxMillerLab",
        "type": "User"
      },
      "public": false,
      "readme": "",
      "shortDescription": "Project page for the foreign influence repository.",
      "title": "Foreign Influence",
      "url": "https://github.com/users/MaxMillerLab/projects/2"
    }
  ],
  "project_items": {
    "14": {
      "title": "Census",
      "items": [
        {
          "content": {
            "body": "@spkim1228 can you please email Shital to check on the status of your badge?",
            "number": 1,
            "repository": "MaxMillerLab/census",
            "title": "Get badge for Paul",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/census/issues/1"
          },
          "id": "PVTI_lAHOAkGERM4A4YgbzgaEbAY",
          "repository": "https://github.com/MaxMillerLab/census",
          "status": "Done",
          "title": "Get badge for Paul"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "## Census documentation\n\nThere are three main goals:\n- [ ] Create list of and high level description of various datasets we have available to us in the project\n- [ ] Create detailed description of variable available in each dataset\n- [x] Understand how well census compute resources work\n\n### Dataset description\n\nThis should go through and discuss the various datasets we have, what frequency they are available at, and broadly what information they have in them. Keep a few things in mind:\n1. Our goal is to construct a firm-county-level measure of investment and profits. You should be looking for various measures that might allow up to do this.\n2. We allow want to document trends in concentration (think Herfindahl\u2013Hirschman index style measures). Think through various series that might be interesting to document this on\n3. We want to understand how to merge the data with NETS/Orbis/Contracts data. Think through identifying information that might allow for this\n\n### Variable description\n\nHere, we want:\n- Understanding of what variables are present\n- How good is there coverage (e.g. available all years, some years, basic summary stats, etc)\n- Other information you think will be helpful\n\n### Compute resources\n\n- Begin to understand what resources we have for working with large data.\n- How fast does the code run?",
            "number": 2,
            "repository": "MaxMillerLab/census",
            "title": "Create documentation for census data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/census/issues/2"
          },
          "id": "PVTI_lAHOAkGERM4A4Ygbzga-OKc",
          "repository": "https://github.com/MaxMillerLab/census",
          "start date": "2025-06-02",
          "status": "In Progress",
          "target completion date": "2025-08-05",
          "title": "Create documentation for census data"
        }
      ]
    },
    "13": {
      "title": "Lab Manual Wiki",
      "items": [
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "These are the top priority files to update. This is the MVP for bringing the wiki online.\n- [x] AI-guide\n- [x] Analyst,-not-a-coder-and-common-challenges\n- [x] Code-style-guide\n- [x] Github-guide\n- [x] Goals,-Job-Description,-Rhythms,-and-Professional-Dev't\n- [x] IT-platform-guide\n- [x] IT-setup-guide\n- [x] Memo-writing-guide\n- [x] HBS-Grid-Guide",
            "number": 1,
            "repository": "MaxMillerLab/lab_manual",
            "title": "Update top priority files for lab wiki",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/lab_manual/issues/1"
          },
          "id": "PVTI_lAHOAkGERM4A4SLlzgaBQZ0",
          "priority": "P0",
          "repository": "https://github.com/MaxMillerLab/lab_manual",
          "status": "Done",
          "title": "Update top priority files for lab wiki"
        }
      ]
    },
    "12": {
      "title": "Propaganda",
      "items": [
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "Currently, the training data for the NER algorithm we are training is messed up because I am randomly feeding in entities from a previous NER run. Instead, we need to randomize on lines and examine all NER output.",
            "number": 4,
            "repository": "MaxMillerLab/propaganda",
            "title": "Feed full lines into GPT for labelling media meetings",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/propaganda/issues/4"
          },
          "id": "PVTI_lAHOAkGERM4A3Q10zgZ0T6Y",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/propaganda",
          "status": "Done",
          "title": "Feed full lines into GPT for labelling media meetings"
        },
        {
          "assignees": [
            "emilysilcock"
          ],
          "content": {
            "body": "@emilysilcock I've run the basic NER on the data and will use GPT to extract media mentions on some random sample. After that, maybe we can take a crack at training an NER based on some sample we verify as correct?",
            "number": 2,
            "repository": "MaxMillerLab/fara",
            "title": "Train NER for media detection",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/fara/issues/2"
          },
          "id": "PVTI_lAHOAkGERM4A3Q10zgZ0T6c",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/fara",
          "status": "Done",
          "title": "Train NER for media detection"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "@emilysilcock I was considering using [something like this](https://www.expectedparrot.com/content/johnjhorton/grant-letter) for a first pass at some of the disclosed materials. I figure we could try it and see if it (1) works and (2) is more cost effective than paying people. Before I coded it up, I wanted to see if you have experience with this... I know some people might view the Horton stuff with suspicion.",
            "number": 1,
            "repository": "MaxMillerLab/propaganda",
            "title": "Use LLM to transcribe provided materials?",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/propaganda/issues/1"
          },
          "id": "PVTI_lAHOAkGERM4A3Q10zgZ0T6k",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/propaganda",
          "status": "Done",
          "title": "Use LLM to transcribe provided materials?"
        },
        {
          "content": {
            "body": "@emilysilcock I just had the thought that there are two kinds of news we should be concerned about. One is news about the country. We'd like to see this slant towards more positive sentiment after contact. However, there is also news about, say, dissidents within the county. For many of our countries this will be very important. Ideally, we'd see this coverage become more negative. Just wanted to have this noted here as we make progress.",
            "number": 3,
            "repository": "MaxMillerLab/propaganda",
            "title": "Separate \"negative\" and \"positive\" news",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/propaganda/issues/3"
          },
          "id": "PVTI_lAHOAkGERM4A3Q10zgZ0T6s",
          "repository": "https://github.com/MaxMillerLab/propaganda",
          "status": "Done",
          "title": "Separate \"negative\" and \"positive\" news"
        },
        {
          "content": {
            "body": "I just had the thought that there are two kinds of news we should be concerned about. One is news about the country. We'd like to see this slant towards more positive sentiment after contact. However, there is also news about, say, dissidents within the county. For many of our countries, this will be very important. Ideally, we'd see this coverage become more negative. Just wanted to have this noted here as we make progress.\n\n",
            "id": "DI_lAHOAkGERM4A3Q10zgJaeOQ",
            "title": "Separate \"negative\" and \"positive\" news",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4A3Q10zgZ0UGE",
          "status": "Ideas",
          "title": "Separate \"negative\" and \"positive\" news"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "@MaxMillerLab  First pass at the substantive D^3 questions under docs/funding_apps (though let me know if you think this makes more sense somewhere else) \n\nQ9 is very GPT-heavy, so I will take another look through that one, but I'm about to break for the day and wanted to pass it across to you. \n\nQ10 is very much overkill and I'm definitely not suggesting that we actually do this \ud83d\ude0a\n",
            "number": 6,
            "repository": "MaxMillerLab/propaganda",
            "title": "D^3 applicatiom",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/propaganda/issues/6"
          },
          "id": "PVTI_lAHOAkGERM4A3Q10zgbri1Y",
          "repository": "https://github.com/MaxMillerLab/propaganda",
          "status": "Done",
          "title": "D^3 applicatiom"
        },
        {
          "assignees": [
            "emilysilcock"
          ],
          "content": {
            "body": "Starting the parsing of newspaper data on ProQuest. There's a lot of bit of cleaning and parsing that take a bit of time so I've made myself a tracker below so I know where I've got to \n\nFor dates, I've gone from the earliest available in the modern ProQuest dataset (though there are earlier dates from historical ProQuest) and I've gone up until the end of 2023 for now \n\n(Minor github incompetence below \ud83e\udd26\u200d\u2640\ufe0f)\n\nNewspaper | ProQuest Coverage | HTML parsed | Cleaned | Country names search | Sentiment analysis \n-- | -- | -- | -- | -- | -- \nNY Daily News | 3/1/1995+ | \u2705 | \u2705 | \u2705 | \nUSA Today | 2/17/1997+ | \u2705 | \u2705 | \u2705 | \nWSJ | 1/2/1984+ | \u2705 | \u2705 | \u2705 |  \nNY Times | 6/1/1980-8/25/2021 | \u2705 | \u2705 | \u2705 |  \nLA Times | 1/1/1985+ | \u2705 | \u2705 | \u2705 | \nChicago Tribune | 1/1/1985+ | \u2705 | \u2705 | \u2705 | \nWashington Post | 1/1/1987+ | \u2705 | \u2705 | \u2705 | \nNY Post | No ProQuest, but I have this scraped from Factiva 1998-2023 | \u2705 | \u2705 |  | \nPhiladelphia Inquirer | 1/1/1983+ | \u2705 | \u2705 | \u2705 |  | \nDetroit Free Press | 1/1/1999+ | \u2705 | \u2705 | \u2705 |  | \nThe Boston Globe | 1/1/1997+ | \u2705 | \u2705 | \u2705 |  | \nMinnesota Star Tribune | 1/1/1986+ | \u2705 | \u2705 | \u2705 |  | \n",
            "number": 5,
            "repository": "MaxMillerLab/propaganda",
            "title": "Newspaper country mentions and sentiment on ProQuest",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/propaganda/issues/5"
          },
          "id": "PVTI_lAHOAkGERM4A3Q10zgc1CdM",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/propaganda",
          "status": "In progress",
          "title": "Newspaper country mentions and sentiment on ProQuest"
        }
      ]
    },
    "11": {
      "title": "Bill Probability and Impact",
      "items": [
        {
          "assignees": [
            "farnikn",
            "rifaki"
          ],
          "category": "Probability",
          "content": {
            "body": "@rifaki I would like you to take @tanarcsin's neural network code, refine it, and use it to produce an initial measure of probability of passage. The steps required to do this are:\n\n- [x] Check the data cleaning done by @tanarcsin and assure the data are structured appropriately;\n- [x] Check that the neural net is coded efficiently and uses only prior information (e.g. no information leakage);\n- [x] Check how well our estimated probabilities do. Ideally, we would like to compare realized probabilities of passage to our estimates.\n\nWhat do you think is a realistic timeline for you to complete this in? Would end of the month be too aggressive?",
            "number": 12,
            "repository": "MaxMillerLab/bills",
            "title": "Predict probability of passage using neural network",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/12"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgQXHPU",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/bills",
          "status": "Done",
          "title": "Predict probability of passage using neural network"
        },
        {
          "assignees": [
            "tanarcsin",
            "calebeynon"
          ],
          "category": "Impact",
          "content": {
            "body": "Using stock and option prices, we should regress the change in bill probability of passage on a few different stock return portfolios.\n1. Aggregate market;\n2. Industry portfolios;\n3. Individual returns\n\nWe should experiment with various windows. We should also test overreaction and underreaction to large or small probabilities.",
            "number": 19,
            "repository": "MaxMillerLab/bills",
            "title": "Create exposure measure",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/19"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgQXHUA",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/bills",
          "start date": "2025-05-19",
          "status": "Done",
          "target completion date": "2025-07-16",
          "title": "Create exposure measure"
        },
        {
          "category": "Impact",
          "content": {
            "body": "",
            "id": "DI_lAHOAkGERM4Ajyz0zgGxQB8",
            "title": "Use GPT to \"sign\" bills",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgQXHUs",
          "status": "Backlog",
          "title": "Use GPT to \"sign\" bills"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "tanarcsin",
            "rifaki"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "@rifaki I was hoping you could go through the relevant literature and see what is out there. I think that there are three main streams we are concerned with:\n1. Estimating the likelihood that bills will pass (this will mostly be a literature in political science);\n2. Estimating the policy and political risk exposure of countries and firms (there is a vibrant literature here, but it generally creates aggregate characterizations of political or policy risk);\n3. Using options markets to measure policy risk exposure.\n\nThere are a number of papers in each of these. For (2) and (3) we should focus on the \"top 5\" economics journals, \"top 3\" finance journals, and papers with a large number of cites. For (1), I am open to all journals. This could be a rolling search, but lets target the end of January for having all relevant research uncovered.",
            "number": 13,
            "repository": "MaxMillerLab/bills",
            "title": "Literature review",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/13"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgVR4Go",
          "repository": "https://github.com/MaxMillerLab/bills",
          "status": "Done",
          "title": "Literature review"
        },
        {
          "assignees": [
            "tanarcsin",
            "rifaki"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "@tanarcsin @MaxMillerLab If we want to assess the potential for temporal leakage in our modeling framework, we need clarity on when sponsorship and cosponsorship fields are updated or finalized. Current assumption is that cosponsor data may evolve post-introduction (An example might be: bipartisan status can emerge mid-process).",
            "number": 16,
            "repository": "MaxMillerLab/bills",
            "title": "Remove lookahead bias in sponsorship and cosponsorship data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/16"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgZyOXQ",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/bills",
          "status": "Done",
          "title": "Remove lookahead bias in sponsorship and cosponsorship data"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "tanarcsin"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "@tanarcsin can you write some code that scrapes the PDFs of the congressional reports from congress.gov?",
            "number": 15,
            "repository": "MaxMillerLab/bills",
            "title": "Gather all congressional reports PDFs",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/15"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgZyOXU",
          "repository": "https://github.com/MaxMillerLab/bills",
          "title": "Gather all congressional reports PDFs"
        },
        {
          "assignees": [
            "tanarcsin"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "@tanarcsin can you start cleaning the options data for the project? Here is how I propose doing it:\n\n1. Set up a SQL database for the options data I downloaded (it is in the project space).\n2. Create some basic descriptive statistics (number of options trade in each year, number of different options traded for each company, check if there are foreign country index options present).\n3. After this is done, we will construct our exposure measures.",
            "number": 14,
            "repository": "MaxMillerLab/bills",
            "title": "Clean options data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/14"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgZyOXY",
          "repository": "https://github.com/MaxMillerLab/bills",
          "title": "Clean options data"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "tanarcsin"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "After new data",
            "number": 11,
            "repository": "MaxMillerLab/bills",
            "title": "Figure out how to output probabilities instead of binary classification. It will probably also involve changing the cost function",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/11"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgZyOXc",
          "repository": "https://github.com/MaxMillerLab/bills",
          "title": "Figure out how to output probabilities instead of binary classification. It will probably also involve changing the cost function"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "tanarcsin"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "After running regression as is",
            "number": 9,
            "repository": "MaxMillerLab/bills",
            "title": "Figure out any optimizations/improvements",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/9"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgZyOXg",
          "repository": "https://github.com/MaxMillerLab/bills",
          "title": "Figure out any optimizations/improvements"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "tanarcsin"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "After optimizations/imporvements",
            "number": 10,
            "repository": "MaxMillerLab/bills",
            "title": "Add more data: lobbying, etc",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/10"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgZyOXk",
          "repository": "https://github.com/MaxMillerLab/bills",
          "title": "Add more data: lobbying, etc"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "tanarcsin"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "",
            "number": 8,
            "repository": "MaxMillerLab/bills",
            "title": "Run the current regression as is for all sets of data and fix bugs",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/8"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgZyOXo",
          "repository": "https://github.com/MaxMillerLab/bills",
          "title": "Run the current regression as is for all sets of data and fix bugs"
        },
        {
          "category": "Data Cleaning",
          "content": {
            "body": "\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n\n## Summary by CodeRabbit\n\n- **New Features**\n\t- Introduced multiple scripts for processing legislative data, including converting JSON to CSV, adding dummy variables, and integrating legislator information into bills.\n\t- Implemented fuzzy matching functionalities for name comparisons and OpenAI API integration for name verification.\n\t- Added a machine learning pipeline for classifying legislative data using a neural network and partitioning data based on congressional sessions.\n\n- **Bug Fixes**\n\t- Enhanced error handling and logging capabilities across various data processing scripts.\n\n- **Documentation**\n\t- Updated documentation to reflect new functionalities and usage instructions for the introduced scripts and classes.\n\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->",
            "number": 7,
            "repository": "MaxMillerLab/bills",
            "title": "1 create probability of bill passage",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/bills/pull/7"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgZyOXs",
          "repository": "https://github.com/MaxMillerLab/bills",
          "title": "1 create probability of bill passage"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "tanarcsin"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "I checked the JSON file for that bill and the acted_at dates are actually all the same. I then checked the XML file at the website and the action_dates are also all the same. Also checking the congress website, https://www.congress.gov/bill/116th-congress/house-joint-resolution/110/all-actions, all actions for the bill happened on the same day, so I guess its just an irregular occurrence. ",
            "number": 2,
            "repository": "MaxMillerLab/bills",
            "title": "Acted_At Dates the Same",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/2"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgZyOXw",
          "repository": "https://github.com/MaxMillerLab/bills",
          "title": "Acted_At Dates the Same"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "category": "Probability",
          "content": {
            "body": "@rifaki we discussed transforming bill text into embeddings yesterday. Here is the model I think we should start with. We should be able to run everything on the grid for this. Let me know if you have questions.\n\nhttps://huggingface.co/RepresentLM/RepresentLM-v1",
            "number": 17,
            "repository": "MaxMillerLab/bills",
            "title": "Create embeddings for bills",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/17"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgZ9OrU",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/bills",
          "status": "Done",
          "title": "Create embeddings for bills"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "Examining the current state of bills text data to help with embeddings. This should help with the data fed into the neural network.",
            "number": 23,
            "repository": "MaxMillerLab/bills",
            "title": "Examine data for NN",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/23"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgbmmFI",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/bills",
          "start date": "2025-06-18",
          "status": "Done",
          "target completion date": "2025-06-30",
          "title": "Examine data for NN"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "1. Closes issue #22\r\n2. I have review the 2 modified files in the pull request and the changes are as expected\r\n3. Purpose: correctness\r\n4. 1 hour",
            "number": 25,
            "repository": "MaxMillerLab/bills",
            "title": "Issue 022 monthly returns",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/bills/pull/25"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgbo3cc",
          "repository": "https://github.com/MaxMillerLab/bills",
          "reviewers": [
            "MaxMillerLab"
          ],
          "status": "Done",
          "title": "Issue 022 monthly returns"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "content": {
            "body": "Take the python code used to generate monthly returns and fix some style errors. Additionally, use SQL instead of python to perform the operations.",
            "number": 26,
            "repository": "MaxMillerLab/bills",
            "title": "Reformat monthly returns python code",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/26"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgbpgxU",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/bills/pull/27"
          ],
          "repository": "https://github.com/MaxMillerLab/bills",
          "status": "Done",
          "title": "Reformat monthly returns python code"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "content": {
            "body": "Set up Claude code on machine and get following MCPs:\n- firecraw\n- GitHub\n- fin-dev-files\n\nuse rclone to put project files on local machine for easier integration.",
            "number": 28,
            "repository": "MaxMillerLab/bills",
            "title": "Claude code set up",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/28"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgb0N64",
          "repository": "https://github.com/MaxMillerLab/bills",
          "status": "Done",
          "title": "Claude code set up"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "Writing tests to determine the nature of the data we are pulling about bills.\n- do we track bill text as it changes?\n- do we track amendments?\n- do we track cosponsor changes?",
            "number": 29,
            "repository": "MaxMillerLab/bills",
            "title": "Testing Government Data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/29"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcBltc",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/bills/pull/31"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/bills",
          "status": "Done",
          "target completion date": "2025-06-30",
          "title": "Testing Government Data"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "category": "Probability",
          "content": {
            "body": "Step 2. Creating a neural network to output a probability of bill passage using the data pipeline. This probability will be used in creating the exposure measure.",
            "number": 24,
            "repository": "MaxMillerLab/bills",
            "title": "Coding the NN",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/24"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcD06U",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/bills",
          "start date": "2025-07-15",
          "status": "In progress",
          "target completion date": "2025-08-08",
          "title": "Coding the NN"
        },
        {
          "content": {
            "body": "1. Closes #29  \r\n2. I have review the 8 modified files in the PR and the changes are as I expected\r\n3. correctness\r\n4. 1hr\r\n5. 3 new scripts are added and then their outputs are added to the output folder. the other 2 files are the md file and then deleting the old md file\r\n6. none",
            "number": 30,
            "repository": "MaxMillerLab/bills",
            "title": "Issue 29 Testing Government Data",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/bills/pull/30"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcE9ek",
          "repository": "https://github.com/MaxMillerLab/bills",
          "reviewers": [
            "MaxMillerLab"
          ],
          "status": "Done",
          "title": "Issue 29 Testing Government Data"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "1. Closes #29  \n2. I have review the 8 modified files in the PR and the changes are as I expected\n3. correctness\n4. 1hr\n5. 3 new scripts are added and then their outputs are added to the output folder. the other 2 files are the md file and then deleting the old md file\n6. none\n\n*Note: This replaces PR #30 which was created with the wrong branch name.*",
            "number": 31,
            "repository": "MaxMillerLab/bills",
            "title": "Issue 29 Testing Government Data",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/bills/pull/31"
          },
          "estimate": 1,
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcFh0E",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/bills",
          "reviewers": [
            "MaxMillerLab"
          ],
          "status": "Done",
          "target completion date": "2025-06-30",
          "title": "Issue 29 Testing Government Data"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "- Goal: build on the dataset that will be fed into the neural network with committee data\n- Get the committee assignments from the Congress API\n- Match cosponsors of bills with their committee assignments\n",
            "number": 32,
            "repository": "MaxMillerLab/bills",
            "title": "Pull committee data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/32"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcFiis",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/bills/pull/36"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/bills",
          "status": "Done",
          "target completion date": "2025-07-09",
          "title": "Pull committee data"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "category": "Probability",
          "content": {
            "body": "- Create randomly generated change in probabilities centered at 0 and truncated at -1 and 1\n- run regressions on 1 day, 3 day, 5 day, 10 day periods with cumulative returns depending on probabilities\n- target end date July 2\n ",
            "number": 33,
            "repository": "MaxMillerLab/bills",
            "title": "Run regressions on equity returns",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/33"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcIIv4",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/bills/pull/34"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/bills",
          "start date": "2025-07-01",
          "status": "Done",
          "target completion date": "2025-07-03",
          "title": "Run regressions on equity returns"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "Preparing our input data to go into the neural network. This includes bill actions, cosponsors, sponsors, and committees. This should incorporate the changing element of bills through time. It seems that RNNs can handle inputs that are different sizes which makes it easier to have time-varying info. I will have to create a consistent sequence of features.",
            "number": 35,
            "repository": "MaxMillerLab/bills",
            "title": "Processing data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/35"
          },
          "estimate": 7,
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcUUsQ",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/bills/pull/40"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/bills",
          "start date": "2025-07-09",
          "status": "Done",
          "target completion date": "2025-07-14",
          "title": "Processing data"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "content": {
            "body": "1. Closes issue #32 \r\n2. I have reviewed the 6 modified files and the changes are as I expected.\r\n3. correctness\r\n4. 1.5hr\r\n5. All the files are used in pulling committee data and processing it. 1 of the files is the .md file\r\n6. none",
            "number": 36,
            "repository": "MaxMillerLab/bills",
            "title": "Issue 32 pull committee data",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/bills/pull/36"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcalVg",
          "repository": "https://github.com/MaxMillerLab/bills",
          "reviewers": [
            "MaxMillerLab"
          ],
          "status": "Done",
          "title": "Issue 32 pull committee data"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "In the amendment metadata we have brief purpose statements. I had Claude analyze some patterns in the text and it found some clear distinctions: Structural, Funding, Reporting, Restriction, Timing, Technical, Title Changes. \n\nCreating this as a small issue to convert amendment purposes into binary classifications in our data so it's usable in the neural network. \n\nI think I have to wait for the merge from issue #35 to finish before working with the data.\n",
            "number": 37,
            "repository": "MaxMillerLab/bills",
            "title": "Classifying amendment purposes",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/37"
          },
          "estimate": 2,
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcbHUM",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/bills/pull/39"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/bills",
          "start date": "2025-07-09",
          "status": "Done",
          "target completion date": "2025-07-11",
          "title": "Classifying amendment purposes"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "There are two specific columns in our data that need to be embedded to go through the neural network.\n- Bill Text\n- Title\nI need to ensure that there is no look ahead bias when choosing the model to do this. This issue is more-so for after the first neural network run, but I will work on it while the other data processes.",
            "number": 38,
            "repository": "MaxMillerLab/bills",
            "title": "Text Embeddings",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/38"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcjuCc",
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/bills",
          "start date": "2025-07-14",
          "status": "In progress",
          "target completion date": "2025-08-01",
          "title": "Text Embeddings"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "content": {
            "body": "1. Closes issue #37 \r\n2. I have review the 4 modified files in the pull request and the changes are as I expected\r\n3. correctness\r\n4. 15 minutes\r\n5. I have the one python script that classifies amendment purpose, and the 3 others were added to work with LLMs by you\r\n6. none",
            "number": 39,
            "repository": "MaxMillerLab/bills",
            "title": "Issue 37 classify amendments",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/bills/pull/39"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcmupU",
          "repository": "https://github.com/MaxMillerLab/bills",
          "reviewers": [
            "MaxMillerLab"
          ],
          "status": "Done",
          "title": "Issue 37 classify amendments"
        },
        {
          "content": {
            "body": "1. Closes issue #33 \r\n2. I have reviewed the 14 modified files in the pull request and the changes are as I expected\r\n3. correctness\r\n4. 2hr\r\n5. deleted two old markdown files. The main script is `probability_returns_analysis_by_bill.py` which is tested by its corresponding test file. Other files to be changed are archived versions of this or output.\r\n6. None",
            "number": 34,
            "repository": "MaxMillerLab/bills",
            "title": "Issue 33 regressions on returns",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/bills/pull/34"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcmxOM",
          "repository": "https://github.com/MaxMillerLab/bills",
          "status": "Pause",
          "title": "Issue 33 regressions on returns"
        },
        {
          "content": {
            "body": "1. Closes issue #26 \r\n2. I have reviewed the two files it changes\r\n3. correctness\r\n4. <1hr",
            "number": 27,
            "repository": "MaxMillerLab/bills",
            "title": "Issue 26 reformat returns code",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/bills/pull/27"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcmxOQ",
          "repository": "https://github.com/MaxMillerLab/bills",
          "status": "Pause",
          "title": "Issue 26 reformat returns code"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "content": {
            "body": "Producing a series of monthly stock returns for all equity securities using equity .db file.",
            "number": 22,
            "repository": "MaxMillerLab/bills",
            "title": "Monthly stock returns for equity securities",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/22"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcmxOU",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/bills/pull/25"
          ],
          "repository": "https://github.com/MaxMillerLab/bills",
          "status": "Pause",
          "title": "Monthly stock returns for equity securities"
        },
        {
          "content": {
            "body": "## Summary\n- expand each folder's README with clearer explanations of its contents and role\n- document GitHub workflows, analysis outputs, and data-processing scripts\n\n## Testing\n- `pytest -q` *(fails: No module named 'yaml')*",
            "number": 20,
            "repository": "MaxMillerLab/bills",
            "title": "Add detailed READMEs for project directories",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/bills/pull/20"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcmxOY",
          "labels": [
            "codex"
          ],
          "repository": "https://github.com/MaxMillerLab/bills",
          "status": "Pause",
          "title": "Add detailed READMEs for project directories"
        },
        {
          "content": {
            "body": "## Summary\n- trim README to provide a concise overview of repository layout and usage\n\n## Testing\n- `pytest -q` *(fails: ModuleNotFoundError: No module named 'yaml')*",
            "number": 21,
            "repository": "MaxMillerLab/bills",
            "title": "Shorten top-level README",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/bills/pull/21"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcmxOg",
          "labels": [
            "codex"
          ],
          "repository": "https://github.com/MaxMillerLab/bills",
          "status": "Pause",
          "title": "Shorten top-level README"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "content": {
            "body": "1. Closes issue #35 \r\n2. I have reviewed the 4 modified files in the pull request and the changes are as I expected\r\n3. correctness\r\n4. 1.5hr\r\n5. deleting outdated scripts. Add the main preprocessing file. \r\n6. none",
            "number": 40,
            "repository": "MaxMillerLab/bills",
            "title": "Issue 35 input data",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/bills/pull/40"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcqOqk",
          "repository": "https://github.com/MaxMillerLab/bills",
          "reviewers": [
            "MaxMillerLab"
          ],
          "status": "Done",
          "title": "Issue 35 input data"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "It would be helpful to track changes to actual bill text for predicting the probability of passage. I plan on searching through some government API resources to see if this is even possible.",
            "number": 41,
            "repository": "MaxMillerLab/bills",
            "title": "Search for proposed bill text history",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/41"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgcu4Gw",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/bills/pull/45"
          ],
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/bills",
          "start date": "2025-07-18",
          "status": "In progress",
          "target completion date": "2025-08-31",
          "title": "Search for proposed bill text history"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "category": "Impact",
          "content": {
            "body": "The previous exposure measure script was created to take randomly generated test data as input. Now that we have neural network output, I need to adjust the script to take those probabilities as input instead.",
            "number": 43,
            "repository": "MaxMillerLab/bills",
            "title": "Update exposure measure script",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/bills/issues/43"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgc8JRI",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/bills/pull/44"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/bills",
          "start date": "2025-07-24",
          "status": "Done",
          "target completion date": "2025-07-25",
          "title": "Update exposure measure script"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "content": {
            "body": "1. Closes issue #43 \r\n2. I have reviewed the 4 modified files in the pull request and the changes are as I expected\r\n3. correctness\r\n4. 1 hr\r\n5. the complete_pipeline script uses the neural network to predict probabilities then puts them in a dataframe with bill id and date. The calculate_probability_changes script just converts those predicted probabilities into running changes.\r\n6. none ",
            "number": 44,
            "repository": "MaxMillerLab/bills",
            "title": "Issue 43 update exposure script",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/bills/pull/44"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgc-rp8",
          "repository": "https://github.com/MaxMillerLab/bills",
          "reviewers": [
            "MaxMillerLab"
          ],
          "status": "Done",
          "title": "Issue 43 update exposure script"
        },
        {
          "assignees": [
            "calebeynon"
          ],
          "content": {
            "body": "1. Closes issue #41 \r\n2. I have reviewed the 3 modified files in the pull request and the changes are as I expected\r\n3. Correctness\r\n4. <1hr\r\n5. `get_govinfo_bills_list.py` exists to get a list of bills for reference by `download_bill_texts.py` which then downloads the text from the govinfo API.\r\n6. none",
            "number": 45,
            "repository": "MaxMillerLab/bills",
            "title": "Issue 41 bill text",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/bills/pull/45"
          },
          "id": "PVTI_lAHOAkGERM4Ajyz0zgdLBGE",
          "repository": "https://github.com/MaxMillerLab/bills",
          "reviewers": [
            "MaxMillerLab"
          ],
          "title": "Issue 41 bill text"
        }
      ]
    },
    "10": {
      "title": "Health",
      "items": [
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Refactoring",
          "content": {
            "body": "We would like a better mapping of SSW to households. We're using the PSID and the full simulated microdata to predict SSW. This will, importantly, let us get the correlation between SSW and marketable wealth right.",
            "id": "DI_lAHOAkGERM4AjyzJzgGxP2M",
            "title": "Revisit neural net",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgQXG4w",
          "priority": "Low",
          "status": "Backlog",
          "title": "Revisit neural net"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Refactoring",
          "content": {
            "body": "",
            "id": "DI_lAHOAkGERM4AjyzJzgGxP2Q",
            "title": "Fill in SConscript",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgQXG5A",
          "priority": "Medium",
          "status": "Ready",
          "title": "Fill in SConscript"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "sylcath",
            "siyusun1108"
          ],
          "category": "Health Benefits",
          "content": {
            "body": "This doesn't seem like the right way to do it. Can you please add some documentation about why this is happening?",
            "number": 3,
            "repository": "MaxMillerLab/health",
            "title": "Why is there a loop over years in the deaton-paxson regression code?",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/3"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgQXHGY",
          "labels": [
            "help wanted"
          ],
          "repository": "https://github.com/MaxMillerLab/health",
          "status": "Done",
          "title": "Why is there a loop over years in the deaton-paxson regression code?"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "sylcath",
            "siyusun1108"
          ],
          "category": "Health Benefits",
          "content": {
            "body": "@siyusun1108 can you update the deaton-paxson regression code to drop the cohorts that are not yet retired (which happens at 62 years of age). We think that the projection is messed up by the younger cohorts where we have no data.",
            "number": 4,
            "repository": "MaxMillerLab/health",
            "title": "Drop people that are not retired from the deaton-paxson regression",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/4"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgQXHGc",
          "labels": [
            "bug",
            "help wanted"
          ],
          "repository": "https://github.com/MaxMillerLab/health",
          "status": "Done",
          "title": "Drop people that are not retired from the deaton-paxson regression"
        },
        {
          "category": "Miscellaneous",
          "content": {
            "body": "",
            "id": "DI_lAHOAkGERM4AjyzJzgGxP8c",
            "title": "American time-use survey for value of at home activities",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgQXHJ8",
          "priority": "Low",
          "status": "Backlog",
          "title": "American time-use survey for value of at home activities"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "jjustindoesstats"
          ],
          "content": {
            "body": "@jjustindoesstats I'm going to start taking a more active role in this project. Because of that, I spent the night going through the existing codebase. Unfortunately, it's a total disaster. You and I need to chat about how to do clean-up. Can you chat tomorrow (Monday)? I would love it if we can have everything clean and ready to go by the end of the week. Let me know.\n\nClean up schedule:\n- [x] Replace `yield_curves` with `discount_rates` scripts in the code\n- [x] Replace survival probability code in `meps` with code in `mortality`\n- [ ] Make it such that `read_medicare_projections.py` runs\n- [x] Make it such that `download_all_meps_data.py`\n- [x] Create readme that describes the order in which things should be run",
            "number": 14,
            "repository": "MaxMillerLab/health",
            "title": "Refactor existing codebase",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/14"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgZx7-g",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/health",
          "status": "Done",
          "title": "Refactor existing codebase"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "jjustindoesstats"
          ],
          "content": {
            "body": "Doing the same projection that we did for Medicare but for Medicaid. Running the average Medicaid benefits by income centile-age-year. ",
            "number": 13,
            "repository": "MaxMillerLab/health",
            "title": "Creating Medicaid benefits by age-centile-year",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/13"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgZx7-k",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/health",
          "status": "Done",
          "title": "Creating Medicaid benefits by age-centile-year"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "jjustindoesstats"
          ],
          "content": {
            "body": "@jjustindoesstats we should try to increase our sample and number of variables available. The Health Surveys are available through [IPUMS](https://nhis.ipums.org/nhis/) and we should download them through there because they are already harmonized.\n\nAs a note, for both requests using IPUMS, it may be useful to automate by accessing their API.",
            "number": 12,
            "repository": "MaxMillerLab/health",
            "title": "Gather NHIS and MEPS data from IPUMS",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/12"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgZx7-o",
          "repository": "https://github.com/MaxMillerLab/health",
          "title": "Gather NHIS and MEPS data from IPUMS"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "jjustindoesstats"
          ],
          "content": {
            "body": "@jjustindoesstats please download ACS data (all years) from [IPUMS](https://usa.ipums.org/usa/) and create a model that maps P(Receive Medicaid) to  income, children, race, gender, etc. Start with linear probability model, then go to logit, and then we can try random forest (ask GPT for code).",
            "number": 11,
            "repository": "MaxMillerLab/health",
            "title": "Create model that links P(Receive Medicaid) to income, children, race, gender, etc",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/11"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgZx7-s",
          "repository": "https://github.com/MaxMillerLab/health",
          "title": "Create model that links P(Receive Medicaid) to income, children, race, gender, etc"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "jjustindoesstats"
          ],
          "content": {
            "body": "@jjustindoesstats we need to collect data from the Medicare actuaries on how large the funding shortfall is going forward. We should collect these data under the various scenarios they report. Could you look for these data? We're going to use this to construct the present value of benefits that are actually payable.",
            "number": 10,
            "repository": "MaxMillerLab/health",
            "title": "Collect data on Medicare funding shortfall",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/10"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgZx7-w",
          "repository": "https://github.com/MaxMillerLab/health",
          "title": "Collect data on Medicare funding shortfall"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "jjustindoesstats"
          ],
          "content": {
            "body": "@jjustindoesstats we need to project benefits for people of all ages in each year from 1996 on. We then need to discount these at the appropriate rate. Also, we will likely need to get Medicare spending projections back to 1989.",
            "number": 9,
            "repository": "MaxMillerLab/health",
            "title": "Create Medicare benefits PV",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/9"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgZx7-0",
          "repository": "https://github.com/MaxMillerLab/health",
          "title": "Create Medicare benefits PV"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "jjustindoesstats"
          ],
          "content": {
            "body": "@jjustindoesstats do this in 3 steps:\r\n1. Regression the share of Medicare spending at each age on population share for that age. Do across all MEPS survey years. Level of observation is age-survey year. Save the coefficients on age. Run without a constant.\r\n2. Divide Medicare spending per capita by the sum of population shares and regression coefficients from (1).\r\n3. That number is our price series.\r\n\r\nAfter doing this for our survey years, do this using the projections, assuming that the coefficients on age from the regression in (1) remain the same.",
            "number": 8,
            "repository": "MaxMillerLab/health",
            "title": "Medicare spending by age projections",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/8"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgZx7-4",
          "repository": "https://github.com/MaxMillerLab/health",
          "title": "Medicare spending by age projections"
        },
        {
          "assignees": [
            "jjustindoesstats"
          ],
          "content": {
            "body": "@jjustindoesstats please gather some data on age distribution projections in the United States.",
            "number": 6,
            "repository": "MaxMillerLab/health",
            "title": "Gather age distribution projections",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/6"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgZx7-8",
          "repository": "https://github.com/MaxMillerLab/health",
          "title": "Gather age distribution projections"
        },
        {
          "assignees": [
            "jjustindoesstats",
            "siyusun1108"
          ],
          "content": {
            "body": "@jjustindoesstats in the MEPS data, please create a plot that shows the average Medicare benefits received at each age for all survey years.",
            "number": 7,
            "repository": "MaxMillerLab/health",
            "title": "Create average medicare spending in each survey at each age",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/7"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgZx7_A",
          "repository": "https://github.com/MaxMillerLab/health",
          "title": "Create average medicare spending in each survey at each age"
        },
        {
          "content": {
            "body": "Merge branch 2 with the main branch\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n## Summary by CodeRabbit\n\n- **New Features**\n  - Introducing guidelines for writing good code in the new file `coding_practices.md`.\n  - Providing best practices for code readability, emphasizing self-documentation, clear structure, and Pythonic variable naming conventions.\n  - Demonstrating coding practices with an example of generating Fibonacci numbers in Python and Stata.\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->",
            "number": 5,
            "repository": "MaxMillerLab/health",
            "title": "2 reorganize code files using programs and functions",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/health/pull/5"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgZx7_E",
          "repository": "https://github.com/MaxMillerLab/health",
          "reviewers": [
            "MaxMillerLab"
          ],
          "title": "2 reorganize code files using programs and functions"
        },
        {
          "assignees": [
            "siyusun1108"
          ],
          "content": {
            "body": "Hey Siyu,\r\n\r\nI am hoping you can help reorganize our code files so that they are structured by programs and functions in the way that we discussed today. This will hopefully make them much, much easier to read and follow. Please thoroughly comment when doing this. Please comment on this thread each time a code file is updated. Thank you!\r\n\r\nWarmly,\r\nMax",
            "number": 2,
            "repository": "MaxMillerLab/health",
            "title": "Reorganize code files using programs and functions",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/2"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgZx7_I",
          "labels": [
            "enhancement"
          ],
          "linked pull requests": [
            "https://github.com/MaxMillerLab/health/pull/5"
          ],
          "repository": "https://github.com/MaxMillerLab/health",
          "title": "Reorganize code files using programs and functions"
        },
        {
          "assignees": [
            "siyusun1108"
          ],
          "content": {
            "body": "Hey Siyu,\r\n\r\nI need you to reorganize all our data folders into \"raw\" and \"derived\" according to the structure described in \"docs/storage.md\". The folder \"yield_curves\" is a good example, though that is missing documentation and the raw files. Remember \"raw\" contains original and lightly edited files, whereas \"derived\" contains output from the \"source\" codes. Don't hesitate to reach out with questions.\r\n\r\nFor pointers on when and how to address issues like this one, visit the \"docs/workflow.md\".\r\n\r\nWarmly,\r\nMax",
            "number": 1,
            "repository": "MaxMillerLab/health",
            "title": "Reorganize data files in datastore",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/1"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgZx7_M",
          "labels": [
            "enhancement",
            "help wanted"
          ],
          "repository": "https://github.com/MaxMillerLab/health",
          "title": "Reorganize data files in datastore"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "Create the following plots:\n- [ ] Medicare price series;\n- [ ] Aggregate Medicare PV by year;\n- [ ] Inequality with and without Medicare;\n- [ ] Plots using only the funded portion of Medicare\n",
            "number": 15,
            "repository": "MaxMillerLab/health",
            "title": "Create plots with medicare data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/15"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgZyN8g",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/health",
          "status": "Done",
          "title": "Create plots with medicare data"
        },
        {
          "content": {
            "body": "This will involve:\n1. Assuming that medicaid is funded out of general tax revenue\n2. Calculating it's portion of the projected deficit\n3. Assuming benefits are cut or taxes are raised to cover this deficit",
            "id": "DI_lAHOAkGERM4AjyzJzgJaeFQ",
            "title": "Project funding gap for Medicaid",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgZ0Rxs",
          "priority": "High",
          "status": "Backlog",
          "title": "Project funding gap for Medicaid"
        },
        {
          "assignees": [
            "jjustindoesstats"
          ],
          "content": {
            "body": "@jjustindoesstats could you please create code in `source/analysis/medicare` which creates a plot of medicare wealth by age for the year 2019. Please export as a PDF to `output/analysis/medicare`. The code should only create this plot.",
            "number": 16,
            "repository": "MaxMillerLab/health",
            "title": "Create plot of medicare wealth by age",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/16"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgaY3bs",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/health",
          "status": "Done",
          "title": "Create plot of medicare wealth by age"
        },
        {
          "assignees": [
            "jjustindoesstats"
          ],
          "content": {
            "body": "@MaxMillerLab \nThree wealth categories: Health+networth+ssw/total wealth\n\nUse stacked bar graph format for all plots \n\nPlot:\n\n- [x] By 5 year age buckets (eg.20-25) for initial sample year (2001) and end sample year (2022)\n\n- [x] By networth decile for 2001 and 2022\n\n- [x] By year (2001-2022)\n\n- [x] Adding Medicare funding short fall (intermediate and high scenario) to all plots above.\n",
            "number": 17,
            "repository": "MaxMillerLab/health",
            "title": "Creating wealth share plots",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/17"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgacZBY",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/health",
          "status": "Done",
          "title": "Creating wealth share plots"
        },
        {
          "assignees": [
            "leomushunje"
          ],
          "content": {
            "body": "## Objective\nConvert the SCF (Survey of Consumer Finances), MEPS (Medical Expenditure Panel Survey), and PSID (Panel Study of Income Dynamics) data from their current format into SQLite databases for improved accessibility and query performance.\n\n## Implementation Details\n\n### Database Technology\n- **Database**: SQLite\n- **Rationale**: Lightweight, serverless, and perfect for research data analysis\n\n### Data Locations\n- **Source data**: `datastore/raw/<data_type>/orig`\n  - `datastore/raw/scf/orig`\n  - `datastore/raw/meps/orig` \n  - `datastore/raw/psid/orig`\n\n### Output Locations\n- **Target location**: `datastore/raw/<data_type>/data`\n  - `datastore/raw/scf/data`\n  - `datastore/raw/meps/data`\n  - `datastore/raw/psid/data`\n\n## Tasks\n- [ ] Analyze the structure and format of existing data files in each `orig` directory\n- [ ] Design appropriate SQLite schema for each dataset (SCF, MEPS, PSID)\n- [ ] Create conversion scripts to transform data into SQLite format\n- [ ] Implement data validation to ensure integrity during conversion\n- [ ] Generate the `.db` files in the respective `data` directories\n- [ ] Document the database schemas and conversion process\n- [ ] Create example queries for common use cases\n\n## Deliverables\n- SQLite database files (`.db`) for each dataset\n- Conversion scripts with proper error handling\n- Documentation of database schemas\n- Data validation reports\n\n## Notes\n- Preserve all original data integrity\n- Ensure proper indexing for common query patterns\n- Consider adding metadata tables for data provenance\n",
            "number": 19,
            "repository": "MaxMillerLab/health",
            "title": "Convert SCF, MEPS, and PSID data into SQLite databases",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/19"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgbapu8",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/health",
          "status": "Done",
          "title": "Convert SCF, MEPS, and PSID data into SQLite databases"
        },
        {
          "assignees": [
            "zihuahe"
          ],
          "content": {
            "body": "by age group\n2000-2024\n\ngo back further to 1980/9\n\n2025-2100\n\n2000-2075\n\n1989+75\n\nregion? US only\n\nsource? social security administration, potential UN, WB\ncopy raw data to another location\nhttps://www.dropbox.com/home/Zihua%20He/Health/Empirics/data/raw/healthcare/price_index/orig\n\noutcome\ndata\n\ndeadline\ntwo weeks",
            "number": 24,
            "repository": "MaxMillerLab/health",
            "title": "Population projection",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/24"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgcmv7M",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/health",
          "start date": "2025-07-15",
          "status": "In progress",
          "target completion date": "2025-07-29",
          "title": "Population projection"
        },
        {
          "assignees": [
            "zahrahnabdul"
          ],
          "content": {
            "body": "Gather Medicare expenses and revenue projections and age distribution data from 1995 to 1989",
            "number": 25,
            "repository": "MaxMillerLab/health",
            "title": "Collect long-run demographic & health-spending data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/25"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgcpCXI",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/health",
          "start date": "2025-07-15",
          "status": "Ready",
          "target completion date": "2025-08-08",
          "title": "Collect long-run demographic & health-spending data"
        },
        {
          "assignees": [
            "zahrahnabdul"
          ],
          "content": {
            "body": "Max & I to discuss week of 7/28\n\nMy current understanding of the task (could be inaccurate):\n1. Define terminal ratios: pick target Medicare-to-GDP and Medicaid-to-GDP shares for year 100\n2. Assume persistence of the gap: state assumption that any spread between spending and revenue shares remains constant into the infinite future\n3. Project both series forward: grow each at long-run real GDP growth (or alternate scenario) beyond year 100\n4. Discount to present: use agreed real discount rate (Natasha suggested \u22482%) to compute the present value of the infinite tail for each program\n5. Add PV of current-cohort shortfall: use age-decile PVs for everyone 15+ alive today to capture obligations over the first 100 years\n6. Combine pieces: sum current-cohort PV + terminal tail PV to obtain total unfunded obligation\n",
            "number": 26,
            "repository": "MaxMillerLab/health",
            "title": "Total-obligation exercise outside the simulation",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/26"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgcpDAA",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/health",
          "status": "Ready",
          "title": "Total-obligation exercise outside the simulation"
        },
        {
          "assignees": [
            "siyusun1108"
          ],
          "content": {
            "body": "Goal: Predict the probability of Medicaid receipt as a function of agent observables from ACS data.\n\nTasks:\n- [x] Run a logit regression:\n      Outcome: receives Medicaid (binary)\n      Predictors: age, age\u00b2, age\u00b3, gender, income, income\u00b2, income\u00b3, log-income, family income\n- [x] Save coefficients and define prediction function\n- [x] Document dataset and variables used\n",
            "number": 27,
            "repository": "MaxMillerLab/health",
            "title": "Estimate Medicaid receipt via regression (ACS data)",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/27"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgcpbqU",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/health/pull/29",
            "https://github.com/MaxMillerLab/health/pull/31"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/health",
          "status": "Done",
          "target completion date": "2025-07-19",
          "title": "Estimate Medicaid receipt via regression (ACS data)"
        },
        {
          "assignees": [
            "siyusun1108"
          ],
          "content": {
            "body": "- [x] Create a new `medicaid/` module (similar to `medicare/` and `ss/`)\n- [x] Write a function to compute Medicaid receipt probability using regression coefficients\n- [x] Bring that logic into the simulation \u2014 will need to read how existing programs are integrated\n- [ ] Decide whether to simulate receipt (binary) or just attach probability\n- [ ] Locate in the simulation script where simulated are generated (can be across cohort, age, and year), so we can explore different policy options\n\n",
            "number": 28,
            "repository": "MaxMillerLab/health",
            "title": "Implement Medicaid receipt regression into simulation",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/28"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgcpcIw",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/health",
          "start date": "2025-07-16",
          "status": "Ready",
          "target completion date": "2025-07-31",
          "title": "Implement Medicaid receipt regression into simulation"
        },
        {
          "assignees": [
            "siyusun1108"
          ],
          "content": {
            "body": "Closes #[27]\r\n\r\n1. I have reviewed the modified files in the pull request and the changes are as I expected.\r\n2. Purpose: This PR completes Issue #27 by estimating the probability of Medicaid receipt using a logit regression on ACS data (2008-2023), exporting coefficients for simulation integration, and providing a markdown deliverable.\r\n3. Estimated review time: [10-15 minutes]\r\n4. Only two files are changed: the regression do-file and the deliverable markdown. Two new files have been generated (exported coefficients in stata and csv format from datastore/derived/healthcare/medicaid/medicaid_receipt_coefficients). No other outputs or scripts are affected.\r\n5. No merge conflicts are expected.",
            "number": 29,
            "repository": "MaxMillerLab/health",
            "title": "27 estimate medicaid receipt via regression",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/health/pull/29"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgct3ys",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/health",
          "reviewers": [
            "MaxMillerLab"
          ],
          "status": "Done",
          "target completion date": "2025-07-19",
          "title": "27 estimate medicaid receipt via regression"
        },
        {
          "assignees": [
            "siyusun1108"
          ],
          "content": {
            "body": "- [ ] Extract Medicare age coefficients from \"02 create medicare benefits code\"\n- [ ] Create function to calculate Medicare benefits by age (conditional on receipt)\n- [ ] Implement forward projection: calculate Medicare benefits \"h years ahead\" for each person\n- [ ] Integrate Medicare benefits calculation into simulation loop\n- [ ] Add proper discounting for future benefits (cumulative inflation, wage growth, survival probabilities)",
            "number": 30,
            "repository": "MaxMillerLab/health",
            "title": "Integrate Medicare benefits calculation into the simulation to compute expected Medicare benefits for each individual over their lifetime",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/health/issues/30"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgdD3ZA",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/health",
          "start date": "2025-07-29",
          "status": "Ready",
          "target completion date": "2025-08-08",
          "title": "Integrate Medicare benefits calculation into the simulation to compute expected Medicare benefits for each individual over their lifetime"
        },
        {
          "assignees": [
            "siyusun1108"
          ],
          "content": {
            "body": "1. Closes #27\r\n\r\n2. Files modified: 03_medicaid_receipt_logit_regression.do\r\n\r\n3. Purpose: Correctness - Updated age range restriction and added validation checks\r\n\r\n4. Time estimate for review: 5 minutes\r\n\r\n5. Changes made:\r\n   - Restricted age range to 20-66 years for working-age population analysis\r\n   - Added comprehensive sense checks for model validation\r\n   - Updated coefficients for simulation use\r\n\r\n6. No merge conflicts\r\n",
            "number": 31,
            "repository": "MaxMillerLab/health",
            "title": "Update issue 27 with age 20-66",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/health/pull/31"
          },
          "id": "PVTI_lAHOAkGERM4AjyzJzgdGYNI",
          "repository": "https://github.com/MaxMillerLab/health",
          "reviewers": [
            "MaxMillerLab"
          ],
          "status": "Done",
          "title": "Update issue 27 with age 20-66"
        }
      ]
    },
    "9": {
      "title": "Inventory",
      "items": [
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "Produce some results that address the concerns of R1. They are covered in the various sub-issues. The final thing to do is to write up the various response in the response to R1 and the editor.\n",
            "number": 1,
            "repository": "MaxMillerLab/inventory",
            "title": "Address R1 comments",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/inventory/issues/1"
          },
          "id": "PVTI_lAHOAkGERM4AjyvTzgZ0yjk",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/inventory",
          "size": "L",
          "status": "Done",
          "title": "Address R1 comments"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "Go through existing code and make sure it is well documented, clean, and efficient.",
            "number": 2,
            "repository": "MaxMillerLab/inventory",
            "title": "Refactor existing codebase",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/inventory/issues/2"
          },
          "id": "PVTI_lAHOAkGERM4AjyvTzgZ7Onw",
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/inventory",
          "status": "Done",
          "title": "Refactor existing codebase"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "For this: \n1. Obtain three series---TIPS yields, nominal government bond yields, and survey expected inflation (or realized inflation).\n2. Back out the data implied inflation risk premium. \n3. Show the model to this inflation risk premium. Do this in two regimes (from 2001-2021 and from 2022-2024).",
            "number": 3,
            "repository": "MaxMillerLab/inventory",
            "title": "Compute inflation risk premium using nominal bonds and TIPS",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/inventory/issues/3"
          },
          "id": "PVTI_lAHOAkGERM4AjyvTzgZ7XO0",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/inventory",
          "status": "Done",
          "title": "Compute inflation risk premium using nominal bonds and TIPS"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "To address this:\n1. Highlight that the new figure from #3 addresses this\n2. Add table that includes this calibrations (can decide if in main text or appendix later)",
            "number": 4,
            "repository": "MaxMillerLab/inventory",
            "title": "Provide more quantitative evidence for the rise in inflationary default risk post-Covid",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/inventory/issues/4"
          },
          "id": "PVTI_lAHOAkGERM4AjyvTzgZ7Y1U",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/inventory",
          "status": "Done",
          "title": "Provide more quantitative evidence for the rise in inflationary default risk post-Covid"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "To do this:\n1. Produce the CAPE in the model\n2. Plot that against the CAPE in the data\n3. See how that looks and then decide",
            "number": 5,
            "repository": "MaxMillerLab/inventory",
            "title": "Provide some analysis for the CAPE ratio",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/inventory/issues/5"
          },
          "id": "PVTI_lAHOAkGERM4AjyvTzgZ7ZOU",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/inventory",
          "status": "Done",
          "title": "Provide some analysis for the CAPE ratio"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "Provide some additional plots in the appendix:\n1. Plot expected inflation in the mode vs the data\n2. Plot consumption-dividend ratio in the model and in the data",
            "number": 6,
            "repository": "MaxMillerLab/inventory",
            "title": "Show how other series in the model evolve relative to the data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/inventory/issues/6"
          },
          "id": "PVTI_lAHOAkGERM4AjyvTzgZ7Z7g",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/inventory",
          "status": "Done",
          "title": "Show how other series in the model evolve relative to the data"
        }
      ]
    },
    "8": {
      "title": "Colonial Hedge",
      "items": [
        {
          "category": "Financial Hedge",
          "content": {
            "body": "Please isolate the UK data and pick out dates where the V-Dem main polyarchy index increases. Focus on the \"Electoral Democracy Index\".\r\n\r\nAfter you obtain this list, lookup online why each of the increases or decreases in the score happened.",
            "number": 7,
            "repository": "MaxMillerLab/colonialism",
            "title": "Identify democratization shocks in the data using V-Dem",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/7"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgQXE4A",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "status": "Done",
          "title": "Identify democratization shocks in the data using V-Dem"
        },
        {
          "assignees": [
            "radvilaspelanis"
          ],
          "category": "Refactor",
          "content": {
            "body": "@radvilaspelanis can you go through the existing codebase, takes notes of what is confusing, and refactor code for cleanliness where you can? Thanks!",
            "number": 8,
            "repository": "MaxMillerLab/colonialism",
            "title": "Take notes and refactor existing codebase",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/8"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgQXE5M",
          "labels": [
            "good first issue"
          ],
          "linked pull requests": [
            "https://github.com/MaxMillerLab/colonialism/pull/10"
          ],
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "status": "Done",
          "title": "Take notes and refactor existing codebase"
        },
        {
          "assignees": [
            "radvilaspelanis"
          ],
          "category": "Change Dem Likelihood",
          "content": {
            "body": "@radvilaspelanis can you go through the FMP site and take notes on strikes and protests happening in the UK in the 19th and early 20th century? Take down the date, location, newspaper, and any other relevant information you find in the article. Also, try to put together a selenium scraper that does this automatically (but don't get my account canceled). Excited to see what you find!",
            "number": 9,
            "repository": "MaxMillerLab/colonialism",
            "title": "Go through FMP page and look for strikes and protests",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/9"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgQXE5Q",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "status": "Done",
          "title": "Go through FMP page and look for strikes and protests"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "radvilaspelanis"
          ],
          "category": "Financial Hedge",
          "content": {
            "body": "Create basic summary stats for the finance data:\r\n\r\n- [x] What is the \"beta\" of a colonial stock?\r\n- [x] Do they have significant \"alpha\"\r\n- [x] What industries are the concentrated in?",
            "number": 12,
            "repository": "MaxMillerLab/colonialism",
            "title": "Finance data summary stats",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/12"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgRADhg",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "status": "Done",
          "title": "Finance data summary stats"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "adityadhar0"
          ],
          "content": {
            "body": "@adityadhar0 We should also look that the cumulative stock market respond to shocks to our labor mentions series using a local projection.",
            "number": 16,
            "repository": "MaxMillerLab/colonialism",
            "title": "Labor mention shock local projection",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/16"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgZ0WV0",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "title": "Labor mention shock local projection"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "adityadhar0"
          ],
          "content": {
            "body": "@adityadhar0 We should try the methods we discussed to create labor betas. Excited to see the results!",
            "number": 15,
            "repository": "MaxMillerLab/colonialism",
            "title": "Create labor betas",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/15"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgZ0WV8",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "title": "Create labor betas"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "radvilaspelanis"
          ],
          "content": {
            "body": "@radvilaspelanis can you please identify bond issues in the UK stocks data? Please try to differentiate between government and corporate bonds. Also, try to affirmatively identify equities too.",
            "number": 14,
            "repository": "MaxMillerLab/colonialism",
            "title": "Identify bond issues and equity in the data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/14"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgZ0WWA",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "title": "Identify bond issues and equity in the data"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "radvilaspelanis"
          ],
          "content": {
            "body": "@carissajc is coming up with a list of additional company related keywords to scrape. Do we have all the list of new keywords? And are links allocated to each device?",
            "number": 13,
            "repository": "MaxMillerLab/colonialism",
            "title": "Search additional keywords",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/13"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgZ0WWE",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "title": "Search additional keywords"
        },
        {
          "assignees": [
            "radvilaspelanis"
          ],
          "content": {
            "body": "@radvilaspelanis there was one issue with the merge from the `check_vdem.ipynb` file. Can you track down the differences and merge these two files? It should be pretty quick. `check_vdem_old.ipynb` is what was on your branch and `check_vdem_new.ipynb` is what was on `main`. Thanks a ton!",
            "number": 11,
            "repository": "MaxMillerLab/colonialism",
            "title": "Review check_vdem_old and check_vdem_new",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/11"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgZ0WWI",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "title": "Review check_vdem_old and check_vdem_new"
        },
        {
          "assignees": [
            "radvilaspelanis"
          ],
          "content": {
            "body": "Finalized the code\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n## Summary by CodeRabbit\n\n- **New Features**\n  - Introduced functionality to scrape articles from a website using Selenium.\n  - Implemented a script to scrape links from a website after logging in with specified credentials.\n  - Updated functionality for scraping UK and US data on fundamentals and prices.\n  - Added utility functions for interacting with the Global Financial Data API.\n  - Introduced a Selenium-based web scraping script to extract data from a specific website.\n  - Enhanced clarity and efficiency in data processing for historical and descriptive data related to UK territories.\n  - Improved fuzzy matching logic for efficient handling of matches and ratios.\n  - Updated scraping functions for British royal charters and UK company listings on Wikipedia.\n  - Refactored the `name_fuzzy_match_score` function for improved modularity.\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->",
            "number": 10,
            "repository": "MaxMillerLab/colonialism",
            "title": "8 take notes and refactor existing codebase",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/colonialism/pull/10"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgZ0WWM",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "reviewers": [
            "MaxMillerLab"
          ],
          "title": "8 take notes and refactor existing codebase"
        },
        {
          "content": {
            "body": "Please locate domestic, foreign, and colonial companies in the `datastore/raw/gfd/orig/UK Stocks.xlsx` file. You should use 2 different columns to do this:\r\n1. Country: separate into 3 buckets. Provide a map of your colonial keywords to these countries.\r\n2. Description: keyword search to identify colonial ventures.",
            "number": 6,
            "repository": "MaxMillerLab/colonialism",
            "title": "Basic methods for determining colonial exposure",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/6"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgZ0WWU",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "title": "Basic methods for determining colonial exposure"
        },
        {
          "content": {
            "body": "Please move the code in the \"analysis/code\" folder to \"derived\". Folders in \"derived\" correspond to a folder in \"datastore\". The folder structure is organized to allow for and understanding of what data is produced by each code file.",
            "number": 4,
            "repository": "MaxMillerLab/colonialism",
            "title": "Move code in \"analysis\" to appropriate folder in \"derived\"",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/4"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgZ0WWY",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "title": "Move code in \"analysis\" to appropriate folder in \"derived\""
        },
        {
          "content": {
            "body": "Will,\r\n\r\nEach of the datasets in \"datastore/derived\" should have a corresponding code folder and file that create them within \"source\". For example, each of the \"gfd\" files in \"derived/gfd\" should be created using code from \"source/gfd\" and all necessary inputs to create these files should be in \"raw/gfd\" with appropriate documentation. Happy to explain more, but we need to reorganize all code and data files to adhere to this convention.\r\n\r\nMax",
            "number": 3,
            "repository": "MaxMillerLab/colonialism",
            "title": "Matching datastore to source",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/3"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgZ0WWc",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "title": "Matching datastore to source"
        },
        {
          "content": {
            "body": "All code files should follow the structure of the gfd_api.py file. There is a \"Main\" function at the tops and auxiliary functions that follow below.",
            "number": 5,
            "repository": "MaxMillerLab/colonialism",
            "title": "Please recode .ipynb files as .py",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/5"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgZ0WWg",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "title": "Please recode .ipynb files as .py"
        },
        {
          "content": {
            "body": "Will,\r\n\r\nFor the wiki data code. The existing files have been stored in \"derived/wiki\". In \"raw/wiki\" I would like for you to put the html code of the pages that we use. This way we have the versions controlled, as future edits could be made to these pages. Please fill in the readme.md with the correct information and links to the pages. Then have the wiki code link to the the HTML files in raw. Happy to explain further if this isn't clear.\r\n\r\nMax ",
            "number": 2,
            "repository": "MaxMillerLab/colonialism",
            "title": "Wiki code and data structure",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/2"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgZ0WWk",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "title": "Wiki code and data structure"
        },
        {
          "content": {
            "body": "Will, \r\n\r\nPlease migrate the existing code to analysis. Also, figure out best practices for SCons. I'll be playing around with it too and we can converge next week!\r\n\r\nMax",
            "number": 1,
            "repository": "MaxMillerLab/colonialism",
            "title": "Test plus reminder",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/1"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgZ0WWo",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "title": "Test plus reminder"
        },
        {
          "assignees": [
            "radvilaspelanis"
          ],
          "content": {
            "body": "@radvilaspelanis this is to keep track of the OCR and parsing progress of the UK petitions data. Could you please provide regular updates of our status here?",
            "number": 17,
            "repository": "MaxMillerLab/colonialism",
            "title": "OCR and parse petitions data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/17"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgZ0Wh8",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "start date": "2025-04-28",
          "status": "In progress",
          "title": "OCR and parse petitions data"
        },
        {
          "assignees": [
            "adityadhar0"
          ],
          "content": {
            "body": "@adityadhar0 this is to keep track of the first pass for the model. I think there are 4 main ingredients we should have:\n1. Two assets with exogenous cashflows. The \"colonial asset\" makes up X% of the economy.\n2. Repression of democratic \"step\" which costs $\\kappa$.\n3. Democratic instutitions follow a step process.\n4. Continuous income distribution such that each \"step\" comes with some known amount of redistribution based on a median voter rule.\n\nYou do not need to do the \"revolution constraint\" that I do. It adds little here. It's just Elites trading off some cost kappa with the \"hit\" from redistribution. We should then do comparative statics on X.\n",
            "number": 18,
            "repository": "MaxMillerLab/colonialism",
            "title": "First pass at model",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/18"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgZ0XKw",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "status": "Done",
          "title": "First pass at model"
        },
        {
          "assignees": [
            "adityadhar0"
          ],
          "content": {
            "body": "@adityadhar0 we discussed that you have made some progress on this. Can you please put here what you're doing for this?",
            "number": 19,
            "repository": "MaxMillerLab/colonialism",
            "title": "Crisis factor",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/19"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgZ9N_E",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "status": "Done",
          "title": "Crisis factor"
        },
        {
          "assignees": [
            "radvilaspelanis"
          ],
          "content": {
            "body": "@radvilaspelanis can you find some estimates for the share of UK wealth associated with colonial possessions over time? Preferably starting in 1800?",
            "number": 20,
            "repository": "MaxMillerLab/colonialism",
            "title": "Find share of UK wealth in colonies over time",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/20"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgaV00E",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "status": "Done",
          "title": "Find share of UK wealth in colonies over time"
        },
        {
          "assignees": [
            "Ukhansky"
          ],
          "content": {
            "body": "@adityadhar0 and @radvilaspelanis we discussed some sources for colonial exposure measures for individuals. Any chance you guys remember some of them? We need to:\n1. List the various sources;\n2. Track down the data.\n\nCan you guys please respond below with what you remember from the sources? I just realized I scattered my notes somewhere, but I will add mine too.",
            "number": 21,
            "repository": "MaxMillerLab/colonialism",
            "title": "Look into sources for colonial exposure",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/21"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgaV4yk",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "status": "Done",
          "title": "Look into sources for colonial exposure"
        },
        {
          "assignees": [
            "radvilaspelanis"
          ],
          "content": {
            "body": "Goal: to keep track of the transfer of petition files between grid and RCP",
            "number": 22,
            "repository": "MaxMillerLab/colonialism",
            "title": "Transfer files between grid and RCP",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/22"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgaf_G4",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "status": "Done",
          "title": "Transfer files between grid and RCP"
        },
        {
          "assignees": [
            "adityadhar0"
          ],
          "content": {
            "body": "@MaxMillerLab this is to keep track of the second pass at the model. \n\nWe currently have \n1. Domestic macroeconomic environment is a standard Lucas Tree, colonial asset is the same process\n2. Ex-ante wealth is distributed Pareto\n3. In each period, voting share increases by $s_t$ with probability $p$, where Elites set the probability $p$\n4. Tax policy is implemented by the median voter $v_t/2$. \n5. Colonial asset decreases willingness to repress\n\nTo add:\n1. Asset pricing implications - want to show colonial asset acts as a hedge.\n2. Colonial asset production with declining returns to scale (most productive colonies exploited first)\n3. Potential growth-redistribution tradeoff\n4. Potentially allow $s_t$ to be negative\n",
            "number": 23,
            "repository": "MaxMillerLab/colonialism",
            "title": "Second pass at model",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/23"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzga5ih4",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "start date": "2025-05-28",
          "status": "In progress",
          "title": "Second pass at model"
        },
        {
          "assignees": [
            "Ukhansky"
          ],
          "content": {
            "body": "@Ukhansky please scrape the three sources that you mentioned in #21. \n\n**Sources to scrape:**\n- [x] Blue Books\n- [x] Colonial Office Lists\n- [x] India \u2013 Not included in Blue Books\n\n**Instructions:**\nPlease store scripts you use in `source\\derived\\colonial_exposure` and remember to number them in the order they should be run. Please store the scraped files into three separate zipfiles in `datastore\\raw\\colonial_exposure`. The zipfiles should be titled `blue_books.zip`, `colonial_office_lists.zip`, and `india.zip`. Inside them should be the OCRed text data. If possible, please give each file inside an informative name (e.g. jamaica_1901.txt).\n\n**Other:**\nIn the project space (which I have connected), please provide a target end date for this (e.g. when you expect it to be finished).",
            "number": 24,
            "repository": "MaxMillerLab/colonialism",
            "title": "Scrape colonial exposure sources",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/24"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgbAwR4",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "start date": "2025-06-05",
          "status": "In review",
          "target completion date": "2025-06-21",
          "title": "Scrape colonial exposure sources"
        },
        {
          "assignees": [
            "Ukhansky"
          ],
          "content": {
            "body": "@Ukhansky I would like you to run some \"off-the-shelf\" NER models on the text data that you have scraped. Here is the proposed workflow:\n1. Look for top NER models on HuggingFace\n2. Write script that uses your chosen model to locate entities.\n3. Verify success on small subsample of data\n4. Run script on the grid using GPU (that will speed things up substantially).\n5. Goal: dataset with entity-colony-year\n\nNote: we may want to use an LLM to refine our output... we can discuss this after building the NER code.",
            "number": 28,
            "repository": "MaxMillerLab/colonialism",
            "title": "Run NER on colonial exposure documents",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/28"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgbpFhI",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "start date": "2025-06-20",
          "status": "Ready",
          "target completion date": "2025-06-26",
          "title": "Run NER on colonial exposure documents"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "adityadhar0"
          ],
          "content": {
            "body": "@adityadhar0 going to use this thread to document some of the model implications I think are present:\n1. Increasing colonial wealth granger causes colonial expansion\n2. The return on colonial assets is lower than the return on domestic assets\n3. Positive shocks to the cost of repression cause:\n   - Contemporaneous high returns on colonial assets\n   - The expansion of colonial assets\n\nI know there are more, but these come to mind. I also chatted with Matteo Leombroni about the paper and I think we can do a little more on the asset pricing side. I will start a new issue to describe the empirics, but I wonder if the model will generate:\n\n1. Colonial assets have a negative alpha under the correct risk model (equivalent to 2. above, but after adjusting for other risk exposure)\n2. Leaving out the Democracy shocks from a CAPM-style regression will lead to **lower** beta estimates for the colonies and _higher_ beta estimates for the domestics\n3. Can we come up with expressions for the price of both democracy and consumption risks in the model?",
            "number": 29,
            "repository": "MaxMillerLab/colonialism",
            "title": "Model implications",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/29"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgcKT4s",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "start date": "2025-07-02",
          "status": "Done",
          "target completion date": "2025-07-17",
          "title": "Model implications"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "adityadhar0"
          ],
          "content": {
            "body": "@adityadhar0 convo with Matteo last night was helpful for me thinking through some other AP results that would help us. I think we should do the following:\n1. Construct a long-short \"labor risk factor\" by: \n   * Estimating betas on our labor strikes series, \n   * Creating portfolios (either quintiles or terciles depending on how volatile the estimates are)\n   * Subtracting bottom from top quantile and using this as a risk factor\n2. We should then do the normal Fama-Macbeth stuff to back out the price of risk and use this to calibrate the model\n3. We should estimate CAPM-betas with and without the labor risk factor and see what they look like\n4. We should see what the alpha is on colonial assets under the \"correct\" (i.e. all relevant risk factors included) model\n5. We should understand why the history literature thinks that the return to these assets were low",
            "number": 30,
            "repository": "MaxMillerLab/colonialism",
            "title": "Revisiting asset pricing implications",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/30"
          },
          "id": "PVTI_lAHOAkGERM4AjyuXzgcKVJs",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "start date": "2025-07-02",
          "status": "Ready",
          "target completion date": "2025-07-31",
          "title": "Revisiting asset pricing implications"
        },
        {
          "assignees": [
            "Ukhansky"
          ],
          "content": {
            "body": "I\u2019ve been reviewing the OCR pipeline and noticed a few things we should improve before scaling it further. While the current script works for extracting Markdown, it\u2019s not saving all of the data returned by the API: images referenced in the Markdown and the full API response object itself.\n\nAlso, there are cases where pages appear to be silently skipped (but we don't know). We need clearer logging when that happens and a retry mechanism to make sure we don\u2019t miss data.\n\n## Steps\n\n- [x] Save the `image_base64` content from each page to a `.jpeg` file.\n- [x] Match the Markdown image references to actual saved image files (e.g., `img-18.jpeg`).\n- [x] Save the **entire OCR API response** (as `.md`) per document, for transparency and debugging.\n- [ ] Add a retry loop (with backoff) for pages that fail.\n- [x] Zip together all output from a single run (`.md`, `.jpeg`, `.json`, etc.) to make the results portable.\n\n\nRight now we\u2019re not seeing the full picture of what the API returns. Moving forward, we should be clearer on what\u2019s happening before spending on full runs (we should avoid stupid mistakes that would cost several hundred bucks). Having the raw response + page-level logging will help us identify errors earlier and make reruns more precise if needed.\n\nI\u2019ll start by making some of these changes (starting with saving `image_base64` and logging skipped pages). I\u2019ll also grab a sample full API response and include it here for reference. Open to thoughts from others once the branch is up.\n",
            "number": 33,
            "repository": "MaxMillerLab/colonialism",
            "title": "Save Full OCR API Output (Including Images) and Improve Script Structure",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/colonialism/issues/33"
          },
          "estimate": 1,
          "id": "PVTI_lAHOAkGERM4AjyuXzgc-A0c",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/colonialism",
          "size": "S",
          "start date": "2025-07-25",
          "status": "Ready",
          "target completion date": "2025-08-01",
          "title": "Save Full OCR API Output (Including Images) and Improve Script Structure"
        }
      ]
    },
    "7": {
      "title": "Who values democracy?",
      "items": [
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Response letter",
          "content": {
            "body": "- [x] Editor letter\n- [x] R1 report\n- [x] R2 report\n- [x] R3 report\n- [x] R4 report",
            "number": 4,
            "repository": "MaxMillerLab/democracy",
            "title": "Organize comments by priority",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/democracy/issues/4"
          },
          "id": "PVTI_lAHOAkGERM4AjyqjzgQXCc8",
          "repository": "https://github.com/MaxMillerLab/democracy",
          "status": "Done",
          "title": "Organize comments by priority"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Response letter",
          "content": {
            "body": "- [x] Editor letter\n- [x] R1 report\n- [x] R2 report\n- [x] R3 report\n- [x] R4 report",
            "number": 5,
            "repository": "MaxMillerLab/democracy",
            "title": "Fill in response letter with broad plan",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/democracy/issues/5"
          },
          "id": "PVTI_lAHOAkGERM4AjyqjzgQXCdM",
          "repository": "https://github.com/MaxMillerLab/democracy",
          "status": "Done",
          "title": "Fill in response letter with broad plan"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "spkim1228"
          ],
          "category": "Replication code",
          "content": {
            "body": "Go through all scripts and figure out the necessary files to run them. Then move those files into the new datastore and redefine paths.\r\n\r\n@spkim1228 will handle:\r\n- [x] `cleaning`\r\n- [x] `inputs`\r\n- [x] `load_and_merge`\r\n- [x] `setup`\r\n\r\n@MaxMillerLab will handle the mess in:\r\n- [ ] `model`\r\n- [ ] `results`\r\n\r\nThe `programs` folder could be handled by either.",
            "number": 1,
            "repository": "MaxMillerLab/democracy",
            "title": "Redo file paths in all scripts",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/democracy/issues/1"
          },
          "id": "PVTI_lAHOAkGERM4AjyqjzgR608w",
          "repository": "https://github.com/MaxMillerLab/democracy",
          "status": "Done",
          "title": "Redo file paths in all scripts"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Replication code",
          "content": {
            "body": "",
            "id": "DI_lAHOAkGERM4AjyqjzgHT-EA",
            "title": "Recode input and setup files",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4AjyqjzgR60_k",
          "priority": "High",
          "status": "Backlog",
          "title": "Recode input and setup files"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Replication code",
          "content": {
            "body": "",
            "id": "DI_lAHOAkGERM4AjyqjzgHT-EM",
            "title": "Recode merging files",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4AjyqjzgR60_4",
          "priority": "High",
          "status": "Backlog",
          "title": "Recode merging files"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Replication code",
          "content": {
            "body": "",
            "id": "DI_lAHOAkGERM4AjyqjzgHT-Eo",
            "title": "Recode cleaning files",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4AjyqjzgR61AY",
          "priority": "High",
          "status": "Backlog",
          "title": "Recode cleaning files"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Replication code",
          "content": {
            "body": "",
            "id": "DI_lAHOAkGERM4AjyqjzgHT-FA",
            "title": "Recode results files and move them to analysis",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4AjyqjzgR61Bs",
          "priority": "High",
          "status": "Backlog",
          "title": "Recode results files and move them to analysis"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Replication code",
          "content": {
            "body": "",
            "id": "DI_lAHOAkGERM4AjyqjzgHT-FM",
            "title": "Recode model files",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4AjyqjzgR61B8",
          "priority": "High",
          "status": "Backlog",
          "title": "Recode model files"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "spkim1228"
          ],
          "category": "Formatting",
          "content": {
            "body": "The paper needs to be reformatted to allow it to be in JPE style. Notes on how to do this can be found [here](https://uchicago.app.box.com/s/r30zd1bllp361x8untzruwbfov0ptr4z).",
            "number": 7,
            "repository": "MaxMillerLab/democracy",
            "title": "Format the paper for publication in JPE",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/democracy/issues/7"
          },
          "id": "PVTI_lAHOAkGERM4AjyqjzgZ9WhE",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/democracy",
          "start date": "2025-05-02",
          "status": "Ready",
          "target completion date": "2025-08-22",
          "title": "Format the paper for publication in JPE"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Formatting",
          "content": {
            "body": "Checking figures and code that creates figures to see if it matches the JPE instructions for Figures. I am planning on cross checking it with the files in `source/figures`. These guidelines include:\n\n- [x] Making the figures Black & White (if you were planning on doing color printing, please correct me!)\n- [x] High resolution PDFs\n- [ ] It also requests that figure legends be separate from the figure itself.\n\nThe other requirements appear to be taken care of in terms of labelling.",
            "number": 10,
            "repository": "MaxMillerLab/democracy",
            "title": "Checking Figures for JPE Format",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/democracy/issues/10"
          },
          "id": "PVTI_lAHOAkGERM4Ajyqjzgc7gFo",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/democracy",
          "start date": "2025-07-23",
          "status": "In progress",
          "target completion date": "2025-07-31",
          "title": "Checking Figures for JPE Format"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Formatting",
          "content": {
            "body": "The JPE instructions request that all datasets include a README.pdf containing a list of all files included and guiding the user on how to use them for replication. Additionally, this denotes a difference in procedure for proprietary datasets. I must make note of which datasets are proprietary and summarize the information accordingly.",
            "number": 11,
            "repository": "MaxMillerLab/democracy",
            "title": "Creating README files for all Datasets",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/democracy/issues/11"
          },
          "id": "PVTI_lAHOAkGERM4AjyqjzgdAK8U",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/democracy",
          "start date": "2025-07-29",
          "status": "In progress",
          "target completion date": "2025-08-01",
          "title": "Creating README files for all Datasets"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Replication code",
          "content": {
            "body": "Figure out minimum datawork that needs to be done to produce main text results, separate from the appendix.\n\nThis involves understanding all the necessary variables to get the final results, grouping by various variables, including equity, macro, political, event, etc.\n\nThe output will involve a master dataset with all the necessary variables and data to replicate currently existing results.",
            "number": 12,
            "repository": "MaxMillerLab/democracy",
            "title": "Figure out Minimum Datawork to Obtain Main Results",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/democracy/issues/12"
          },
          "id": "PVTI_lAHOAkGERM4AjyqjzgdF4dE",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/democracy",
          "start date": "2025-07-29",
          "status": "In progress",
          "target completion date": "2025-07-31",
          "title": "Figure out Minimum Datawork to Obtain Main Results"
        }
      ]
    },
    "6": {
      "title": "Social Security II",
      "items": [
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "I need to do two things:\n1. Recode the existing shocks code in Matlab\n2. Find an extended labor income series from Fatih and co.\n\nFor (2), the series we are using comes from Table C.1 of their XX paper. I checked in their [GRID database](https://data.grid-database.org/#/view_cart) for an updated series, but it doesn't seem to be there.\n\n",
            "number": 1,
            "repository": "MaxMillerLab/interest_rate_risk",
            "title": "Produce historical rf, h, g, and s series for simulation",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/interest_rate_risk/issues/1"
          },
          "id": "PVTI_lAHOAkGERM4AjyjrzgZ0SZQ",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/interest_rate_risk",
          "status": "Done",
          "title": "Produce historical rf, h, g, and s series for simulation"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "Do proof to show that our IRS is correct. Then argue that we get the \"growth component\" right in the empirics because we do our SMM exercise.",
            "number": 2,
            "repository": "MaxMillerLab/interest_rate_risk",
            "title": "Proof to show that our measure is IRS (in a partial derivative sense)",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/interest_rate_risk/issues/2"
          },
          "id": "PVTI_lAHOAkGERM4AjyjrzgZ0b0Q",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/interest_rate_risk",
          "status": "Done",
          "title": "Proof to show that our measure is IRS (in a partial derivative sense)"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "This will mostly be copy and pasting from Simulation.m. This involves updating the policy functions and the shocks.",
            "number": 3,
            "repository": "MaxMillerLab/interest_rate_risk",
            "title": "Update SimulationOLG.m",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/interest_rate_risk/issues/3"
          },
          "id": "PVTI_lAHOAkGERM4AjyjrzgaTNjI",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/interest_rate_risk/pull/7"
          ],
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/interest_rate_risk",
          "status": "Done",
          "title": "Update SimulationOLG.m"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "",
            "number": 4,
            "repository": "MaxMillerLab/interest_rate_risk",
            "title": "Rerun the model validation graphs",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/interest_rate_risk/issues/4"
          },
          "id": "PVTI_lAHOAkGERM4AjyjrzgaTOes",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/interest_rate_risk",
          "status": "Done",
          "title": "Rerun the model validation graphs"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "We need to show that we match:\n1. Equity market participation - indicator variable\n2. Equity dollars - mean and median (scale by wage index)\n3. Housing dollars - mean and median (scale by wage index)\n4. Home ownership rate - might not look good\n",
            "number": 5,
            "repository": "MaxMillerLab/interest_rate_risk",
            "title": "Create new graphs",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/interest_rate_risk/issues/5"
          },
          "id": "PVTI_lAHOAkGERM4AjyjrzgaTPWc",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/interest_rate_risk",
          "status": "Done",
          "title": "Create new graphs"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "The model will generate a rise in wealth inequality. We want to break down what is important in generating wealth inequality.\n1. Show there is a strong interest rate component;\n2. Show stock market effect;\n3. Show housing effect;\n4. Show labor income;\n5. Show Social Security.\n\nRun counterfactuals:\n1. Run average path in the simulation\n2. Run historical path - how anomalous was the historical path?\n3. Look at how different cohorts were helped/hurted from a welfare perspective\n    - Decompose into stock, housing, interest rates\n\nThink about \"Asset Price Redistribution\" but within the context of the model.",
            "number": 6,
            "repository": "MaxMillerLab/interest_rate_risk",
            "title": "Evolution of the top wealth share",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/interest_rate_risk/issues/6"
          },
          "end date": "2025-07-31",
          "id": "PVTI_lAHOAkGERM4AjyjrzgaTRYk",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/interest_rate_risk",
          "start date": "2025-05-12",
          "status": "Done",
          "title": "Evolution of the top wealth share"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "Explore how well we match the data for housing and equity along the wealth dimension, for a given age group (I guess 40-45). This is the analog of Figure 8 for these outcomes",
            "number": 14,
            "repository": "MaxMillerLab/interest_rate_risk",
            "title": "Check housing and stock wealth along income and wealth",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/interest_rate_risk/issues/14"
          },
          "id": "PVTI_lAHOAkGERM4AjyjrzgdAK8k",
          "repository": "https://github.com/MaxMillerLab/interest_rate_risk",
          "title": "Check housing and stock wealth along income and wealth"
        }
      ]
    },
    "5": {
      "title": "Social Security and Inequality",
      "items": [
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "",
            "number": 9,
            "repository": "MaxMillerLab/ssw",
            "title": "Paper numbers code",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/ssw/issues/9"
          },
          "id": "PVTI_lAHOAkGERM4AjyjEzgQW-gc",
          "repository": "https://github.com/MaxMillerLab/ssw",
          "status": "Done",
          "title": "Paper numbers code"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "- [x] Table 1\n- [x] Table 2\n- [x] Table 3",
            "number": 7,
            "repository": "MaxMillerLab/ssw",
            "title": "Table code",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/ssw/issues/7"
          },
          "estimate": 1,
          "id": "PVTI_lAHOAkGERM4AjyjEzgQW-gw",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/ssw/pull/8"
          ],
          "repository": "https://github.com/MaxMillerLab/ssw",
          "status": "Done",
          "title": "Table code"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "siyusun1108"
          ],
          "content": {
            "body": "@siyusun1108 let's start with the main body figures. And let's create a new branch to work on this together. Here is a checklist of the ones outstanding with them assigned to each of us:\r\n- [x] Figure 1 (Max)\r\n- [x] Figure 4 (Max)\r\n- [x] Figure 5 (Siyu)\r\n- [x] Figure 6 (Siyu)\r\n- [x] Figure 7 (Max)\r\n- [x] Figure 8 (Max)\r\n- [x] Figure 9 (Siyu)\r\n- [x] Figure 10 (Siyu)\r\n- [x] Figure 11 (Max)\r\n- [x] Figure 12 (Max)\r\n- [x] Figure 13 (Max)\r\n- [x] Figure 14 (Max)\r\n\r\nFirst one to finish wins!",
            "number": 4,
            "repository": "MaxMillerLab/ssw",
            "title": "Recode figures in separate scripts",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/ssw/issues/4"
          },
          "id": "PVTI_lAHOAkGERM4AjyjEzgQW-lk",
          "labels": [
            "enhancement"
          ],
          "linked pull requests": [
            "https://github.com/MaxMillerLab/ssw/pull/5"
          ],
          "repository": "https://github.com/MaxMillerLab/ssw",
          "status": "Done",
          "title": "Recode figures in separate scripts"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "siyusun1108"
          ],
          "content": {
            "body": "@siyusun1108 can you please write a suite of tests that compare the output of the replication code to the output of the old code and check for large differences? The ideal code would flag these differences and make a list of files to check. Thanks!!",
            "number": 2,
            "repository": "MaxMillerLab/ssw",
            "title": "Create automated tests for replication code to old data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/ssw/issues/2"
          },
          "id": "PVTI_lAHOAkGERM4AjyjEzgQW-mc",
          "labels": [
            "help wanted"
          ],
          "repository": "https://github.com/MaxMillerLab/ssw",
          "status": "Done",
          "title": "Create automated tests for replication code to old data"
        },
        {
          "assignees": [
            "siyusun1108"
          ],
          "content": {
            "body": "@siyusun1108 can you help me fill in the sources and targets  in the SConscript file in `source/analysis`? I'm working on finishing the last of the appendix figures and tables now.",
            "number": 6,
            "repository": "MaxMillerLab/ssw",
            "title": "Fill in SConscript file in analysis",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/ssw/issues/6"
          },
          "id": "PVTI_lAHOAkGERM4AjyjEzgQdiq8",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/ssw/pull/13"
          ],
          "repository": "https://github.com/MaxMillerLab/ssw",
          "status": "Done",
          "title": "Fill in SConscript file in analysis"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "- [x] Figure IA1. Life expectancy differentials\n- [x] Figure IA2: Adjusting for differential in life expectancy\n- [x] Figure IA3. Discount rate adjustment and fraction receiving the market rate\n- [x] Figure IA4. Top 10% and Top 1% wealth shares \u2014 Higher and lower heterogeneous discount rates\n- [x] Figure IA5. Market Implied and Social Security Administration Yield Curve Estimates\n- [x] Figure IA6. Zero-Social Security Income Estimates: Deaton-Paxson Regressions",
            "number": 10,
            "repository": "MaxMillerLab/ssw",
            "title": "Appendix figures code",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/ssw/issues/10"
          },
          "id": "PVTI_lAHOAkGERM4AjyjEzgQdrMU",
          "repository": "https://github.com/MaxMillerLab/ssw",
          "status": "Done",
          "title": "Appendix figures code"
        },
        {
          "assignees": [
            "siyusun1108"
          ],
          "content": {
            "body": "Hey @siyusun1108 can you go through the tex file and remove all the commented out text? We just want the text that is actually printed to the PDF showing. Thanks a ton!",
            "number": 11,
            "repository": "MaxMillerLab/ssw",
            "title": "Remove commented out text from tex file",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/ssw/issues/11"
          },
          "id": "PVTI_lAHOAkGERM4AjyjEzgQd9qY",
          "repository": "https://github.com/MaxMillerLab/ssw",
          "status": "Done",
          "title": "Remove commented out text from tex file"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "siyusun1108"
          ],
          "content": {
            "body": "For the final step, we must write a series of \"acceptance tests\". To do this, we must adjust the old project code to output a series of datasets with columns that can be matched to the recoded versions. We must then track down all changes and decide whether the change comes from a corrected error or an error made in the refactoring.\n\nNext, a series of sanity tests must be implemented to assure the data make sense.\n\nAfter this, the replication code is complete and we can resubmit to JF.",
            "number": 12,
            "repository": "MaxMillerLab/ssw",
            "title": "Write final acceptance tests",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/ssw/issues/12"
          },
          "id": "PVTI_lAHOAkGERM4AjyjEzgQd-4g",
          "repository": "https://github.com/MaxMillerLab/ssw",
          "status": "Done",
          "title": "Write final acceptance tests"
        }
      ]
    },
    "4": {
      "title": "PEPs",
      "items": [
        {
          "assignees": [
            "freddypinzon"
          ],
          "category": "Party Members",
          "content": {
            "body": "@freddypinzon I think @mounu1 mentioned this, but a while back he asked you guys to update the elections results data from he and @ecolonnelli's AER. The data are currently up to 2016. Can you work on extending this to 2022? This takes precedence over refactoring the existing codebase.",
            "number": 10,
            "repository": "MaxMillerLab/peps",
            "title": "Create elections outcome data (Party Members)",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/10"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgQV1mA",
          "repository": "https://github.com/MaxMillerLab/peps",
          "status": "Done",
          "title": "Create elections outcome data (Party Members)"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "spkim1228"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "- [x] Move price and return data into datastore.\n- [ ] Track down balance sheet information for each ISIN.\n- [x] Dataset: CNPJ (tax ID for Brazilian companies) and ISIN mapping for each company.\n\nCNPJ has 8 digits at the firm level and 14 (8 for firm plus 6 for establishment)",
            "number": 11,
            "repository": "MaxMillerLab/peps",
            "title": "Track down stock information",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/11"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgQVyLw",
          "repository": "https://github.com/MaxMillerLab/peps",
          "status": "Done",
          "title": "Track down stock information"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "freddypinzon",
            "spkim1228",
            "BenLisboa"
          ],
          "category": "Refactoring",
          "content": {
            "body": "@freddypinzon we have one last push on refactoring the old code base. Here is what remains to be done:\r\n\r\n## PEPs code\r\n\r\nThere are 6 files that need to be revisited:\r\n- [x] A1_clean_CEIS_owners_managers.R\r\n- [ ] A1_clean_DC_main.R\r\n- [x] A1_clean_DC_owners_managers.R\r\n- [x] A1_clean_SOE_owners_managers.R\r\n- [x] A2_merge_datasets.do\r\n- [x] A2_merge_datasets_FIXED.do\r\n- [x] A6_first_year_peps.do\r\n\r\nLastly, we need to figure out where the `PartyMembers.dta` dataset comes from. Once these are finished, the refactoring of the project is complete. I'll bring on additional RA hours to help with this soon.\r\n\r\nUPDATE: `PartyMembers.dta` comes from the Politics at Work paper and should be treated as raw data for the moment.\r\n\r\n- [x] Move `PartyMembers.dta` to raw\r\n\r\nFinally we need to add the code that @freddypinzon and @siwovalle began on Brazilian elections.\r\n- [x] Find and finish Brazilian election outcome code.\r\n\r\n## CVM and securities code\r\nThere are a few other codes to refactor:\r\n- [x] Recode fifo_returns.R\r\n- [x] ISIN retrieval script\r\n- [x] Alan transaction data cleaning code (Bernardo)\r\n\r\nWe also need to find the company information for the ISINs we have (e.g. balance sheet data)\r\n- [ ] Track down balance sheet data for the ISINs we have\r\n- [ ] Track down factor returns for Brazil\r\n",
            "number": 9,
            "repository": "MaxMillerLab/peps",
            "title": "Final push on refactoring old code and putting together the necessary data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/9"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgQXDMg",
          "repository": "https://github.com/MaxMillerLab/peps",
          "status": "Done",
          "title": "Final push on refactoring old code and putting together the necessary data"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Party Members",
          "content": {
            "body": "## Summary stats to produce \n\n- [ ] Do people from different parties trade different securities depending on who is in office?\n- [x] Do they earn different returns?\n- [ ] Do high aggregate or industrial returns while a politician is in office encourage people to become party members?\n\n## Next steps\n\n- [x] Work with Alan to run the code easily on his end",
            "number": 12,
            "repository": "MaxMillerLab/peps",
            "title": "Create party members summary stats",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/12"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgQacPE",
          "repository": "https://github.com/MaxMillerLab/peps",
          "status": "Done",
          "title": "Create party members summary stats"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "freddypinzon"
          ],
          "category": "PEPs",
          "content": {
            "body": "@freddypinzon can you go through these papers and code up stuff to do the same analysis? We will want to also do this by various PEP and party member groups.\n\n[Kelley and tetlock 2013 JF.pdf](https://github.com/user-attachments/files/18907394/Kelley.and.tetlock.2013.JF.pdf)\n\n[Chakrabarty-PerformanceShortTermInstitutional-2017 (1).pdf](https://github.com/user-attachments/files/18907398/Chakrabarty-PerformanceShortTermInstitutional-2017.1.pdf)",
            "number": 27,
            "repository": "MaxMillerLab/peps",
            "title": "Replicate imbalance measures/results from JF and JFQA",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/27"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0Im0",
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/peps",
          "status": "Done",
          "title": "Replicate imbalance measures/results from JF and JFQA"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "freddypinzon"
          ],
          "content": {
            "body": "Use data on underlying prices, returns, and volatility to obtain options returns using Black-Scholes. @freddypinzon is working on this currently. We will do the same for futures/forwards soon.",
            "number": 26,
            "repository": "MaxMillerLab/peps",
            "title": "Create options returns",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/26"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0Im4",
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Create options returns"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "freddypinzon"
          ],
          "content": {
            "body": "@freddypinzon I would like to write some code that turns people into ETFs. Basically, we want to create monthly \"holdings\" data off of there trades. If someone sells something without buying it, it means that they bought it before our sample begins. Once we create this holdings data, we can get the performance of the \"person-ETF\".",
            "number": 23,
            "repository": "MaxMillerLab/peps",
            "title": "Turn people into ETFs",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/23"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0InA",
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Turn people into ETFs"
        },
        {
          "content": {
            "body": "Recoded .ipynb file to .py files of same names",
            "number": 25,
            "repository": "MaxMillerLab/peps",
            "title": "15 recode isin to cnpj mapping",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/peps/pull/25"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0InE",
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "15 recode isin to cnpj mapping"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@spkim1228 can you please look into this? Please comment regularly on your progress.",
            "number": 15,
            "repository": "MaxMillerLab/peps",
            "title": "Recode ISIN to CNPJ mapping",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/15"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0InI",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/peps/pull/25"
          ],
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Recode ISIN to CNPJ mapping"
        },
        {
          "assignees": [
            "freddypinzon"
          ],
          "content": {
            "body": "@freddypinzon we are still missing quite a few elections in our data. I was hoping you could help track these down. The data I am referring to is `datastore/derived/elections/municipal_elections_2000_2020.dta`. For example, IBGE municipality code 1100031 is missing the 2012 election. I was able to locate 891 municipality codes missing data. You can find the code that locates the missing data in `source/analysis/party_members/check_close_elections.do`. Please let me know if you have questions. This takes precedence over the issue #23.",
            "number": 24,
            "repository": "MaxMillerLab/peps",
            "title": "Track down missing elections data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/24"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0InM",
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Track down missing elections data"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "freddypinzon"
          ],
          "content": {
            "body": "@freddypinzon can you please go through the code `source/analysis/peps/sample_code_did_preselection_peps.do` and fill in the missing lines. Also, be sure to put the output in the `SConscript` file in `source/analysis`. I will go through when you're done to check it out.",
            "number": 21,
            "repository": "MaxMillerLab/peps",
            "title": "Fill in PEPs Presidential Election code",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/21"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0InQ",
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Fill in PEPs Presidential Election code"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "freddypinzon",
            "mounu1"
          ],
          "content": {
            "body": "@freddypinzon we need to check whether the party members RDD matching of investors to elections winners and losers is done correctly. I think that this can be done just using the data in `cvm_datastore/raw/cvm/orig/cvm_party_member_characteristics` and the elections data. Assume that all party member investors are present from 2012-2022 and see how many are matched in each municipality to winners and losers in the elections data. This will give us the upper bound on the number of matches we can have.",
            "number": 22,
            "repository": "MaxMillerLab/peps",
            "title": "Check out match of elections winners and losers to party members",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/22"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0InU",
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Check out match of elections winners and losers to party members"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@spkim1228 can you please update the results report captions to make it so that people can understand what is going on? In fact, I think we just want something simple, almost like a list. \n\nFor example, it could say: \"Outcome variable: returns over the next year for bought securities; treated group: campaign donors and workers; analysis window: 4-years.\"\n\nI'm open to other suggestions. Please try to get this done ASAP. Ideally by EoD tomorrow.",
            "number": 20,
            "repository": "MaxMillerLab/peps",
            "title": "Add captions and clarity to report",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/20"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0InY",
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Add captions and clarity to report"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@spkim1228 were you able to update the report code to add the RDD results and text from Bernardo (the Summary_*.tex files)?",
            "number": 19,
            "repository": "MaxMillerLab/peps",
            "title": "Add RDD results and description text to reports",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/19"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0Inc",
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Add RDD results and description text to reports"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "freddypinzon"
          ],
          "content": {
            "body": "Hey @freddypinzon this is the issue to do what we discussed yesterday. I'm looking for you to connect the analysis codes that @mounu1 attached to the sconscript file. You should also make sure that the output goes to the correct output folder.\r\n\r\nThanks a ton for your help with this!",
            "number": 18,
            "repository": "MaxMillerLab/peps",
            "title": "Add analysis code to scons + connect Mounu scripts to data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/18"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0Ing",
          "labels": [
            "high priority"
          ],
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Add analysis code to scons + connect Mounu scripts to data"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "spkim1228"
          ],
          "content": {
            "body": "@spkim1228 we need to write code that produces a tex report. Here are the steps:\r\n\r\n- [x] Find a suitable tex preamble\r\n- [x] Write code that places single table in tex document\r\n        - Create title for tables\r\n        - Assure caption is present, but does not need to be filled in \r\n        - Assure tex document compiles\r\n- [x] Write code that places single figure in tex document\r\n        - Create title for figures\r\n        - Assure caption is present, but does not need to be filled in \r\n        - Assure tex document compiles\r\n- [x] Loop over figures to create sections, subsections, and subsubsections\r\n        - Sections are PEPs and Party Members\r\n        - Subsections are methodology used: Descriptive Statistics, DiD Event Study, RDD\r\n        - Subsubsections are outcome variables: Value transacted, quantity, future returns\r\n\r\nPlease ask questions whenever confused. This is a very high priority task!",
            "number": 16,
            "repository": "MaxMillerLab/peps",
            "title": "Produce results report",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/16"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0Ink",
          "labels": [
            "high priority"
          ],
          "linked pull requests": [
            "https://github.com/MaxMillerLab/peps/pull/17"
          ],
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Produce results report"
        },
        {
          "content": {
            "body": "Python file to create tex document for tables. Modifications need to be made after new file naming convention.\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n\n## Summary by CodeRabbit\n\n- **New Features**\n\t- Introduced a new script to automate the generation of LaTeX reports from data tables.\n\t- Enhanced reporting capabilities with structured sections and figures integration.\n- **Bug Fixes**\n\t- Improved the handling of data directory paths for more reliable report generation.\n\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->",
            "number": 17,
            "repository": "MaxMillerLab/peps",
            "title": "16 produce results report",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/peps/pull/17"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0Ino",
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "16 produce results report"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "alangenaro"
          ],
          "content": {
            "body": "We need to change the paths in `cvm_raw_data_cleaning.R` to correspond with the new repo structure.",
            "number": 14,
            "repository": "MaxMillerLab/peps",
            "title": "Update paths in CVM cleaning code",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/14"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0Inw",
          "labels": [
            "good first issue"
          ],
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Update paths in CVM cleaning code"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "mounu1"
          ],
          "content": {
            "body": "Create dataset with the dates of all municipal elections and the winning party.",
            "number": 13,
            "repository": "MaxMillerLab/peps",
            "title": "Code election dates dataset",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/13"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0In0",
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Code election dates dataset"
        },
        {
          "assignees": [
            "freddypinzon"
          ],
          "content": {
            "body": "Begin to move the data from the Raw folder to our reorganization/raw folder. When doing this be sure to:\r\n\r\n1. Check that the data is being used in some code file.\r\n2. Pester people about what the data is and be sure that there is adequate documentation that describes what the variables are.\r\n3. A readme describing how the data was obtained and what is does needs to be added to the folder.\r\n\r\nThis will be ongoing, so check in frequently with progress.",
            "number": 2,
            "repository": "MaxMillerLab/peps",
            "title": "Begin moving raw data to reorganization folder: put in folder structure and add readme",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/2"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0In4",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Begin moving raw data to reorganization folder: put in folder structure and add readme"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "siwovalle",
            "freddypinzon"
          ],
          "content": {
            "body": "Start to recode the project in readable way with all files run in single button push using SCons. Please refer to guides on best coding practices. @MaxMillerLab will post these guides soon.",
            "number": 3,
            "repository": "MaxMillerLab/peps",
            "title": "Begin recoding project in new project format",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/3"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0In8",
          "labels": [
            "enhancement"
          ],
          "linked pull requests": [
            "https://github.com/MaxMillerLab/peps/pull/5"
          ],
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Begin recoding project in new project format"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "alangenaro",
            "freddypinzon"
          ],
          "content": {
            "body": "## Overview\r\n\r\nWe want to create event study plots that describes how positions and returns respond to informational treatments (these would be the events data that we did not recode, but should). The trade data cover the period 2012--2020. Alan has created a dataset at the investor-stock-date level, that gives us changes in positions. Alan will send us this code.\r\n\r\n## To do\r\n\r\nWe want to create two event study plots:\r\n1. Changes in net stock positions leading into the informational treatment.\r\n2. Returns from investing in companies receiving the informational treatment.\r\n\r\nWe need to write Stata code that produces these event study plots. We should really on programs to do this as much as possible. These programs that we write should also come with a suite of unit tests that we should discuss. Alan will provide sample code that allow us to do this.",
            "number": 6,
            "repository": "MaxMillerLab/peps",
            "title": "Create the event study plot",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/6"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0IoA",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Create the event study plot"
        },
        {
          "assignees": [
            "siwovalle",
            "freddypinzon"
          ],
          "content": {
            "body": "@freddypinzon or @siwovalle can you help me understand where the sample data that describes PEPs characteristics are? We want the one from the CVM. Ideally we want a dataset that has InvestorFakeID and then a set of characteristics. Where is this?",
            "number": 8,
            "repository": "MaxMillerLab/peps",
            "title": "URGENT: PEPs characteristics sample data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/8"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0IoM",
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "URGENT: PEPs characteristics sample data"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "ecolonnelli"
          ],
          "content": {
            "body": "Hey @ecolonnelli, I've added some documentation around the [goals for output we want to produce](https://github.com/MaxMillerLab/peps/blob/main/docs/project_goals.md) and [outstanding questions about the data](https://github.com/MaxMillerLab/peps/blob/main/docs/project_questions.md). Please add to these anything I'm missing. Happy to show you how to update them.",
            "number": 4,
            "repository": "MaxMillerLab/peps",
            "title": "Add to proposed project scope and outstanding data questions",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/4"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0IoU",
          "labels": [
            "documentation"
          ],
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Add to proposed project scope and outstanding data questions"
        },
        {
          "content": {
            "body": "",
            "number": 7,
            "repository": "MaxMillerLab/peps",
            "title": "3 begin recoding project in new project format",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/peps/pull/7"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0IoY",
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "3 begin recoding project in new project format"
        },
        {
          "content": {
            "body": "",
            "number": 5,
            "repository": "MaxMillerLab/peps",
            "title": "3 begin recoding project in new project format",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/peps/pull/5"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0Iog",
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "3 begin recoding project in new project format"
        },
        {
          "assignees": [
            "freddypinzon"
          ],
          "content": {
            "body": "@freddypinzon you need to work with Booth IT to figure out how to set up a symbolic link from the Github to the dropbox. Remember the symbolic link MUST be called \"datastore\".",
            "number": 1,
            "repository": "MaxMillerLab/peps",
            "title": "Figure out symbolic link",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/peps/issues/1"
          },
          "id": "PVTI_lAHOAkGERM4Ajw1dzgZ0Iok",
          "repository": "https://github.com/MaxMillerLab/peps",
          "title": "Figure out symbolic link"
        }
      ]
    },
    "3": {
      "title": "Emerging markets risk",
      "items": [
        {
          "assignees": [
            "oliverwang266",
            "federicogolonia",
            "SimoneGozzini"
          ],
          "category": "Data cleaning",
          "content": {
            "body": "Provisional (before Monday RA's call) \n\nManual check of report types:\nSimone: Jp Morgan\nFederico: Credit Swiss\n\n",
            "number": 26,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Come up with granular report types",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/26"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgQzTlY",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Come up with granular report types"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "ChernXiangyu"
          ],
          "category": "Data cleaning",
          "content": {
            "body": "Link to OCR software: https://github.com/VikParuchuri/surya",
            "number": 30,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Use Surya to OCR reports",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/30"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgRD_nI",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Use Surya to OCR reports"
        },
        {
          "assignees": [
            "federicogolonia",
            "SimoneGozzini"
          ],
          "content": {
            "body": "Do some research as to what are the key retail investing platforms in all major economies and regions (try to catch new small players in frontier economies if you can, such as [Daba](https://dabafinance.com/en) in francophone africa). all the main players such as robinhood should be there. would be good to have a brief description of each, eg what they allow trading on, how many people are actively trading on their platform, and anything else you find interesting.",
            "number": 31,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Report  about key retail investing platforms in all major economies and regions",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/31"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgRLpfU",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Report  about key retail investing platforms in all major economies and regions"
        },
        {
          "assignees": [
            "federicogolonia",
            "SimoneGozzini"
          ],
          "content": {
            "body": "Search (within thee platforms or outside of them) for sources of text and discussion data to understand how retail investors \"talk\" about investing in certain companies / countries / regions? i can imagine there will be massive reddit communities for this, but i can also imagine there to be specific magazines or other material that can provide similar sources of info/data. maybe even podcasts. and of course it'd be great to also do this not just with a US/europe focus, but also across other key regions/countries in the world. basically the idea is that i'd like to know if we can find a similar source / pool of text-like data to do a similar analysis like the one we are doing, but trying to capture how \"retail\" investors think about EMs and EM risk.",
            "number": 32,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Report about how retail investors \"talk\" about investing in certain companies / countries / regions",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/32"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgRLpqk",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Report about how retail investors \"talk\" about investing in certain companies / countries / regions"
        },
        {
          "assignees": [
            "ChernXiangyu",
            "oliverwang266",
            "federicogolonia",
            "SimoneGozzini"
          ],
          "category": "Data cleaning",
          "content": {
            "body": "Look into LSEG Portal and try to find which reports are missing so that we can have a full sample. Provide a short summary of your thoughts and insights from this search and send it on slack, so that Alex can download the reports we are missing.",
            "number": 35,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "LSEG manual check",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/35"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgROfbk",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "LSEG manual check"
        },
        {
          "assignees": [
            "oliverwang266",
            "federicogolonia",
            "SimoneGozzini"
          ],
          "content": {
            "body": "On political risk and other categories of country risk, would be good to find out what the main measures are, eg the economist intelligence unit and a few others. ideally we can make a list of the top 20 (or more if you think you can easily confirm these are top providers of such measures/services) and see how they compare and start seeing how much variation there is in rating of a given country or country/risk topic across these different providers. this should be a subsection in itself and a very important one at this stage (see Slack Message of Emanuele, 08-02-24, Weekly Updates)",
            "number": 36,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Risk categorization from main providers",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/36"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgRcfUs",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Risk categorization from main providers"
        },
        {
          "assignees": [
            "federicogolonia",
            "SimoneGozzini"
          ],
          "content": {
            "body": "but i think it's missing some related papers i had shared in literature that were presented at NBER recently? (So look at the literature channel better and incorporate these papers). \n\none thing to dig into is a bit more on country-risk. Hassan's restud paper with schreger uses conf calls, so can we look for examples like that one that does sth similar to what we want to do, perhaps with text or surveys, and see what's out there with a similar set of search criteria? (See emanuele's comment on clas, 02-08-24 Literature)",
            "number": 37,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Report Literature review - incorporate Emanuele's suggestions",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/37"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgRcfu0",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Report Literature review - incorporate Emanuele's suggestions"
        },
        {
          "assignees": [
            "federicogolonia",
            "SimoneGozzini"
          ],
          "content": {
            "body": "",
            "number": 38,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Find API information and how to get data from retail investors reports",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/38"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgRvyKY",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Find API information and how to get data from retail investors reports"
        },
        {
          "assignees": [
            "oliverwang266",
            "federicogolonia",
            "SimoneGozzini"
          ],
          "content": {
            "body": "Find a list of websites where it is possible to get sell-side analyst reports (what you can find that is not LSEG)",
            "number": 39,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Find a list of websites where it is possible to get sell-side analyst reports",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/39"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgR1n0Y",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Find a list of websites where it is possible to get sell-side analyst reports"
        },
        {
          "assignees": [
            "oliverwang266",
            "federicogolonia",
            "SimoneGozzini"
          ],
          "category": "Data cleaning",
          "content": {
            "body": "Look into Ravenpack and try to understand what it is and to what extent we have access to these data. Download some of them.",
            "number": 40,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Ravenpack",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/40"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgSKJ-M",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Ravenpack"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "category": "Reports",
          "content": {
            "body": "@ChernXiangyu you should use the paper Ema posted on Slack (that you also posted)",
            "number": 53,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Create stereotypes measure from paper",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/53"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgWbCqI",
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Create stereotypes measure from paper"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "ChernXiangyu"
          ],
          "category": "Reports",
          "content": {
            "body": "",
            "number": 54,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Create sentiment measure",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/54"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgWbCtg",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Create sentiment measure"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "ChernXiangyu"
          ],
          "category": "Reports",
          "content": {
            "body": "",
            "number": 55,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "OCR new batch of reports",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/55"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgWbCvM",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "OCR new batch of reports"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "ChernXiangyu",
            "oliverwang266",
            "federicogolonia"
          ],
          "category": "Reports",
          "content": {
            "body": "",
            "number": 56,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Parse new batch of reports",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/56"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgWbCwY",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Parse new batch of reports"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "category": "Reports",
          "content": {
            "body": "",
            "number": 57,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Run Suproteem representation thing for stereotypes",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/57"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgWbCx0",
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Run Suproteem representation thing for stereotypes"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "- [x] Oxford\n- [x] PRS\n- [x] IMF\n- [x] EIU\n- [x] World Bank",
            "number": 63,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Link countries to Oxford, PRS, IMF, World Bank reports",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/63"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmLU",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Link countries to Oxford, PRS, IMF, World Bank reports"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "category": "Reports",
          "content": {
            "body": "- [x] LSEG 47k\n- [x] LSEG 104k\n- [x] Oxford\n- [x] PRS\n- [x] EIU\n- [x] IMF\n- [x] World Bank",
            "number": 69,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Parse and OCR PDF reports",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/69"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmLk",
          "labels": [
            "enhancement"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Parse and OCR PDF reports"
        },
        {
          "assignees": [
            "oliverwang266",
            "SimoneGozzini"
          ],
          "category": "Reports",
          "content": {
            "body": "We need to decide whether to collect alternative EIU reports via Factiva.",
            "number": 71,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Collecting alternative EIU Reports on Factiva",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/71"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmLw",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Collecting alternative EIU Reports on Factiva"
        },
        {
          "assignees": [
            "oliverwang266",
            "SimoneGozzini"
          ],
          "category": "Reports",
          "content": {
            "body": "Map the metadata to the GPT generated results:\n\n- with the last 8 digits report id with the column DCN in the metadata\n\n- also use the unique_id to match with specific textbox.",
            "number": 66,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Link LSEG metadata and the GPT results",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/66"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmME",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Link LSEG metadata and the GPT results"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "",
            "number": 60,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Explore the data of size of tradable and non tradable",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/60"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmMQ",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Explore the data of size of tradable and non tradable"
        },
        {
          "assignees": [
            "freddypinzon",
            "ChernXiangyu"
          ],
          "content": {
            "body": "Use the stata code from another project to merge the PitchBook Data from HBS, some column names need to be changed and there are some subtle differences in the data.",
            "number": 67,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Merge PitchBook Data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/67"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmMg",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Merge PitchBook Data"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "",
            "number": 59,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Explore the data of foreign investment by industry in different countries",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/59"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmMs",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Explore the data of foreign investment by industry in different countries"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "",
            "number": 62,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Link companies to headquarter country data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/62"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmNA",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Link companies to headquarter country data"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "We should go through the sentiment measure @ChernXiangyu produced and make sure it looks reasonable. @oliverwang266 And @federicogolonia can you help with this?",
            "number": 64,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Check sentiment measure",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/64"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmNU",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Check sentiment measure"
        },
        {
          "assignees": [
            "oliverwang266",
            "SimoneGozzini"
          ],
          "category": "Reports",
          "content": {
            "body": "Check how chatGPT classifies text blocks based on our risk tree classification.",
            "number": 70,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Checking Risk Designation",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/70"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmNk",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Checking Risk Designation"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "Replace current election variables from V-dem with the presidential and parliamentary elections variable from Natioanl Election database.",
            "number": 68,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Add new election variable",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/68"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmNs",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Add new election variable"
        },
        {
          "assignees": [
            "SimoneGozzini"
          ],
          "category": "Reports",
          "content": {
            "body": "Checking the number of firms and reports by year, comparing them to hassan' s paper.\n\nRelated paper link:\n[Customer Capital](https://faculty.chicagobooth.edu/-/media/faculty/amir-sufi/research/he_mostrom_sufi_customercapital_2025.pdf)\n\n[Economic Surveillance using Corporate Text](https://drive.google.com/file/d/1CTxaSIpA4FVkWPFYrRuvMIn6Ths-SyZm/view)",
            "number": 65,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Check the 10-K and Conference Call data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/65"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmNw",
          "labels": [
            "enhancement"
          ],
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Check the 10-K and Conference Call data"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "",
            "number": 61,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Generate new time stamps for downloading reports",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/61"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmN8",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Generate new time stamps for downloading reports"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "Given that it\u2019s unlikely we can directly download the metadata and tags from PDFs, we\u2019ll need to extract these tags ourselves. The tags we aim to extract include:\r\n\r\n- **Author information**, such as name, phone number, email, etc.\r\n- **Report Type**, but it seems that we mainly focus on Economic Reports so this may not be a problem.\r\n- **Report Series**, which might refer to the series the report belongs to, for example, \"Asian Daily.\"\r\n- **Countries of focus** in the article.\r\n- **Industries of focus** in the article.\r\n- **Companies of focus** in the article.\r\n\r\nHere\u2019s my current implementation plan (we can discuss other ideas in the comments below):\r\n\r\nObviously, we can use GPT to handle the entire task, and the only issues we need to resolve are how to write the prompts and how to parse the output into structured data. We can slice the article and use Marvin to complete the tasks for each slice, then merge the results.\r\n\r\nHowever, using GPT for all articles still presents issues with speed and cost (though, if this isn\u2019t actually a problem, we can go ahead and use it\u2014I\u2019m just thinking out loud). So, similar to the text-cleaning task, besides classification, SpaCy also provides information extraction and NER (Named Entity Recognition) functionalities. We can use GPT to complete the task and then use its output to train a SpaCy model.",
            "number": 46,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Extract important data from text",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/46"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmOE",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Extract important data from text"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "",
            "number": 58,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Explore OECD country bilateral capital position data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/58"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmOI",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Explore OECD country bilateral capital position data"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "Please check the paper Measuring group differences in high-dimensional choices: Method and application to congressional speech(https://scholar.harvard.edu/shapiro/publications/measuring-group-differences-high-dimensional-choices-method-and-application)\r\n of Gentzkow, Matthew, Jesse M. Shapiro, and Matt Taddy. And see if their methodology maybe useful reference for our measure of risk. ",
            "number": 44,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Check the paper of Measuring group difference",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/44"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmOc",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Check the paper of Measuring group difference"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "Download some reports from LSEG, get random samples (of decent size) of all other main categories. ",
            "number": 43,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Download reports in different types and languages",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/43"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmOg",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Download reports in different types and languages"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "Try with previous textual analysis method with new OCR result and see what the risk measure contains.",
            "number": 42,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Create new text-based measure of country risk",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/42"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmOs",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Create new text-based measure of country risk"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "More labels, integrated layout analysis, and new OCR model",
            "number": 52,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Test updated version of Surya",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/52"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmPA",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Test updated version of Surya"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "Test",
            "number": 51,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Create Issue with Slack",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/51"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmPE",
          "labels": [
            "good first issue"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Create Issue with Slack"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "Hi Max,\r\nThe branch 47 is ready to merge, all names are lowercased, and I think you could deleted the derived/data/GFD folder in the main branch, since they are also contained in my branch, and I lowercase their names too. Thanks!",
            "number": 50,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "47 merge financial and political data",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/50"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmPI",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "reviewers": [
            "MaxMillerLab"
          ],
          "title": "47 merge financial and political data"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "We previously used Surya + Tesseract for layout analysis, reading order analysis, and OCR on PDF documents. Now we can clean up the text data according to the new information.\r\n\r\n**Our goal**: To extract the **main body** of a report, which should be readable, with a high density of information and no meaningless paragraphs. It would be even better if we could identify the titles and structure of the document.\r\n\r\nBased on the previous PDF parsing and OCR results, the issues we may need to address are:\r\n- **Column splitting issues** (solved by layout analysis!),\r\n- **Meaningless characters and text** (mostly from figures and tables, which layout analysis can identify),\r\n- **OCR errors** (there don't seem to be many; my plan is to use a spell-checking library to detect error rates and then correct them),\r\n- **Disclaimers and other meaningless text** (this currently appears to be the biggest problem).\r\n\r\n@oliverwang266 hope you recover soon...... If you have time, could you let me know where your previous code for text cleaning is? I'd like to refer to it. You can also share what you've completed so far and your cleaning strategy here (no pressure, you don't even need to reply). If you'd like to continue working on this task, feel free to update the discussion under this issue.\r\n\r\nI'll start with the relatively simple task of cleaning up nonsense characters and OCR errors, aiming to produce a text that is smooth to read. For the disclaimer part, here\u2019s my idea (which Oliver might have already thought of):\r\n\r\n- We previously extracted frequently occurring sentences from 40,000 reports, which I think are quite useful. Now that our data includes not only text but also bounding boxes, we can design an algorithm to check the bboxes in the first/final section of the document, which often contain these sentences, and treat them as disclaimers.\r\n\r\n- Once these disclaimers are identified, we can use SpaCy to perform a binary classification task: whether the bbox (or sentence) is part of a disclaimer or not. I will split the data into training set and testing set, hopefully our model could apply  to millions of reports.\r\n\r\n- Using GPT for classification is, of course, an option. It would definitely be more accurate than relying on \"frequently occurring sentences,\" but it's slower and more expensive. A compromise could be to use GPT to label a large number of bboxes/sentences and let SpaCy learn from it. The GPT toolbox is almost ready, so we could try applying it to this task while also testing and debugging it.",
            "number": 45,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Clean text from parsed pdf data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/45"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmPQ",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Clean text from parsed pdf data"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "Collect the following data and be create readme files in each folder, also be precise about where each source is when finish downloading them. \r\n\r\n- IMF BOP Capital Flows data (Broner et al. (2013) for a good reference)\r\n- IMF import, export, GDP, and unemployment data\r\n- Nominal exchange rates from IFS database\r\n- Stock market returns (MSCI, Datastream, GFD)\r\n- Political risk data from ICRG (Aguiar et al. (2009), Gourio et al. (2015))\r\n- IBES Earning Expectations for both US and International Firms\r\n",
            "number": 48,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Collecting data from multiple sources",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/48"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmPU",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Collecting data from multiple sources"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "We have gathered:\r\n\r\n1. IMF BOP Capital Flows data (Broner et al. (2013) for a good reference)\r\n2. IMF import, export, GDP, and unemployment data\r\n3. Nominal exchange rates from IFS database\r\n4. Stock market returns (MSCI, Datastream, GFD)\r\n5. Political risk data from ICRG (Aguiar et al. (2009), Gourio et al. (2015))\r\n\r\nWe need to merge them into a single panel dataset by country&year.",
            "number": 47,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Merge financial and political data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/47"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmPw",
          "labels": [
            "enhancement"
          ],
          "linked pull requests": [
            "https://github.com/MaxMillerLab/finance_and_dev/pull/49"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Merge financial and political data"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "This is the code of downloading composite index return and dividend/PE Ratio via GFD account. Besides, parsing them into txt format ready to merge. ",
            "number": 49,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "47 merge financial and political data",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/49"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmP4",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "reviewers": [
            "MaxMillerLab"
          ],
          "title": "47 merge financial and political data"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "Please feed text into chatGPT to also try to categorize the text of reports. See how it works to compare a developed market is described compared to an emerging market (feeding a bunch of examples of each). Also pay attention to if indeed emerging markets use a different language, perhaps one that seems to be more subject to stereotypes or a narrative of \"perceived\" risk.",
            "number": 41,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Experiment textual data with ChatGPT",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/41"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmRU",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Experiment textual data with ChatGPT"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "@oliverwang266  Use the metadata and textual analysis tools to extract author information from the reports, and see which analyst moved between banks and may cause potential bias in certain countries/regions.\r\n",
            "number": 33,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Author linkage and potential bias analysis",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/33"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmRk",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Author linkage and potential bias analysis"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n## Summary by CodeRabbit\n\n- **New Features**\n\t- Enhanced text processing and data extraction methods for country risk frequency and risk keywords.\n\t- Introduced new functionalities for generating datasets and word clouds from reports.\n  \n- **Bug Fixes**\n\t- Corrected file path issues across multiple scripts to ensure data is saved in the correct directories.\n\n- **Refactor**\n\t- Updated scripts to use modern libraries like spaCy, NLTK, and Gensim for improved text analysis.\n\n- **Chores**\n\t- Updated `requirements.txt` with new dependencies and specific version requirements.\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->",
            "number": 11,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "3 convert pdf files to txt",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/11"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmR0",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "3 convert pdf files to txt"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "@oliverwang266 Update the textual analysis with all the information we get currently, including periodicity, manually reading, literature review, new methodology on textual analysis, author linkage, etc., into a single report.",
            "number": 34,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Update the textual analysis report",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/34"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmSE",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Update the textual analysis report"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "",
            "number": 29,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Survey ideas and brainstorming",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/29"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmSI",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Survey ideas and brainstorming"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "",
            "number": 28,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Experimental part",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/28"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmSU",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Experimental part"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n\n## Summary by CodeRabbit\n\n- **New Features**\n  - Added functionality to analyze country count data for different banks and regions.\n  - Introduced the ability to extract information from text files within zip archives and determine risk categories using predefined bigrams and synonyms.\n  - Implemented functionality to analyze risks related to different topics and generate a CSV report based on the processed data.\n\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->",
            "number": 25,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "13 create new text based country risk measure",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/25"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmSc",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "reviewers": [
            "MaxMillerLab"
          ],
          "title": "13 create new text based country risk measure"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "Hey @oliverwang266 can you help with the following:\r\n- [x] Update the SConscript file in `source/derived` to contain all dependencies.\r\n- [x] Provide a txt file will all requirements to run the code (e.g. required modules)\r\n- [x] Move the bank zip files to a subfolder\r\n- [x] Move all methodology files into `notes` in the dropbox (when completed send email to Ema, Tim, and I)",
            "number": 9,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Update the SConscript file and do other housekeeping",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/9"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmSo",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Update the SConscript file and do other housekeeping"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "@oliverwang266 \r\n\r\nCan you put together some code that gets summary statistics about the LSEG reports? For example, we want to understand:\r\n1. Which countries are covered in the reports;\r\n2. How the total number of reports look over time;\r\n3. The number of reports by various banks.\r\nWe're going to get the rest of the data soon, so start putting this together and then we'll run again when the new data arrive. I hope all is well and talk soon!",
            "number": 6,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Put together summary statistics about reports",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/6"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmS4",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Put together summary statistics about reports"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "Dear Max,\r\nThis is the pull request for the issue 6, and to merge my branch to the main branch. Thank you so much. \r\n\r\nI will add the files and dependencies to the SConscript file ASAP.\r\n\r\nBest,\r\nOliver",
            "number": 8,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "3 convert pdf files to txt",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/8"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmTI",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "reviewers": [
            "MaxMillerLab"
          ],
          "title": "3 convert pdf files to txt"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "@oliverwang266 can you help me look into some papers that analyze text data to produce risk measures?\r\n\r\nI think these papers are a good place to start:\r\n1. [Firm-level political risk](https://academic.oup.com/qje/article/134/4/2135/5531768?guestAccessKey=6001f6d9-a285-48bb-ad3c-390ef73762ad&login=false)\r\n2. [Earnings call measures](https://academic.oup.com/restud/advance-article/doi/10.1093/restud/rdad080/7243248?utm_source=authortollfreelink&utm_campaign=restud&utm_medium=email&guestAccessKey=38d75aad-25d1-4734-956e-83ed683008bb)\r\n3. [Textual analysis of 10-K risk disclosures](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3313663)\r\n\r\nWe should come up with a list of potential methods that would allow us to do this. Please provide a detailed report about the methods that they use in each paper.",
            "number": 7,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Look into text analysis and risk papers",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/7"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmTQ",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Look into text analysis and risk papers"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "Hey Oliver,\r\n\r\nCan you experiment with some methods of extracting the text in the LSEG pdf files to .txt files? All .txt files should also be stored in zip files but in \"datastore/derived/lseg\" where each investment back should have it's own zipfile. Also, please create a linked issue branch for this. When the task is complete, create a pull request and assign me at the reviewer. Instructions on workflow can be found in this dropbox in \"docs/workflow.md.\" Please let me know if you have questions.\r\n\r\nWarmly,\r\nMax",
            "number": 3,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Convert PDF files to .txt",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/3"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmTc",
          "labels": [
            "enhancement"
          ],
          "linked pull requests": [
            "https://github.com/MaxMillerLab/finance_and_dev/pull/4"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Convert PDF files to .txt"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "",
            "number": 5,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "3 convert pdf files to txt",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/5"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmTs",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "reviewers": [
            "MaxMillerLab"
          ],
          "title": "3 convert pdf files to txt"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "Hi Max,\r\nI am trying with this pull request for you to review my current code convet_pdf.py in source/derive/reports, which can convert pdf file into txt file and zip them within the same name as the raw zip folder. Please take a look. Am I doing the right thing with the pull request? Thank you so much.\r\n\r\nBest,\r\nOliver",
            "number": 4,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "3 convert pdf files to txt",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/4"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmTw",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "reviewers": [
            "MaxMillerLab"
          ],
          "title": "3 convert pdf files to txt"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "Hey Oliver,\r\n\r\nThere are too many PDFs in our orig folders for both LSEG and PRS. I want you to write some code that:\r\n1) Creates a zip file.\r\n2) Iteratively adds PDFs to that zip file.\r\n3) Deletes the PDFs after they have been added to the zip file.\r\n\r\nI do not want this to just zip all PDFs in the folder currently! The reason why is that we will get more PDFs going forward and I do not want to unzip, add, and then rezip each time. You are automating this step.\r\n\r\nREMEMBER: the code for this should be stored in \"source/derived/reports\".\r\n\r\nWarmly,\r\nMax",
            "number": 2,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Too many PDFs in orig",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/2"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmUA",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Too many PDFs in orig"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "Oliver,\r\n\r\nCan you please add and update the readme files for the PRS and LSEG data in \"datastore/raw\"?\r\n\r\nWarmly,\r\nMax",
            "number": 1,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Add readme files to datastore",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/1"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnmUI",
          "labels": [
            "documentation"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Add readme files to datastore"
        },
        {
          "assignees": [
            "oliverwang266"
          ],
          "content": {
            "body": "Clean a report-level metadata that HBS library collected until April.15 and keep the useful columns only.",
            "number": 72,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Clean a report-level metadata",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/72"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZnvGU",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/finance_and_dev/pull/77"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Clean a report-level metadata"
        },
        {
          "assignees": [
            "freddypinzon"
          ],
          "category": "Data cleaning",
          "content": {
            "body": "",
            "number": 73,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Track corporate ownership in ORBIS",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/73"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZxBZI",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Track corporate ownership in ORBIS"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Analysis",
          "content": {
            "body": "I'd like to do two things:\n1. Run results using single-country reports;\n2. Run results using only mentions from companies.\n\nDoing the second requires using the merged Orbis and metadata. I'll then look at both as functions of the foreign capital position.",
            "number": 75,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Run additional metadata results",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/75"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZx8Ug",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "start date": "2025-04-28",
          "status": "Done",
          "title": "Run additional metadata results"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "category": "Reports",
          "content": {
            "body": "Many reports reference multiple companies or countries. We need to identify which sections of each report pertain to specific entities.\n\n- Perform Named Entity Recognition (NER) on each text block.\n- Match the extracted entities to the metadata entities using a link transformer.\n- Apply forward-fill to assign entities to blocks where no entities are detected.",
            "number": 78,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Identify and assign entities in multi-entity reports",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/78"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZykO4",
          "labels": [
            "enhancement"
          ],
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "In progress",
          "title": "Identify and assign entities in multi-entity reports"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "content": {
            "body": "@ChernXiangyu can you try to get one of the small R1 versions of Deepseek working on the grid? It would be great to save some money and use that to check entities.",
            "number": 79,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Try Deepseek on the grid",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/79"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZ5cYo",
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Try Deepseek on the grid"
        },
        {
          "assignees": [
            "suproteemsarkar",
            "ChernXiangyu"
          ],
          "category": "Reports",
          "content": {
            "body": "We aim to transform the OCR output of financial reports into embedding vectors to support potential tasks such as summary statistics and regression, traditional downstream tasks like clustering, semantic search, and topic modeling. There are a few key components that need to be discussed and finalized:\n\n### 1. **Choice of Embedding Model**\n\nWe need to select an appropriate embedding model that balances performance, efficiency, and compatibility with our use case. Here are a few candidates under consideration:\n\n- [[RepresentLM-v1](https://huggingface.co/RepresentLM/RepresentLM-v1)](https://huggingface.co/RepresentLM/RepresentLM-v1): A clean, task-agnostic model trained without look-ahead bias, suitable for general-purpose embedding.\n- [[NV-Embed-v2 (NVIDIA)](https://huggingface.co/nvidia/NV-Embed-v2)](https://huggingface.co/nvidia/NV-Embed-v2): A large, high-performance embedding model with state-of-the-art capabilities.\n\nOther models:\n- [[ModernBERT-base](https://huggingface.co/answerdotai/ModernBERT-base)](https://huggingface.co/answerdotai/ModernBERT-base): A modern BERT-style model that supports longer input sequences (up to 8192 tokens), improving contextual representation for long-form text.\n- [[Jina CLIP v2](https://huggingface.co/jinaai/jina-clip-v2)](https://huggingface.co/jinaai/jina-clip-v2): A multimodal model that can embed both text and images\u2014potentially useful for capturing embedded charts or tables in reports.\n\nWe can refer tp the [[MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard)](https://huggingface.co/spaces/mteb/leaderboard) to evaluate models across a wide range of tasks.\n\n\n### 2. **Handling Long Texts and Chunking Strategy**\n\nDue to input length limitations of most embedding models (typically 256 to 8192 tokens), we need an effective chunking strategy for long OCR documents:\n\n- BERT-based models: typically limited to 512 tokens.\n- Longer-context models can help reduce information loss due to truncation.\n- A **sliding window** approach (e.g., 50% overlap) can help to preserve context across adjacent segments.\n\n\n### 3. **Aggregation Strategy for Report-Level Embeddings**\n\nTo obtain a unified representation of an entire report composed of multiple chunks, we should consider:\n\n- **Simple average** of chunk-level embeddings.\n- **Weighted aggregation**, e.g., by chunk length, confidence score, or position.\n- **No aggregation**, keeping segment-level vectors for more fine-grained analysis.\n\n\n### 4. **Storage Format for Embedding Vectors**\n\nOnce embeddings are generated, we need to choose a storage solution that best fits our scale and usage patterns:\n\n- **Chroma**: Convenient and supports persistent storage to disk. Easy to use but relatively slow for large-scale retrieval.\n- **Faiss**: Fast and powerful for similarity search, but works in-memory. Ideal for server-side usage where memory is sufficient.\n- **Parquet (batch saving via pandas/dask, recommended for saving in the first place) + Numpy**: Flexible (could change distance measurement easily), easy to use, and accurate. Supports scalable batch processing, and avoids the approximation errors that come with vector databases using ANN (Approximate Nearest Neighbor) techniques.",
            "number": 80,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Convert OCR report text to embeddings",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/80"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZ6dp4",
          "labels": [
            "enhancement"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "In progress",
          "title": "Convert OCR report text to embeddings"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "oliverwang266"
          ],
          "category": "Reports",
          "content": {
            "body": "After match with Orbis Identifiers, there are still some unmatched tickers from metadata, we will then match them using other data sources.",
            "number": 81,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Match the remaining tickers from the metadata to other sources",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/81"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZ9IWc",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/finance_and_dev/pull/87"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Match the remaining tickers from the metadata to other sources"
        },
        {
          "assignees": [
            "ChernXiangyu",
            "oliverwang266"
          ],
          "content": {
            "body": "@oliverwang266 and @ChernXiangyu I would like to start producing the results for the reports we can assign to entities easily. To this end, can we please create a report-level measure of specificity, sentiment, and risk categories for all of our \"single-entity\" reports. A report has a single entity if:\n1. There is only one ticker present in the metadata;\n2. There is only one country present in the metadata.\n\nFor case (1), please merge in the country and drop the primary ticker. This way we have a report-country unit of observation. Then provide me a dataset with:\n- Country information, \n- Specificity scores, \n- Sentiment scores, \n- Risk categories present, \n\nAlso, merge with the metadata so that I can have:\n- Bank information,\n- Author information,\n- The two author reliability scores,\n- Page length.\n\nThis is a high-priority task. Please put it ahead of other tasks you are working on.",
            "number": 82,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Create report-level measures for single-entity reports",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/82"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZ_oe0",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/finance_and_dev/pull/86"
          ],
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Create report-level measures for single-entity reports"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Analysis",
          "content": {
            "body": "Model idea to make the paper a little more relevant for the present moment: talk about how changes in the capital account from tariffs impact the production of information.\n\nWe could also do a piece of this in the data looking at blended tariff rates in the production of information in the past. Probably not super relevant to the US, but would let us speak to how some emerging markets will be affected.",
            "id": "DI_lAHOAkGERM4AjwvrzgJblyA",
            "title": "Map change in capital account from tariffs to information",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgZ_vTg",
          "priority": "Low",
          "status": "Ideas",
          "title": "Map change in capital account from tariffs to information"
        },
        {
          "assignees": [
            "freddypinzon",
            "oliverwang266"
          ],
          "category": "Reports",
          "content": {
            "body": "@oliverwang266 I need you to do some cleaning on `matched_ticker_metadata.parquet` and merge in additional info.\n\n- [x] **Cleaning**: this file does not adhere to our column names style, namely, all lowercase with \"_\" used as a separator. Can you please put the file in this format?\n- [x] **Merging**: can you please merge in the ISO3 code for the matched BVDID?\n- [x] **Other things**: we might need some other data from Orbis. Could you please save some of the financials, ownership data, etc for the matched BVDIDs that we have? Coordinate with @freddypinzon for this. \n \nPlease do the the first two items first as they will be fast. We can split off this third item to its own issue if needed. The first two items have equal priority to #82.",
            "number": 83,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Clean up matched_ticker_metadata.parquet",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/83"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgaA8Do",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/finance_and_dev/pull/94"
          ],
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "target end date": "2025-05-15",
          "title": "Clean up matched_ticker_metadata.parquet"
        },
        {
          "assignees": [
            "oliverwang266"
          ],
          "category": "Reports",
          "content": {
            "body": "Our goal for this issue is to link the reports from following sources to country information.\n\n- [ ] World Bank\n- [x] IMF\n- [x] Oxford Economics\n- [x] Political Risk Service\n- [x] EIU\n\n",
            "number": 84,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Link non-LSEG reports to country info",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/84"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgaLDss",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/finance_and_dev/pull/98"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "target end date": "2025-06-20",
          "title": "Link non-LSEG reports to country info"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "category": "Reports",
          "content": {
            "body": "EIU, IMF, Oxford, and PRS reports have been parsed but do not yet have sentiment and specificity scores. \nWorld Bank reports will be processed similarly once parsing is complete.\n\nModels we are using:\n- gtfintechlab/SubjECTiveQA-SPECIFIC: A FinBERT-based model for classifying the specificity of financial content. Outputs probabilities for low, neutral, and high specificity.\n- ProsusAI/finbert: A sentiment analysis model trained on Financial PhraseBank. Outputs probabilities for positive, neutral, and negative sentiment.\n\nNotes:\n- We will save the probability scores of three categories from each model.\n- For long texts, we will apply a sliding window approach with max_length=512 and stride=128.\n- Computation will run on the HBS Research Computing Platform using 4 NVIDIA L4 GPUs with DataParallel.\n",
            "number": 85,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Compute sentiment and specificity for non-LSEG reports",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/85"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgaLL_8",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Compute sentiment and specificity for non-LSEG reports"
        },
        {
          "content": {
            "body": "Hi @MaxMillerLab ,\r\nThis branch has the code of cleaning up single entity and multiple entity from metadata, and it is ready to merge~ Thanks",
            "number": 86,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Upload code of cleaning single entity and multiple entity metadata",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/86"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgaSQ-o",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "reviewers": [
            "MaxMillerLab"
          ],
          "status": "Done",
          "title": "Upload code of cleaning single entity and multiple entity metadata"
        },
        {
          "content": {
            "body": "",
            "number": 77,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Upload code of cleaning report level metadata",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/77"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgaSQ-4",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "reviewers": [
            "MaxMillerLab"
          ],
          "status": "Done",
          "title": "Upload code of cleaning report level metadata"
        },
        {
          "assignees": [
            "oliverwang266"
          ],
          "content": {
            "body": "Use crosswalk files from CRSP and Compustat global, add NAICS and tradable/non-tradable to IBES eps data",
            "number": 74,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Add number of analysts and NAICS to eps data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/74"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgaSQ_A",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/finance_and_dev/pull/76"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Add number of analysts and NAICS to eps data"
        },
        {
          "content": {
            "body": "Hi @MaxMillerLab , the code of adding NAICS and number of analyst to IBES are located in `source/derived/data/eps/add_naics_tradable_number_ibes.py`, thanks~",
            "number": 76,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "74 add stuff to eps",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/76"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgaSQ_I",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "reviewers": [
            "MaxMillerLab"
          ],
          "status": "Done",
          "title": "74 add stuff to eps"
        },
        {
          "assignees": [
            "oliverwang266"
          ],
          "category": "Reports",
          "content": {
            "body": "@oliverwang266 the library has gotten back with some tickers! They went through a few exchanges and wanted some feedback before proceeding with the others. \n\nSome questions I have:\n1. Are we able to match ISINs to BVDIDs?\n2. Can we cross-check some of these matches against some of the reports that we have?\n3. Is there other information that might be helpful to have?\n\nThe data they sent are below. Please let me know ASAP because I want them working on this soon. Thanks a ton!\n\n[Tickers_Exchanges_Deduped_05092025.xlsx](https://github.com/user-attachments/files/20189008/Tickers_Exchanges_Deduped_05092025.xlsx)",
            "number": 88,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Check library matched tickers",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/88"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgaVg2k",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "target end date": "2025-05-14",
          "title": "Check library matched tickers"
        },
        {
          "assignees": [
            "freddypinzon"
          ],
          "category": "Data cleaning",
          "content": {
            "body": "@freddypinzon it would be nice to have some documentation on Orbis and Pitchbook.\n\n### Orbis\n- Can you please write something on how the GUO variables are constructed?\n- Can you talk about ownership shares and how they are determined (e.g. direct vs total)\n- Can you describe the independence indicators you use?\n- Can you provide a definition of all variables?\n\n### Pitchbook\n- Can we get a data dictionary?\n- Can we understand what is present in the way of unformatted text data?",
            "number": 89,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Write/find documentation on Pitchbook and Orbis",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/89"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgaVsDI",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Write/find documentation on Pitchbook and Orbis"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Analysis",
          "content": {
            "body": "Currently, all of the results we have are in the form of local project event study plots. We need to rerun things in tables. We should generate tables for many potential specifications.",
            "number": 90,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Create table versions of main results",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/90"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgaYyqM",
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "target end date": "2025-05-30",
          "title": "Create table versions of main results"
        },
        {
          "assignees": [
            "oliverwang266"
          ],
          "content": {
            "body": "@oliverwang266 after your finals are done, I will want your help with writing the data section of the first draft of the paper. We should use a combination of what you have already written, online sources, and our code to do this. We should find some time to meet and discuss when you are ready.",
            "number": 91,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Write data section for paper",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/91"
          },
          "id": "PVTI_lAHOAkGERM4Ajwvrzga4ef4",
          "labels": [
            "documentation"
          ],
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Write data section for paper"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "oliverwang266"
          ],
          "content": {
            "body": "@oliverwang266 I would like your help producing the exhibit README's I mentioned on the call. Shortly, you will see figures appearing on the repo we need to add these for. Please follow [this guide from the lab manual](https://github.com/MaxMillerLab/lab_manual/wiki/Exhibit-readme-guide).",
            "number": 92,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Create main text figure and table READMEs",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/92"
          },
          "id": "PVTI_lAHOAkGERM4Ajwvrzga4mMk",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Create main text figure and table READMEs"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "category": "Reports",
          "content": {
            "body": "@oliverwang266 and @ChernXiangyu I would like to run NER on the metadata report titles to understand which entity the reports are about. Perhaps we could also use one of the cheaper LLMs too. I am looking into options for this too. I am open to suggestions.",
            "number": 93,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Run NER on metadata report titles",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/93"
          },
          "id": "PVTI_lAHOAkGERM4Ajwvrzga4mhM",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/finance_and_dev/pull/105"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Run NER on metadata report titles"
        },
        {
          "assignees": [
            "oliverwang266"
          ],
          "content": {
            "body": "Closes #83\r\n\r\nI have reviewed the cleaned and merged `matched_ticker_metadata.parquet` file and confirm that the changes are as expected. Column names are standardized, and ownership and financial variables from Orbis have been successfully merged.\r\n\r\n**Purpose:** Merge the guo files and key financials to the matched ticker metadata\r\n\r\n**Estimated review time:** 15\u201320 minutes\r\n\r\n**Multiple files changed:** This PR involves the merge of multiple datasets:\r\n- Orbis ownership files (2007\u20132022)\r\n- Key financials files across multiple years\r\n- All new variables are clearly named and merged consistently with BVDID and year keys\r\n- Reviewer should verify merged column presence and formats; no figures or charts are affected\r\n\r\nNo merge conflicts encountered.  \r\nAll code for merging and transformation is in the issue-specific branch: `issue_083_clean_up_matched_ticker_metadata`.\r\n\r\nPlease refer to the markdown deliverable for full details:  \r\n[issue_083_clean_up_matched_ticker_metadata.md](https://github.com/user-attachments/files/20537823/issue_083_clean_up_matched_ticker_metadata.md)\r\n\r\n\r\n",
            "number": 94,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "83 clean up matched ticker metadata",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/94"
          },
          "id": "PVTI_lAHOAkGERM4Ajwvrzga_934",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "reviewers": [
            "MaxMillerLab"
          ],
          "status": "Done",
          "title": "83 clean up matched ticker metadata"
        },
        {
          "assignees": [
            "oliverwang266"
          ],
          "content": {
            "body": "@oliverwang266 could you please update the metadata with everything we have when you get a chance? \n\nThis just means rerunning the existing code to bring in all the new metadata Victor has downloaded.\n\nThanks a ton!\n\n",
            "number": 96,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Update metadata",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/96"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgblVVw",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/finance_and_dev/pull/100"
          ],
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Update metadata"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "oliverwang266"
          ],
          "content": {
            "body": "## Summary\n\nThis issue tracks the implementation of a contributor name standardization script that maps financial institution subsidiaries and divisions to their parent organizations.\n\n## What was done\n\n1. **Analyzed the data**: Examined 13,969,750 rows with 1,241 unique contributors in `report_level_metadata.pq`\n\n2. **Identified patterns**: Found common naming patterns including:\n   - Division suffixes (Fixed Income, Equity Research, Economics, etc.)\n   - Regional suffixes (North America, Europe, Asia, Japan)\n   - Corporate suffixes (LLC, Ltd, Inc, Corporation, etc.)\n   - Historical markers\n\n3. **Created standardization logic**: \n   - Mapped major financial institutions and their variations (e.g., JPMorgan Econ & FI \u2192 JPMorgan)\n   - Handled mergers and acquisitions (e.g., TD Cowen \u2192 TD Securities)\n   - Removed common suffixes while preserving meaningful company names\n\n4. **Implemented comprehensive unit tests** covering:\n   - Exact matches\n   - Subsidiary mappings\n   - Suffix removal\n   - Historical markers\n   - Edge cases\n\n## Results\n\n- **Original unique contributors**: 1,241\n- **Standardized unique contributors**: 1,198\n- **Reduction**: 43 duplicate variations removed (3.5%)\n\n### Top standardizations by volume:\n- BofA Global Research \u2192 Bank of America (759,341 records)\n- UBS Equities \u2192 UBS (587,930 records)\n- RBC Capital Markets \u2192 RBC (340,793 records)\n- Deutsche Bank Equity Research \u2192 Deutsche Bank (196,133 records)\n- BMO Capital Markets \u2192 BMO (164,126 records)\n- TD Cowen \u2192 TD Securities (146,814 records)\n- JPMorgan Econ & FI \u2192 JPMorgan (116,118 records)\n\n## Output files\n\n- `datastore/derived/report_level_metadata_standardized.pq` - Full dataset with standardized_contributor column\n- `datastore/derived/contributor_standardization_summary.csv` - Summary of all standardizations with counts\n\n## Script location\n\n`source/derived/data/metadata/standardize_contributor_names.py`",
            "number": 99,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Standardize contributor names",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/99"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgbxKnw",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "target end date": "2025-06-24",
          "title": "Standardize contributor names"
        },
        {
          "assignees": [
            "oliverwang266"
          ],
          "category": "Data cleaning",
          "content": {
            "body": "@oliverwang266 create readme files for how to process BOP and IIP data, as well as GCAP data. Also, shed lights on how to construct the measure of cod, cif. ",
            "number": 101,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Create README for BOP and IIP",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/101"
          },
          "id": "PVTI_lAHOAkGERM4Ajwvrzgb-JBo",
          "labels": [
            "documentation"
          ],
          "linked pull requests": [
            "https://github.com/MaxMillerLab/finance_and_dev/pull/104"
          ],
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Create README for BOP and IIP"
        },
        {
          "content": {
            "body": "## Summary\n\nThis PR reorganizes the quarterly data processing pipeline by renumbering scripts to create a more logical sequence and cleaning up deprecated files.\n\n## Changes Made\n\n- **Renumbered data processing scripts** to create logical flow:\n  - `08_bilateral_portfolio_shares.do` \u2192 `09_bilateral_portfolio_shares.do`\n  - `09_lseg_report_level_and_metadata.do` \u2192 `10_lseg_report_level_and_metadata.do`\n  - `10_create_analysis_data.do` \u2192 `11_create_analysis_data.do`\n  - `11_create_instruments.do` \u2192 `12_create_and_test_instruments.do`\n\n- **Cleaned up deprecated files:**\n  - Moved `09_lasso_select_instruments.py` to `archive/` folder\n  - Removed `10_test_instruments_final.py` (deprecated)\n  - Removed `lasso_instrument_selection.py` (duplicate/unused)\n\n- **Updated references:**\n  - Modified `source/analysis/instruments/bartik/main_text.do` to reflect new script numbering\n\n## Purpose\nThis reorganization improves the clarity and maintainability of the data processing pipeline by establishing a clear numerical sequence for the quarterly data processing steps.",
            "number": 103,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "temp spamming instruments",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/103"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgcDx64",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "temp spamming instruments"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "## Description\nThis issue tracks the implementation of LASSO regression to select the most predictive bilateral portfolio share instruments for each country's capital flows.\n\n## Background\nScript 08 creates Bartik-style instruments based on bilateral portfolio shares, resulting in 360 potential instruments per country (3 types \u00d7 120 issuer countries). This high dimensionality requires a principled selection method.\n\n## Implementation\n- [x] Create Python script using scikit-learn's LassoCV\n- [x] Implement country-by-country LASSO fitting\n- [x] Extract and rank top predictors by coefficient magnitude\n- [x] Generate summary statistics and output files\n- [x] Write comprehensive documentation\n\n## Key Results\n- Successfully identifies most predictive bilateral relationships\n- Average R\u00b2 of 0.856 for IIP flows and 0.728 for BOP flows\n- Australia, UK, and Luxembourg emerge as frequent top predictors\n\n## Files Added\n- `source/derived/data/merging/quarterly/09_lasso_select_instruments.py`\n- `source/derived/data/merging/quarterly/lasso_instrument_selection.py` (helper module)\n- `source/derived/data/merging/quarterly/tests/test_lasso_instrument_selection.py`\n- `docs/lasso_instrument_selection.md`\n- Updated `source/lib/environment.yaml` to include scikit-learn\n\n## Next Steps\n- Consider time-varying instrument selection\n- Explore elastic net as alternative to pure LASSO\n- Add bootstrap confidence intervals for coefficient stability",
            "number": 102,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Add LASSO-based instrument selection for bilateral portfolio shares",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/102"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgcDx68",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "Done",
          "title": "Add LASSO-based instrument selection for bilateral portfolio shares"
        },
        {
          "content": {
            "body": "1. Closes #96 \r\n2. I have reviewed the filesin the pull request and the changes are as I expected, the changes are as follows:\r\n\r\n> delete the previous processing and analysis files upload by me in path: `source/derived/data/metadata/`\r\n> upload full metadata processing code from 01 to 11, following with a README file in the same path.\r\n> The computing for mentions code remain unchanged, which are created by @MaxMillerLab \r\n\r\n3. The purpose of this pull request is to process the raw metadata in HBS project space in excel format, match them to BvD ID number or company name using crosswalk files from all sources.\r\n4. Estimate time for review: 60-90min.\r\n5. Create two main files: `report_level_metadata.parquet` and `matched_ticker_metadata.parquet`, stored in Dropbox\r\n6. No merge conflicts\r\n7. Issue deliverable is stored under `issues/issue_96_update_metadata.md`",
            "number": 100,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "96 update metadata",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/100"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgcDx7A",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "reviewers": [
            "MaxMillerLab"
          ],
          "title": "96 update metadata"
        },
        {
          "content": {
            "body": "- Closes #84\r\n- I have reviewed the file link_world_bank_to_country.py in the pull request and the changes are as I expected\r\n- The purpose of this pull request is to merge to the main branch of code that link reports ner result to iso3.\r\n- Estimate time for review: 10-15min.\r\n- Create two new files, `world_bank_to_iso3.parquet` and `id2iso3.parquet`\r\n- No merge conflicts",
            "number": 98,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Upload python scripts link world bank reports to country",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/98"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgcDx7I",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "reviewers": [
            "MaxMillerLab"
          ],
          "title": "Upload python scripts link world bank reports to country"
        },
        {
          "content": {
            "body": "1) Closes #95\n\n2) I have reviewed the 8 commits in the pull request and the changes are as I expected\n\n3) Purpose: **Replication** - This pull request implements systematic testing and validation of instrumental variables used in capital flows analysis to ensure robust identification and improve reliability of causal inference.\n\n4) Time estimate for review: **30-45 minutes** - The changes include new testing frameworks, instrument variants, and comprehensive validation procedures that require careful review of the statistical methodology.\n\n5) Multiple files are changing because this PR implements a comprehensive instrument testing framework that touches several components:\n   - New `test_instruments` program for systematic F-statistic testing\n   - Enhanced Bartik instrument construction with GCAP-based variants\n   - New trade imbalance instrument variants (BOP-based and GDP-normalized)\n   - Quarterly data merging improvements\n   - Main analysis script updates to incorporate new instruments\n   \n   Please verify that important results (particularly F-statistics and instrument relevance tests) have not meaningfully changed from expected values.\n\n6) No merge conflicts encountered - branch was successfully merged with main.\n\n7) This PR implements a robust framework for instrumental variable validation that will be used across multiple capital flows analyses. The new testing procedures help ensure that our instruments meet the standard relevance criteria (F > 10) required for valid causal inference.",
            "number": 97,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "issue 95 find relevant instruments",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/97"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgcDx7M",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "reviewers": [],
          "title": "issue 95 find relevant instruments"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "## Overview\nThis issue tracks the implementation of systematic testing and validation of instrumental variables used in capital flows analysis.\n\n## Changes Implemented\n\n### 1. Instrument Testing Framework\n- Added a new `test_instruments` program that:\n  - Tests first-stage F-statistics for instrument relevance\n  - Uses configurable F-statistic threshold (default: 10)\n  - Supports various regression specifications (with/without fixed effects and clustering)\n  - Provides clear output indicating whether instruments meet relevance criteria\n\n### 2. Comprehensive Testing of Existing Instruments\n\n#### Regression-based Instruments\n- Added systematic testing for global, west, and non-west regression instruments\n- Tests cover both IIP and BOP measures at different aggregation levels\n- Includes quarterly-level BOP instruments\n\n#### Bartik Instruments\n- Implemented testing for GCAP-based Bartik instruments\n- Tests both west and non-west variants\n- Covers multiple dependent variable specifications (IIP levels, BOP ratios)\n\n#### Trade Imbalance Instruments\n- Enhanced trade imbalance instrument creation with additional variants\n- Added BOP-based imbalance instruments\n- Implemented GDP-normalized versions\n- Comprehensive testing across all instrument variants\n\n### 3. Code Quality Improvements\n- Added `qui` (quiet) modifiers to instrument creation programs for cleaner output\n- Improved organization with clear section headers\n- Added example usage documentation\n\n## Technical Details\n\n### New Instrument Variants Created\n- `_z_west_imbalance_bop` and `_z_non_west_imbalance_bop`\n- `_z_west_imbalance_bop_gdp` and `_z_non_west_imbalance_bop_gdp`\n- Renamed trade imbalance instruments for consistency\n\n### Testing Specifications\n- Fixed effects: `country_id year` or `country_id yq`\n- Clustering: `country_id`\n- F-statistic threshold: 10 (standard in literature)\n\n## Impact\nThis enhancement provides systematic validation of instrumental variables, ensuring robust identification in capital flows regressions and improving the reliability of causal inference.",
            "number": 95,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Find relevant instruments for capital flows",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/95"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgcDx7Q",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/finance_and_dev/pull/97"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "title": "Find relevant instruments for capital flows"
        },
        {
          "content": {
            "body": "Hi @MaxMillerLab ,\r\nThis branch have two code of matching tickers, one is using ORBIS and the other is using other sources like Capital IQ, Ibes,etc. This branch is ready to merge. Thanks for waiting~",
            "number": 87,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "81 match tickers",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/87"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgcDx7U",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "reviewers": [
            "MaxMillerLab"
          ],
          "title": "81 match tickers"
        },
        {
          "assignees": [
            "ChernXiangyu"
          ],
          "category": "Reports",
          "content": {
            "body": "## New reports repo\n\n@ChernXiangyu please move all the report processing code to this new repo. You have also been added to the new project space (please let me know if you haven't). Along with moving things over, I think we should also do some other tasks.\n\n**Create documentation**\nWe should have full documentation that covers the whole reports codebase. \n- Do this with the understanding that others will be filling in for you after you leave.\n- Please abide by the code style guide [here](https://github.com/MaxMillerLab/lab_manual/wiki/Code-style-guide).\n- We also need full documentation for the data too.\n\n**Refactor code**\nAll code should be refactored to make it clear and simple.\n\n**Add unit tests**\nWhere applicable, we should have unit tests. We should also assure that all tests pass prior to running the codebase.",
            "number": 1,
            "repository": "MaxMillerLab/reports",
            "title": "Move report processing code to this repo",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/reports/issues/1"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgcDx94",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/reports",
          "status": "Done",
          "title": "Move report processing code to this repo"
        },
        {
          "assignees": [
            "oliverwang266"
          ],
          "category": "Reports",
          "content": {
            "body": "@oliverwang266 we should move the reports metadata processing code over to this repo. We will also move the data into a new project space at HBS.",
            "number": 2,
            "repository": "MaxMillerLab/reports",
            "title": "Move metadata processing code to this repository",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/reports/issues/2"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgcDx98",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/reports/pull/3"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/reports",
          "status": "Done",
          "title": "Move metadata processing code to this repository"
        },
        {
          "content": {
            "body": "Hi @MaxMillerLab ,\r\nJust push the code and txt file Simone sent in the issue chat to branch 106, ready to merge~",
            "number": 107,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "upload code and timestamps file in txt format",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/107"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgdAKOc",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "reviewers": [
            "MaxMillerLab"
          ],
          "status": "In progress",
          "title": "upload code and timestamps file in txt format"
        },
        {
          "assignees": [
            "oliverwang266",
            "SimoneGozzini"
          ],
          "content": {
            "body": "Generate 250+250 timestamps for both restricted and unrestricted, both at least 10 pages.",
            "number": 106,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Generate new timestamps",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/issues/106"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgdAKOk",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/finance_and_dev/pull/107"
          ],
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "In progress",
          "title": "Generate new timestamps"
        },
        {
          "content": {
            "body": "",
            "number": 105,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "93 run ner on metadata report titles",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/105"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgdAKOo",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "status": "In progress",
          "title": "93 run ner on metadata report titles"
        },
        {
          "content": {
            "body": "This pull request simply upload a readme file for capital flow measure we construct, and delete useless SAS code file.\r\n\r\n",
            "number": 104,
            "repository": "MaxMillerLab/finance_and_dev",
            "title": "Upload README file for BoP and IIP construction",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/finance_and_dev/pull/104"
          },
          "id": "PVTI_lAHOAkGERM4AjwvrzgdAKOs",
          "repository": "https://github.com/MaxMillerLab/finance_and_dev",
          "reviewers": [
            "MaxMillerLab"
          ],
          "status": "In progress",
          "title": "Upload README file for BoP and IIP construction"
        }
      ]
    },
    "2": {
      "title": "Foreign Influence",
      "items": [
        {
          "assignees": [
            "MaxMillerLab",
            "spkim1228"
          ],
          "category": "Legislator",
          "content": {
            "body": "@devmadhavani1 I would like your help to generate a dataset that maps the unique row `id` in to meetings with members of the US media and US university faculty and staff. The datasets you should use to do this are:\r\n1. `datastore/raw/fara/supplemental_statements/data/2000_2021_line_id.pkl` \r\n2. `datastore/derived/fara/supplemental_statements/gpt/pkl/people.pkl`\r\n3. `datastore/derived/fara/supplemental_statements/gpt/pkl/organization.pkl`\r\n\r\nThe script that you should emulate is in `source/derived/fara/supplemental_statements/maps/keyword_meeting_maps.py`. Also, please create a separate script for the moment, and we will integrate it into the main keyword script later and create a **linked issue branch** to work on this. Don't hesitate to reach out with questions, and please update this thread regularly.\r\n\r\nOne last thing: the first time we spoke, we set up the `datastore` for you, correct?\r\n",
            "number": 28,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Create media and university meetings maps",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/28"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgQWJqM",
          "labels": [
            "enhancement"
          ],
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/39"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "start date": "2024-06-17",
          "status": "Done",
          "title": "Create media and university meetings maps"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Firm Level",
          "content": {
            "body": "- [x] Add `hqduns` to `subsidy_nets_parent_map.pkl`\r\n- [x] Find all `duns` associated with a `consolidated_hqduns` (do this through the `hq_duns` number you're adding)\r\n- [x] Use `NETS2020_HQs.txt` to obtain all `duns` associated with an `hqduns` in the `subsidy_parent_map` in each year.\r\n- [x] Merge duns from the above with `NETS2020_Emp`.\r\n- [x] Merge in location information from the `NETS2020_FIPS` file.\r\n- [x] Aggregate employment by each `consolidated_hq_duns` and `year`",
            "number": 34,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Obtain state- and district-level employment numbers for firms",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/34"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgRAHY0",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/40"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Obtain state- and district-level employment numbers for firms"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Firm Level",
          "content": {
            "body": "",
            "number": 38,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Merge subsidies and FARA meetings",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/38"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgRARGQ",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/41"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Merge subsidies and FARA meetings"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "spkim1228"
          ],
          "category": "Legislator",
          "content": {
            "body": "Please include:\r\n\r\n1. Meetings with legislators, agencies, the media, and universities;\r\n2. The date of the meeting;\r\n3. The meeting type;\r\n4. The report id;\r\n5. The registrant id;\r\n6. The merged_column\r\n\r\nYou should create a new script to do this.\r\n\r\nHappy to answer any and all quesitons! Excited to see the output!",
            "number": 35,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Create meetings dataset for Saudi Arabia",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/35"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgRASCk",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/48"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Create meetings dataset for Saudi Arabia"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Firm Level",
          "content": {
            "body": "We can use the new data from Orbis to construct other measures of subsidy efficiency and examine them.",
            "number": 66,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Orbis to examine other subsidy efficiency measures",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/66"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgW-Eaw",
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Orbis to examine other subsidy efficiency measures"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "spkim1228"
          ],
          "category": "Firm Level",
          "content": {
            "body": "Create a mapping of BvDID numbers to NETS Duns numbers",
            "number": 75,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Match NETS establishment data to orbis data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/75"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4E0",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Match NETS establishment data to orbis data"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Firm Level",
          "content": {
            "body": "@spkim1228 please do the following:\n\n- [x] Gather daily returns data for each GVKEY in the firm-level risk data (https://www.firmlevelrisk.com/download)\n- [x] Do name fuzzy matching on the firm-level risk data and NETS.",
            "number": 69,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Match firm-level risk data to NETS and gather returns",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/69"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4E8",
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Match firm-level risk data to NETS and gather returns"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Country Level",
          "content": {
            "body": "Follow-up issue to Bill Keyword GPT. Query Congress website for each bill name. Looking to get bill number and associated congress number of each bill.",
            "number": 78,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Scrape Congress for Bill Information",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/78"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4FE",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Scrape Congress for Bill Information"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Firm Level",
          "content": {
            "body": "Finished a script to convert addresses to latitude and longitude, which will be used on the NETS data as ORBIS has pre-existing columns for lat/long. Will see if matching on coordinate information does any better than other columns.",
            "number": 77,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "NETS Coordinate Information",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/77"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4FM",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "NETS Coordinate Information"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Legislator",
          "content": {
            "body": "Check through Federal Election Commission Data to see if there is a unique identifer usable from the data",
            "number": 79,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Write code to parse through L2 data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/79"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4FY",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Write code to parse through L2 data"
        },
        {
          "assignees": [
            "ArjunChidrawar",
            "spkim1228"
          ],
          "category": "Data Cleaning",
          "content": {
            "body": "@ArjunChidrawar we would like your help tracking down Senators and Representatives with common names and assuring that they are mapped to the correct person. An example of this is Reps. Charlie Wilson and Joe Wilson. Sometimes meetings are mapped to Charlie Wilson when he is not living. Here is the proposed workflow:\r\n1. Identify all congresspeople with the same last name;\r\n2. Devise rules that will separate which case is which in the data.\r\n\r\nScripts that match legislators to meetings are in `source\\derived\\fara\\supplemental_statements` and are in the `maps` and `gpt` folders. The main ones are `legislator_meetings_ppl`, `legislator_meetings_org`, `legistorm_meeting_maps`, `keyword_meeting_map`.\r\n\r\nOther issues will certainly arise. Don't hesitate to reach out as they come up. In fact, questions are encouraged!",
            "number": 60,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Congresspeople with common names correction",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/60"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Fc",
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Congresspeople with common names correction"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Using GPT to check the correctness of meeting maps which were initially created based on keyword match",
            "number": 73,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Check Keyword of Legislator Meetings",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/73"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Fo",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Check Keyword of Legislator Meetings"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Use Perplexity API to find the country of origin of family trusts. Pull sources used to determine country.",
            "number": 74,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Find Country of Origin for Family Trusts",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/74"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Fw",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Find Country of Origin for Family Trusts"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Modify current script finding keywords from bills to use GPT to find bills and corresponding information",
            "number": 76,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Bill Keyword GPT",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/76"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4F0",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Bill Keyword GPT"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@spkim1228 your goal is to write a script that processes and cleans these data.",
            "number": 72,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Business Dynamism Statistics",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/72"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4GA",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Business Dynamism Statistics"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Merge CRSP daily returns to the firm-level risk data.",
            "number": 70,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Gather daily returns",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/70"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4GE",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Gather daily returns"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "",
            "number": 71,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Fuzzy match NETS companies with firm-level risk data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/71"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4GQ",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Fuzzy match NETS companies with firm-level risk data"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@MarcoGrotteria and @lnaaraayanan we should also do the committee switcher regressions in three other ways (all appendix results). They are:\r\n1. Non-January switchers: we should only use the Congresspeople that switch in the middle of a Congress instead of between Congresses. this will rule out changes in foreign policy stances are driving the results.\r\n2. Chairperson changes: we would expect to see a large effect here since the chairperson sets the committee agenda.\r\n3. Switching on committees: we should examine people that switch on to influential committees too. We should also check to see if the effect is symmetric.\r\n\r\nI can handle producing a unified committee switcher dataset and I will leave it to you guys to run the initial regressions. I'll go back through them afterward.",
            "number": 62,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Non-January switchers, chairperson changes, and committee entry",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/62"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4GY",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Non-January switchers, chairperson changes, and committee entry"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@MarcoGrotteria and @lnaaraayanan we need to create the unidentified regression results for foreign aid and tariffs. These will provide evidence of the effectiveness of lobbying in the larger sample, which we will pin down in a better identified way using switchers and exits.\r\n\r\nIn the writing of the paper, we must also be concrete about why we think that an identification strategy is necessary.",
            "number": 61,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Unidentified regression results: foreign aid and tariffs",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/61"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Gc",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Unidentified regression results: foreign aid and tariffs"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@spkim1228 just creating an issue for what we discussed yesterday. Excited to see what you're able to come up with!",
            "number": 68,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Create SQL database for NETS",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/68"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Go",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Create SQL database for NETS"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@MarcoGrotteria and @lnaaraayanan, we will obtain these data from Legistorm soon. We should:\r\n1. Go through the data and see if they have what we need;\r\n2. Locate luxury hotels and vacations spots;\r\n3. See if the most connected legislators are more likely to go to the most luxurious places.",
            "number": 67,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Gift travel and foreign gift",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/67"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Gs",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Gift travel and foreign gift"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "I will generate the employment results at the district level. To do these, I will find employment shares in NETS and redo the employment results using the County Business Patterns data. A similar methodology will allow us to understand the effect on wages and GDP too.",
            "number": 65,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Employment results using Census data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/65"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4G4",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Employment results using Census data"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@MarcoGrotteria and @lnaaraayanan, we should break aid into its constituent parts and match, say, defense aid to the committees that might oversee it. We should do this on the switcher regressions and in the unidentified regressions.",
            "number": 64,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Separate aid types",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/64"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4HA",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Separate aid types"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@spkim1228 can you please create two scripts that locate members of the media and universities. Here is what they should do:\r\n1. The first should use a keyword search for newspapers, media programs, and networks along with universities. To do this, you should edit the code located in `source/derived/fara/supplemental_statements/maps/media_keyword_search.py`. This script code be fine, but I want you to check it.\r\n2. You should repurpose the code in `source/derived/fara/supplemental_statements/maps/legislator_meetings_org.py` and `source/derived/fara/supplemental_statements/maps/legislator_meetings_ppl.py` to search for reporters, newspapers, media programs, and networks along with universities and professors. You might need different scripts for the media and universities. These scripts basically use GPT to search for these objects. We would also want to understand whether the media outlets you find are national or local.\r\n3. The end goal is to create a mapping of rows in the supplemental statements data to the media and universities. These maps should be separate files (i.e. one for the media and another for universities). It should look like the other maps that we have.\r\n\r\nAs always, please let me know if you have questions (in fact, I expect there to be questions). This issue has equal priority with #56.",
            "number": 57,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Identify meetings with members of the media and universities",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/57"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4HU",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Identify meetings with members of the media and universities"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@spkim1228 I am looking to locate lobbyists in the FARA meetings data so we can track which lobbyists (and not just lobbying firm) are connected to politicians and bureaucrats. The steps as I see them are below. Note, this will follow the same logic as the matching done in `source/derived/fara/supplemental_statements/maps/legistorm_meeting_map.py` so I would borrow heavily from that code.\r\n1. Do a fuzzy match of the FARA Attachment B data (`datastore/raw/fara/metadata/orig/FARA_All_ShortForms.csv`) on the data in `/datastore/derived/fara/supplemental_statements/gpt/pkl/people.pkl` and `/datastore/derived/fara/supplemental_statements/gpt/pkl/organization.pkl`.\r\n   a. I would only do the match on lobbyists that work for the firm our meetings data say is organizing the meeting. This means you restrict on `consolidated_fara_id`. It's ok if the years they are working do not match, as these are sometimes a little buggy.\r\n2. Check the fuzzy match results using GPT. This will assure that they are true matches. You will need to ask me for my API key to do this.\r\n3. Create a map like in the legistorm file. We might need to assign IDs to the FARA lobbyists.\r\n\r\nPlease do not hesitate to reach out with questions about this. This should also not take very long, since the logic is already there in the `legistorm_meeting_map.py` code I referenced above. Please give this priority above the other things on your plate.",
            "number": 56,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Locate lobbyists in the FARA meetings data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/56"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4HY",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Locate lobbyists in the FARA meetings data"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "I need to go back through the exits file and assure everything is done correctly. This should then be merged with the meetings data.",
            "number": 63,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Prepare final exits file",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/63"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Ho",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Prepare final exits file"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@spkim1228 can you move over the files that @skarim-hbs refactored to the main branch. The steps are:\r\n1. Assure the refactored code produces the same output as the file on the main branch.\r\n2. Replace that file with the refactored file if it does.\r\n\r\nDoes this make sense? This is lower in priority than all of your current tasks.",
            "number": 59,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Merge Samah refactored files",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/59"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4H0",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Merge Samah refactored files"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Pay special attention to the `source/derived/fara/` subfolder.",
            "number": 25,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Go through the existing code base and make notes of what is confusing",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/25"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4IA",
          "labels": [
            "help wanted"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Go through the existing code base and make notes of what is confusing"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@spkim1228 after completing #56 and #57 please locate all meetings with registrants associated with Saudi Arabia and locate their meetings with legislators, agencies, governors, the media, and universities. Please provide the name of the PDF, the FARA ID (not consolidated FARA ID), the date, the foreign principal name (from the iso3 map file), and the outputs from the legislators, agencies, governors, the media, and universities map files. Please let me know if you have questions!",
            "number": 58,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Prepare meetings data for Saudi Arabia",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/58"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4IM",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Prepare meetings data for Saudi Arabia"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@MarcoGrotteria and @lnaaraayanan can you guys see if it's ok to delete the `01a`, `01b`, `01c`, and `02` files in `source\\derived\\subsidy\\redistricting`? I just want to avoid duplicate files. I did check that the output was similar between those files and my refactored ones, but wasn't extremely careful about it. Could we also remove the files they produce from the datastore? I put the output files in different places and removed intermediate files where possible.",
            "number": 49,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Go through new redistricting code and delete old files if satisfied",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/49"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4IY",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Go through new redistricting code and delete old files if satisfied"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@YuanGao625 we have data on legislator retirements that can be found in `datastore\\raw\\government\\legislators\\exits\\retirements`. Can you go through and manually verify that the retirement year is correct for the data in the `data` folder? Can you also verify that they do not hold elected office after this date? Feel free to make a copy of these files and produce a correct, definitive version.",
            "number": 55,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Manually check retirements data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/55"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4JE",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Manually check retirements data"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@YuanGao625 can you please edit the code that is in `source\\report` to create a `.tex` report with all our results. Currently, all results are in either `output\\figures` or  `output\\tables`. The `employment` regression results will be changed soon, so you can focus on `unidentified` and `redistricting` for now. Thank you so much for your help with this!!",
            "number": 50,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Create results report",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/50"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4JM",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Create results report"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Hi Paul \r\n\r\nApologies for the many issues raised and assigned to you. In the past version of the paper we used to have several plots and descriptive statistics. In particular Figure 4 to 7 and Table 1 and Table C.2 of the attached paper.  \r\n\r\nPlease let us know if you have any questions. We are happy to jump on a call.\r\n\r\nBest\r\nMarco and Lakshmi\r\n\r\n[ssrn-4058658.pdf](https://github.com/user-attachments/files/17063344/ssrn-4058658.pdf)\r\n\r\n",
            "number": 54,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Summary statistics and descriptive plots",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/54"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4JU",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Summary statistics and descriptive plots"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@spkim1228 I need your help adding the ISO3 and firm name information to the spreadsheet that Orbis sent us. I am forwarding the email for this now. We just need to combine two files to do this and insert them into the excel template. They are:  `datastore/derived/subsidy/good_jobs_first/subsidy_iso3_map.pkl` and `datastore/raw/subsidy/good_jobs_first/data/good_jobs_first_subsidy_tracker.pkl`. The hardest part is just putting the information into the template. Thanks a ton!",
            "number": 51,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Prepare firms data to be send to Orbis",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/51"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Jc",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Prepare firms data to be send to Orbis"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Hi Paul, we found a few mistakes in the resignation data. It would be great if you can check them and fix them as soon as possible. \r\n\r\nFor instance, Zell Bryan Miller resigned in 2005 not 2002\r\nPeter G Fitzgerald the same resigned in 2005 not 2002\r\n\r\nAlso could you let us know if you can have a look at the retirement data as well? \r\n\r\nBest\r\nMarco\r\n",
            "number": 52,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Please check the resignation data (and then possibly the retirement data as well) ",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/52"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Jk",
          "labels": [
            "bug"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Please check the resignation data (and then possibly the retirement data as well) "
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Hi Paul \r\n\r\nThere is a file in raw/government/legislators/committees/data. The name is expand_committee_uid_monthly.csv . This file was created by Max. Can you please merge it with the committee files, so that we can recover the committee id ?  We need this urgently (hopefully by tomorrow our morning time) . It should be a very fast job . Thank you ! \r\n\r\nBest\r\nMarco\r\n",
            "number": 53,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Merge expand_committee_uid_monthly.csv with the committee name/identity file",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/53"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Jo",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Merge expand_committee_uid_monthly.csv with the committee name/identity file"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "### Redistricting as a shock to connections\n\nIdea: countries are connected to some politicians through FARA. Suddenly the firms that are from that country are now represented by a connected politician whereas they weren't before. This should lead the newly connected firm to receive more resources. Control groups are either (1) all domestic and foreign firms (with similar characteristics) or (2) all other foreign firms.\n\nGoal: a file with zip-year redistricting\n\n- [x] Need data on redistricted zips",
            "number": 29,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Redistricting instrument",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/29"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Js",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Redistricting instrument"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@YuanGao625 like we discussed, we're hoping you can recode our CapIQ match script and check how \"far downstream\" the matches are. Please keep me updated on your progress!",
            "number": 43,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Check and clean Capital IQ match",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/43"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4J8",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Check and clean Capital IQ match"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n\n## Summary by CodeRabbit\n\n- **New Features**\n\t- Enhanced keyword search capabilities for agency and committee data.\n\t- Added new functions to retrieve dictionaries for agency and committee keywords.\n\t- Improved data processing for meeting maps, focusing on more specific identifiers.\n\n- **Improvements**\n\t- Expanded data handling to include additional filters, specifically for Saudi Arabia-related data.\n\t- Consolidated meeting maps and relevant information into a comprehensive final dataframe.\n\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->",
            "number": 48,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "35 create meetings dataset for saudi arabia -- Dev Update",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/foreign_influence/pull/48"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4KA",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "35 create meetings dataset for saudi arabia -- Dev Update"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "finalized refactoring script to be used in checking file outputs of refactored scripts\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n\n## Summary by CodeRabbit\n\n- **New Features**\n\t- Introduced a new `Refactor` class to standardize and streamline the script refactoring process.\n\t- Users can now easily compare output consistency between original and refactored scripts.\n\n- **Bug Fixes**\n\t- Improved handling of output files to ensure accurate comparison results.\n\n- **Chores**\n\t- Added command-line argument support for user-friendly interactions during the refactoring process.\n\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->",
            "number": 47,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "45 create small refactoring script",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/foreign_influence/pull/47"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4KM",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "reviewers": [
            "MaxMillerLab"
          ],
          "title": "45 create small refactoring script"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Hey @spkim1228 can we create the small refactoring script we discussed? It should have 4 arguments:\r\n\r\n1. Old code location\r\n2. New code location\r\n3. List of old code outputs\r\n4. List of new code outputs\r\n\r\nGoal is to test old against new. Thanks a ton!",
            "number": 45,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Create small refactoring script",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/45"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4KQ",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/47"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Create small refactoring script"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "final draft and changes to close exits\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n## Summary by CodeRabbit\n\n\n- **New Features**\n\t- Introduced new scripts for analyzing legislative data: deaths, elections, resignations, and retirements.\n\t- Enhanced data processing capabilities for documenting and analyzing congressional transitions and electoral outcomes.\n\t- Added a script for comparing two code directories and documenting differences between files.\n\n- **Chores**\n\t- Updated the `.gitignore` file to exclude virtual environment and Overleaf project files for improved version control.\n\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->",
            "number": 46,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "44 build dataset on close exits",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/foreign_influence/pull/46"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4KY",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "44 build dataset on close exits"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@spkim1228 there are 4 conditions that we want to capture in these data. We want House members and Senators that\r\n1. Die while in office;\r\n2. Resign while in office from all public office (e.g. exclude resignations to achieve higher office or office in another branch);\r\n3. Retire at the end of their term;\r\n4. Lose in a \"close election\" to a challenger\r\n\r\nThis requires:\r\n- [x] Creating single, definitive deaths file;\r\n- [x] Going through the resignations file to find all resignations not meant to achieve a higher office;\r\n- [x] Create congressional retirements dataset (Blanes i Vidal et al 2012 AER);\r\n- [x] Close election losses for incumbents (use elections dataset). This requires figuring out the incumbent. This can be done using the `cel`. This has already been mapped to the elections data via the `uID`.",
            "number": 44,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Build dataset on \"close exits\"",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/44"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Kc",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/46"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Build dataset on \"close exits\""
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Here are the steps to produce the foreign aid analysis.\r\n\r\n- [ ] Clean up and document extensively where the data are and what they do\r\n- [ ] Reproduce the committee-uID map\r\n- [ ] @MarcoGrotteria will produce code that merges the legislator meetings with the current committee assignments maps\r\n- [ ] Merge the committee keyword data with the committee data from legislator meetings. Do the legislative aid committee members data.\r\n- [ ] Write the code that runs the analysis.",
            "number": 26,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Foreign aid analysis creation",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/26"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Ko",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Foreign aid analysis creation"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@MarcoGrotteria and @lnaaraayanan I'm upgrading this issue to be more general. I want to go all in on data validation this week so that we can run analysis with full confidence next week. Outlined are the issues I want us to examine. We should discuss all of these on Monday.\r\n\r\n## Foreign principal to ISO mapping\r\n\r\nI think we have a small issue with our foreign principals. I wrote a quick script to check our data against the foreign principal metadata FARA makes available. Here are the results:\r\n\r\n1. ~10% of foreign principal registrant pairs we list are never present of the DoJ (this is probably not 10% of meetings, just in terms of pairs).\r\n2. Another ~10% of the pairs are not matched to an ISO3.\r\n\r\nI think we need to look in to these a little. It's approximately 1,300 cases. I think we can have Yuan and some other people help. Upon review, a lot of it seems like just mistakes in our mapping and some weirdness in the data. I think we can correct fairly quickly. Also, I noted bellow, a few facts can really reduce the search.\r\n\r\n1. A little more of half of registrants since 1999 only have 1 foreign principal over their entire life.\r\n2. This goes to 77% when we restrict to registrant-years (i.e. some registrants have 1 foreign principal in a year, but more in subsequent years).\r\n\r\nHow do you feel about using 1 and 2 to check the data? My issue with 2 is that we have some evidence of foreign principals being present in the data prior to FARA documenting them. I think we need to manually check a few of these cases to see. Again, we should discuss on Monday.\r\n\r\n## Number of meetings\r\n\r\nAnother issue is that we have many legislators and agencies with a ton of meetings in our data in particular years and months. I'm fine if these are accurate, but I think many of these observations are indicative of \"bad matches\". I think we should spend some time running this down too. This issues main issues are likely due to:\r\n\r\nOk this is complete, so I will merge the branch in. A few things to note before we run regressions (I also note this in the code):\r\n1. Legislator meetings are incorrectly defined.\r\n\t- Some of the keywords that we use may be too general;\r\n\t- Some of the legislative aid matches may be incorrect.\r\n2. Agency meetings are incorrectly defined.\r\n\t- Some of the keywords that we use may be too general;\r\n\r\nI think we should write a small script that locates these observations with, say over 20-30 in-person meetings in one year. We then need to verify that these observations are actually in-person meetings and that they are matched the way we say they are. These large outliers really throw off the regression results, so we need to hunt them down.\r\n\r\n## Correct matches\r\n\r\nAs we've talked about in the past, there are some legislators and legislative aids with fairly common names and some that come from political families. We probably need to go through these and make sure they are matched to the appropriate person. Some of this is helped by us merging on the years that they were actually serving in office, but more can likely be done on this front.\r\n\r\n## Verifying GPT output\r\n\r\nWe need to have someone go through the GPT output manually and report the false positive and negative rates. We should split this out in cases where we use GPT-3.5 and GPT-4. (NOTE: if I have budget left over around 6/25 I will rerun many of our GPT-3.5 exercises using GPT-4). We should then report these in the paper for interested readers.\r\n\r\n## Recommendations\r\n\r\nIdeally, we would agree on a serious of conditions that should be present in the data (i.e. ceiling on number of meetings, people only matched during their tenure in office, etc.) and write code that tests that these conditions hold. This way, when updating the data, we can be sure if it's fidelity. Also, on this note, we should write unit tests that assure our universal and data cleaning functions behave as we think they should. This will help us automatically detect errors going forward.\r\n\r\n",
            "number": 21,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Data validation and tests",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/21"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4K0",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Data validation and tests"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Creating a second issue so that I can have another linked issue branch that is synced with main.",
            "number": 42,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Merge subsidies and FARA meetings",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/42"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4K4",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Merge subsidies and FARA meetings"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "NETS addresses don't seem to be loading properly into Stata. Figure out what is going on.",
            "number": 33,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Check how NETS addresses are loaded",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/33"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4LI",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Check how NETS addresses are loaded"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n\n## Summary by CodeRabbit\n\n- **New Features**\n\t- Introduced a new module for generating employment data at state and congressional district levels based on subsidy information.\n  \n- **Improvements**\n\t- Enhanced fuzzy matching process for better performance through parallel processing.\n\t- Refactored code for improved organization by using global constants for directory paths, increasing maintainability.\n\n- **Bug Fixes**\n\t- Removed unused variable declarations to streamline the code.\n\n- **Documentation**\n\t- Updated module documentation with current dates for clarity and relevance.\n\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->",
            "number": 41,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "38 merge subsidies and fara meetings",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/foreign_influence/pull/41"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4LM",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "reviewers": [
            "spkim1228"
          ],
          "title": "38 merge subsidies and fara meetings"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Closing this because I finished the issue on a different branch.\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n\n## Summary by CodeRabbit\n\n- **New Features**\n\t- Introduced a new function to retrieve associated data based on specific criteria.\n\t- Enhanced data processing capabilities by merging datasets for improved information association.\n\t\n- **Improvements**\n\t- Streamlined path management across multiple files, improving robustness and clarity.\n\t- Updated the handling of API key paths to enhance code portability and maintainability.\n\n- **Bug Fixes**\n\t- Minor adjustments to path handling to prevent errors associated with string manipulation.\n\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->",
            "number": 40,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "34 obtain state and district level employment numbers for firms",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/foreign_influence/pull/40"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4LU",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "reviewers": [
            "spkim1228"
          ],
          "title": "34 obtain state and district level employment numbers for firms"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@devmadhavani1 the way to officially end this issue is to issue a pull request. I'm doing it here now. I'll kill this branch afterward.\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n## Summary by CodeRabbit\n\n- **New Features**\n  - Introduced keyword search functionality for generating meeting maps specifically for agencies, governors, and legislators.\n  - Implemented a new script for creating address data necessary for geo-coding processes.\n\n- **Refactor**\n  - Enhanced and restructured logic in subsidy-related scripts for improved readability and organization.\n  - Improved file path handling and data file management across various modules.\n\n- **Bug Fixes**\n  - Fixed issues related to function arguments and directory handling to ensure smoother file operations and data processing.\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->",
            "number": 39,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "28 create media and university meetings maps",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/foreign_influence/pull/39"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Lc",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "reviewers": [
            "devmadhavani1"
          ],
          "title": "28 create media and university meetings maps"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n## Summary by CodeRabbit\n\n- **Refactor**\n  - Updated directory paths and global constants for better maintainability and readability across multiple scripts.\n  - Improved path handling using `pathlib` and dynamic root path determination.\n\n- **New Features**\n  - Enhanced GPT model versions to \"gpt-4o-2024-05-13\" for various functions, improving model performance and accuracy.\n\n- **Bug Fixes**\n  - Fixed inconsistencies in file paths and direct calls to enhance code reliability and robustness.\n\n- **Chores**\n  - Standardized import structures and centralized global constants for directory paths to streamline the codebase.\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->",
            "number": 37,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "30 update subsidies tables",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/foreign_influence/pull/37"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Lg",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "reviewers": [
            "spkim1228"
          ],
          "title": "30 update subsidies tables"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "- [x] Re-run foreign principal types code on new foreign principal matches\n- [x] Rerun other maps in using GPT-4o\n- [ ] Obtain state- and district-level employment numbers for firms\n- [ ] Merge subsidies and FARA meetings\n- [ ] Assure there is unique company variable\n- [ ] Match LDA clients with Good Jobs First subsidies",
            "number": 30,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Update subsidies tables",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/30"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Ls",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Update subsidies tables"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "\n\n<!-- This is an auto-generated comment: release notes by coderabbit.ai -->\n\n## Summary by CodeRabbit\n\n- **Refactor**\n  - Enhanced file path handling across multiple scripts using `pathlib`.\n  - Updated import statements and path definitions to improve portability and maintainability.\n\n- **New Features**\n  - Introduced `subsidy_nets_map_with_location.py` for generating location-enriched subsidy data.\n\n<!-- end of auto-generated comment: release notes by coderabbit.ai -->",
            "number": 36,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "31 refactor and understand codebase",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/foreign_influence/pull/36"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4L4",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "reviewers": [
            "spkim1228"
          ],
          "title": "31 refactor and understand codebase"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Any time @spkim1228 you see an issue, log it here and we will discuss and debug.",
            "number": 31,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Refactor and understand codebase",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/31"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4L8",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/36"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Refactor and understand codebase"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@YuanGao625 we need your help validating the GPT output. For this, I would like you to take the files in the `datastore/derived/fara/supplemental_statements/gpt/txt` folder and check how well GPT does at the task that it is given. Each of these files has a corresponding code folder in `source/derived/fara/supplemental_statements/gpt`. Ideally, I would like you to make a 2x2 that has \"true positives\", \"false positives\", \"true negatives\", and \"false negatives\". Randomly choose 200 rows for each file you are examining. Please start with the following datasets:\r\n\r\n- [x] people.txt\r\n- [x] oragnizations.txt\r\n- [x] fuzzy_match_check_congressional_aid.txt\r\n- [x] fuzzy_match_check_organization.txt\r\n- [x] fuzzy_match_check_people.txt\r\n- [x] issues_discussed.txt\r\n- [x] legislator_meetings_people.txt\r\n- [x] legislator_meetings_organization.txt\r\n\r\nThanks a ton for your help!!",
            "number": 27,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "GPT output validation",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/27"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4MA",
          "labels": [
            "documentation"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "GPT output validation"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@MarcoGrotteria with then use the GEOCODER to do this.\r\nSteps:\r\n- [x] Find list of DUNS\r\n- [x] Merge address info",
            "number": 32,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Send Marco csv file for all establishments we want latitude and longitude",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/32"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4O0",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Send Marco csv file for all establishments we want latitude and longitude"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "",
            "number": 18,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "17 move foreign aid code to repository",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/foreign_influence/pull/18"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4PA",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "reviewers": [
            "MaxMillerLab"
          ],
          "title": "17 move foreign aid code to repository"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Finished recoding the FARA data. Feel free to check, but I will merge back to main branch. I have verified that the code runs successfully with a \"push of button.\" I will track down the Subsidies code this week.\r\n\r\nThere is one thing missing, which is the potential to use close elections. I have an idea of how to do this in a slightly different way, but we should discuss it on Wednesday.\r\n\r\nI will verify the merge, but you should dig into the code as well. I propose we implement a \"rule\" that anyone can ask anyone else to better comment their code. If you see something that is unclear, please ask me to clarify.",
            "number": 10,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "9 recode all fara code in new project structure",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/foreign_influence/pull/10"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4PE",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "reviewers": [
            "MarcoGrotteria",
            "lnaaraayanan"
          ],
          "title": "9 recode all fara code in new project structure"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Just wrote one script.",
            "number": 22,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "19 merge un security council members to meetings data",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/foreign_influence/pull/22"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4PM",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "reviewers": [
            "MarcoGrotteria"
          ],
          "title": "19 merge un security council members to meetings data"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Scripts ready for integration. I am checking the merge now. Will then update the SConscript.",
            "number": 20,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "14 move all subsidy code to repository",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/foreign_influence/pull/20"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4PY",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "reviewers": [
            "MarcoGrotteria"
          ],
          "title": "14 move all subsidy code to repository"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "",
            "number": 16,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "11 help with 03 doj merge districts to county and county district mapping",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/foreign_influence/pull/16"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Pc",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "reviewers": [
            "MarcoGrotteria"
          ],
          "title": "11 help with 03 doj merge districts to county and county district mapping"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@YuanGao625 could you please help with writing a script that detects anomalous observations in the data. For this, please focus on the four following data files located in `datastore/derived/fara/meetings`:\r\n1. `legislator_consolidated_fara_id_monthly.dta`\r\n2. `agency_consolidated_fara_id_monthly.dta`\r\n3. `committee_consolidated_fara_id_monthly.dta`\r\n4. `governor_consolidated_fara_id_monthly.dta`\r\n\r\n## What to do\r\n\r\nWrite some code (in `source/derived/fara/meetings`) that automatically detects anomalous observations. Think about this. There are many ways one could do this. The more rows we identify, the better. Do this for in-person, unclassified, emails, and phone calls.",
            "number": 23,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Detect issues with the meetings data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/23"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Po",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Detect issues with the meetings data"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@lnaaraayanan can you please look into data on foreign firm nonprofit donations? Within the new structure of the data, this means that the raw data should be stored in raw/donations/foreign_firms. Please follow the file structure for raw data detailed [here](https://github.com/MaxMillerLab/foreign_influence/blob/main/docs/storage.md).\r\n\r\nSince this is a data only issue, you do not need to have a linked issue branch for this. For workflow tips using issues, you can see [this readme](https://github.com/MaxMillerLab/foreign_influence/blob/main/docs/workflow.md).",
            "number": 12,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Find data on foreign firm nonprofit donations",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/12"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Pw",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Find data on foreign firm nonprofit donations"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@YuanGao625 *THIS TASK TAKES PRIORITY OVER ALL YOUR OTHER TASKS*\r\n\r\nI need your help manually matching ~690 foreign principals to their counterpart in the FARA metadata. There are three files you need to do this, all located in `datastore/derived/fara/supplemental_statements/maps/manual_check/foreign_principal_map`:\r\n1. `unmatched_fp_possibilities.csv`: this is the file you are matching to.  It contains the \"truth\" of all foreign principals lobbying under FARA.\r\n2. `foreign_principal_to_iso3_map_issues_checked.csv`: this is the main file that you need to match. There are two columns:\r\n    - fara_id: registrant ID in the metadata\r\n    - lower_fp: the foreign principal name as presented in our raw data\r\n    - NOTE: I have filled in the first few observations for you to start\r\n    -  Please find the correct \"foreign_principal\" for each row in `unmatched_fp_possibilities.csv`\r\n    - Sometimes two or more foreign principals are present in a row. If that is the case, duplicate the row and create a separate observation for each foreign principal.\r\n3. `unmatched_fp_meetings.csv`: if you are having trouble figuring out who the foreign principal is, please use the information in this file. This contains the meeting text, registrant name, etc. Use the context where other context fails.\r\n\r\nDo not hestitate to reach out with questions. I am happy to fill in any understanding gaps. \r\n",
            "number": 24,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "PRIORITY: Manually matching foreign principals to FARA metadata",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/24"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4P4",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "PRIORITY: Manually matching foreign principals to FARA metadata"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@MarcoGrotteria can you please look into data on foreign government nonprofit donations? Within the new structure of the data, this means that the raw data should be stored in raw/donations/foreign_governments. Please follow the file structure for raw data detailed [here](https://github.com/MaxMillerLab/foreign_influence/blob/main/docs/storage.md).\r\n\r\nSince this is a data only issue, you do not need to have a linked issue branch for this. For workflow tips using issues, you can see [this readme](https://github.com/MaxMillerLab/foreign_influence/blob/main/docs/workflow.md).",
            "number": 13,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Find data on foreign government nonprofit donations",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/13"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4P8",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Find data on foreign government nonprofit donations"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Please do the following:\r\n\r\n1. Merge the uID with the CEL data. However, there will some overlapping observations between House and Senate.\r\n2. To solve this, we need to build a year-month dataset for all UIDs of **when they are actually serving and in what branch**. \r\n3. Example: a house member elected in 2000 serves from January 2001 to January 2003. We want a variable that accounts for the months they were actually serving. The ideal dataset would have uid, house (indicator equal to 1 if legislator is in the house), year, month, election year, and congress (the number of the congress that they are serving in).\r\n4. Some of this information will be in the \"congressional_committees\" folder which you should also bring into the datastore.",
            "number": 4,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Bring in CEL, Congressional Committees, and MIT elections data to datastore and build year-month UID dataset",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/4"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4QE",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Bring in CEL, Congressional Committees, and MIT elections data to datastore and build year-month UID dataset"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Hi Max, I have just created a file \"foreign_influence/raw/un_security_council/data/security_council_member_states.dta\". The file has the ISO3 of all states that were members of UN security council by year from 2000 to 2018 (please let me know if you want me to extend it in any direction as I already downloaded all data from the 40s).  \r\n\r\nThis paper suggests a relation with foreign aid. \r\nhttps://www.hbs.edu/ris/Publication%20Files/JPE_How%20much%20is%20a%20seat%20on%20the%20UN%20Security%20Council%20Worth_efeb37bc-98d4-4e18-b076-29450aea72bb.pdf \r\n\r\nI just want to check whether this related to our meeting variable (and eventually to quid pro quo) . \r\n\r\nPlease remember after you merge in these data to replace the variable \"UN_security_council_member\" to zero for missing observations between 2000 and 2018.  In fact, I created a variable \"UN_security_council_member\" which is always equal to 1 in the dataset. \r\n\r\nThansk ! ",
            "number": 19,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Merge UN Security Council members to meetings data ",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/19"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4QQ",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/22"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Merge UN Security Council members to meetings data "
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Move all subsidy code over to this repository. This involves creating separate scripts that:\r\n\r\n- [x] Find the correct county for each subsidy from Good Jobs First\r\n- [x] Create a mapping of parent company countries to ISO3 code\r\n- [x] Create a subsidy type map\r\n- [x] Create a subsidy agency map\r\n- [x] Moves the subsidy data to a Stata format\r\n- [x] Matches the Good Jobs First data with NETS\r\n- [x] Merges Good Jobs First and NETS\r\n- [x] Merges parent company information\r\n- [x] Subsets NETS data for merging\r\n- [x] Creates the balanced panel\n",
            "number": 14,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Move all subsidy code to repository",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/14"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Qk",
          "labels": [
            "enhancement"
          ],
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/20"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Move all subsidy code to repository"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@MarcoGrotteria and @lnaaraayanan we should move the foreign aid code to the repository. We should bring the instruments code in too. I have created a new directory called \"foreign_aid\" in derived which contains the original Jupyter notebook that I think I wrote the code in. Feel free to use that as a starting point for the code. I'm also happy to recode this in a .py file. Regardless, we should delete this file after we've recoded it. Here is what we should do as a checklist:\r\n\r\n- [x] Recode Jupyter notebook in \"foreign_aid\" directory\r\n- [ ] Add recoded foreign aid instruments code to directory",
            "number": 17,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Move foreign aid code to repository",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/17"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Qs",
          "labels": [
            "enhancement"
          ],
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/18"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Move foreign aid code to repository"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Hi Max \r\n\r\n1)  is cd_county.dta a raw file? did we create it ? We need to integrate that part as well. For now I just add it to datasource , but I don't think it's correct\r\n\r\n2) The current cd_county is different from the one we had before. For instance, before we were splitting fips 2232 in 2 based on the following\r\n\r\n\"* Hoonah-Angoon Census Area, AK (FIPS code = 02105). In 2007, Skagway-Hoonah- Angoon Census Area (FIPS code = 02232) was split into Hoonah-Angoon Census Area and Skagway Municipality (FIPS code = 02230). Hoonah-Angoon Census Area has a category code in the 2013 and 2006 NCHS schemes, but not in the 1990 census-based scheme.\"\r\n\r\nNow you  have already created those 2 fips so the code related to this has now become useless. However, the 2 new fips in your code start in 2005, while the source I found is clear that 02232 should still exist until 2006 , so those 2 should start in 2007.\r\n",
            "number": 15,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "11_help_with_03_merge_district_to_county-and-county-district-mapping",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/15"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Qw",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "11_help_with_03_merge_district_to_county-and-county-district-mapping"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Hey @MarcoGrotteria... first issue... let's give this a try.\r\n\r\nI'm going through the 03_doj_merge_districts_to_county.do file you wrote a while back. You make a lot of manual edits in the FIPS to congressional district mapping. This seems like something we should apply to the main code when we create the FIPS to district mapping. Am I correct in thinking this? If so, would you mind:\r\n\r\n1. Separating those manual edits from 03_doj_merge_districts_to_county;\r\n2. Incorporating them at the end of source/derived/geography/district_county_map.do\r\n\r\nIf I'm off based, disregard and I'll close this.",
            "number": 11,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Help with 03_doj_merge_districts_to_county and county-district mapping",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/11"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Q8",
          "labels": [
            "enhancement"
          ],
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/16"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Help with 03_doj_merge_districts_to_county and county-district mapping"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Please:\r\n1. Recode all FARA code from the previous build in new structure and add to the repository.\r\n2. Please add all relevant data files to the datastore.",
            "number": 9,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Recode all FARA code in new project structure",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/9"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4RA",
          "labels": [
            "enhancement"
          ],
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/10"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Recode all FARA code in new project structure"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "All files save the LDA match have been recoded. Some minor file name and locaton changes may need to be made, but this task is essentially complete.",
            "number": 8,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "7 create code for all gpt api requests in derived",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/foreign_influence/pull/8"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4RI",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "7 create code for all gpt api requests in derived"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Please update the source/derived/fara/supplemental_statements/gpt folder to include all prompts and code from previous jupyter notebook.",
            "number": 7,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Create code for all GPT API requests in derived",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/7"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4RQ",
          "labels": [
            "enhancement"
          ],
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/8"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Create code for all GPT API requests in derived"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Added symbolic link to local directory and name of symbolic link to ignored folders list.",
            "number": 6,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Added api_keys to .gitignore",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/foreign_influence/pull/6"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4RY",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Added api_keys to .gitignore"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Add an api_keys directory via a symbolic link. This involves changing .gitignore.",
            "number": 5,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Add api_keys symbolic link",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/5"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Rg",
          "labels": [
            "enhancement"
          ],
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/6"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Add api_keys symbolic link"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Hey Yuan,\r\n\r\nPlease fill in the markdown file located here: datastore/raw/fara/pdfs/readme.md\r\n\r\nGo to the [FARA webpage](https://www.justice.gov/nsd-fara) and look up what information is contained in each of the different kinds of reports. There are 9 documents in total. Please fill in the information you find in the markdown file.\r\n\r\nWarmly,\r\nMax",
            "number": 3,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Fill in markdown file for FARA PDF documents",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/3"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4Rw",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Fill in markdown file for FARA PDF documents"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Need to adjust the code just to update. This will also require reading in the zip_file_list at a higher level.",
            "number": 2,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Fix scrape_fara_pdfs.py such that it only updates if there are missing documents",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/2"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4R8",
          "labels": [
            "enhancement"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Fix scrape_fara_pdfs.py such that it only updates if there are missing documents"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Use Chat GPT suggestion.\r\n\r\nimport requests\r\nfrom zipfile import ZipFile, ZIP_DEFLATED\r\n\r\n# List of PDF URLs to download\r\npdf_urls = [\r\n    'http://example.com/file1.pdf',\r\n    'http://example.com/file2.pdf',\r\n    # Add more URLs as needed\r\n]\r\n\r\n# Path to save the ZIP file\r\nzip_file_path = 'downloaded_pdfs.zip'\r\n\r\n# Create a ZIP file and open it in write mode\r\nwith ZipFile(zip_file_path, 'w', ZIP_DEFLATED) as zipf:\r\n    for url in pdf_urls:\r\n        # Extract the PDF file name from the URL\r\n        pdf_name = url.split('/')[-1]\r\n        # Send a GET request to the URL\r\n        response = requests.get(url)\r\n        # Check if the request was successful\r\n        if response.status_code == 200:\r\n            # Write the PDF file to the ZIP\r\n            zipf.writestr(pdf_name, response.content)\r\n        else:\r\n            print(f\"Failed to download {url}\")\r\n\r\nprint(f\"All files have been written to {zip_file_path}\")\r\n",
            "number": 1,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Fix scrape_all_pdfs.py to download into separate Zip files",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/1"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZn4SE",
          "labels": [
            "bug"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "title": "Fix scrape_all_pdfs.py to download into separate Zip files"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Firm Level",
          "content": {
            "body": "@spkim1228 For all subsidies that have do not have a duns number, could you please get the geo code? This is highest priority aside from running the matching code",
            "number": 80,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Get coordinates for Good Jobs First subsidies",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/80"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZtZVA",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Get coordinates for Good Jobs First subsidies"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Firm Level",
          "content": {
            "body": "**Tasks needed:**\n- [ ] Rerun the subsidy-NETS mapping code to use the lat-long matching data\n- [ ] Rerun the redistricting analysis\n- [ ] Rerun the analysis code organizing it in the way described below\n\n**Organization requirements:**\nSubsidies analysis code must be placed in the new format. This means creating three folders:\n1. `main` - main body results\n2. `appendix` - appendix results\n3. `other` - other unreported analysis\n\nThe code that produces the appropriate subsidy results should be placed in these folders.",
            "number": 107,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Refactor subsidies analysis code",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/107"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZxob8",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/109"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "start date": "2025-06-15",
          "status": "Done",
          "title": "Refactor subsidies analysis code"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Firm Level",
          "content": {
            "body": "Currently, we have a result that suggests new firm formation drives the increase in real GDP after the exit of a high-meeting type legislator. We would like one more result that suggests high-lobbying industries in that district are driving the result. This involves::\n1. Looking at the share of LDA firms in each industry;\n2. Looking at the share of foreign firms whose country lobbies in that district.",
            "id": "DI_lAHOAkGERM4AjpRhzgJaOd0",
            "title": "Firm formation results by industry",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZxoko",
          "priority": "High",
          "status": "Backlog",
          "title": "Firm formation results by industry"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Firm Level",
          "content": {
            "body": "The contracts data contains a `duns` number which allows it to be merged with NETS.",
            "number": 83,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Add contracts data to NETS",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/83"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZxpLc",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Add contracts data to NETS"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Firm Level",
          "content": {
            "body": "**Goal:** perform similar analysis on contracts that we did for subsidies.\n\n**Ingredients:**\n- NETS parent data\n- NETS merged with government contracts\n- Meetings data\n\n**Analyses:** \n1. Look at contract amount unidentified;\n2. Use committee switchers;\n3. Use redistricting;\n4. Use congressional exits;\n5. Examine employment;\n6. Examine inefficiency measures from Spenkuch and Teso paper;\n\n**Population:**\n- Foreign firms\n- Lobbying domestic firms for domestic firms hiring FARA lobbyists",
            "number": 98,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Add contracts analysis",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/98"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZxqJ0",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "start date": "2025-07-16",
          "status": "Ready",
          "target completion date": "2025-08-15",
          "title": "Add contracts analysis"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Legislator",
          "content": {
            "body": "@spkim1228 please go to the [FEC website](https://www.fec.gov/) and download all of the contributions data you can find. This includes individual and corporate (and any other contributions) that they might have. Please create a new folder in `datastore/raw/contributions/orig` and place them these. Please include all documentation you can find in the `docs` folder.",
            "number": 81,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Find and download complete FEC data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/81"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZ0om4",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Find and download complete FEC data"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Legislator",
          "content": {
            "body": "@spkim1228 can you please get the coordinates based on the addresses provided in the individual contributions data from the FEC. Please use the ARCGis API to do this.",
            "number": 82,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Geocode individual contributions data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/82"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZ0o3g",
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Geocode individual contributions data"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Legislator",
          "content": {
            "body": "@spkim1228 we should obtain the geocodes for the addresses in L2. Also, we should check if there is information for the addresses of where they work. Then we can merge with NETS.",
            "number": 84,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Geocodes for L2 addresses",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/84"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZ51SM",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "target completion date": "2025-06-10",
          "title": "Geocodes for L2 addresses"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Firm Level",
          "content": {
            "body": "@spkim1228 I am currently creating a appended contract file for all government contracts in the US from 2000--2019. Can we obtain the geocodes for these contracts? The file is in `datastore/raw/contracts/data`. We should obtain them for the primary_place_of_performance. If there is time, we should do the same for the recipient information.\n\n- [x] Primary place of performance\n- [x] Recipient location",
            "number": 85,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Geocodes for contracts",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/85"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZ51jk",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "target completion date": "2025-05-13",
          "title": "Geocodes for contracts"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Constituents",
          "content": {
            "body": "Use congressional district shapefiles in `datastore/raw/geography/shapefiles` for the congressional district boundaries. We should then map all NETS establishments into districts. The function should have flexibility over different years.",
            "number": 86,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Write function that maps geocode to congressional district in each year",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/86"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZ52Fo",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "target completion date": "2025-05-13",
          "title": "Write function that maps geocode to congressional district in each year"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Firm Level",
          "content": {
            "body": "@spkim1228 please find a way to combine the different unique keys in the contracts data in the script `source/derived/contracts/01_append_contracts_data.py.` to create a single universal unique identifier. This will require looking at the raw data a little.",
            "number": 87,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Create unique identifier for contracts",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/87"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZ7s7o",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "target completion date": "2025-05-13",
          "title": "Create unique identifier for contracts"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Firm Level",
          "content": {
            "body": "Approximately 10% of contracts data is not merged with NETS. Using geocode information, we should be able to match these. Once Paul has provided these geocodes, we can proceed with the match.",
            "id": "DI_lAHOAkGERM4AjpRhzgJbLYU",
            "title": "Match missing contracts data to NETS",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZ7y7Q",
          "priority": "Low",
          "status": "Pause",
          "title": "Match missing contracts data to NETS"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Firm Level",
          "content": {
            "body": "Get all associated DUNS for each Orbis-NETS match. This will give the number of \"true\" matches.",
            "number": 88,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "See true number of NETS-Orbis matches by tracing ownership chains",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/88"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZ9LfI",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "See true number of NETS-Orbis matches by tracing ownership chains"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Country Level",
          "content": {
            "body": "It would be good to see if our results hold using close elections. This would entail:\n1. Checking the close elections data to assure it is correct\n2. Running as an RDD on our outcome variables of interest\n3. Running as an RDD-DiD to get event studies",
            "id": "DI_lAHOAkGERM4AjpRhzgJbdI4",
            "title": "Add results using close elections",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZ-pVM",
          "priority": "Low",
          "status": "Pause",
          "title": "Add results using close elections"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Country Level",
          "content": {
            "body": "We start from a processed file produced under the old codebase. We need to track down how that file was created and add that into the new codebase (after refactoring).",
            "id": "DI_lAHOAkGERM4AjpRhzgJbdKA",
            "title": "Find our original tariffs files and move into datastore",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZ-pf8",
          "priority": "Low",
          "status": "Pause",
          "title": "Find our original tariffs files and move into datastore"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "We need to add results that examine how the loss of ability to deliver favors to foreign countries may lead to reductions in registrations among foreign-born citizens. This must wait until the L2 data has been processed.",
            "number": 89,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Add committee exits and vote share",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/89"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZ-pmY",
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Add committee exits and vote share"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "MarcoGrotteria"
          ],
          "category": "Writing",
          "content": {
            "body": "We currently have two versions of the introduction. They need to be merged.",
            "id": "DI_lAHOAkGERM4AjpRhzgJbe40",
            "title": "Merge two candidate introductions",
            "type": "DraftIssue"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgZ-4L0",
          "priority": "High",
          "status": "Backlog",
          "title": "Merge two candidate introductions"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Firm Level",
          "content": {
            "body": "@spkim1228 I would like you to recode `merge_gjf_and_nets.py` to improve the match quality. I think we can use the same basic structure as before. \n- The main difference is that we should merge on distance to/from the subsidy place of performance. I think that this should give us more matches. \n- I think we should match on latitude-longitude rounded to increasingly narrower geographies to do this, much like I do in the latitude-longitude matching code for NETS and Orbis.\n- DO NOT RUN GPT CHECKING YET. I think we should separate matching and checking into two different scripts. I will also want you to update the GPT code using a new class I wrote.\n\nPlease let me know if this makes sense. We can chat about it today. I'm around the office earlier than our meeting if you want to swing by. Just ping me on Slack.",
            "number": 90,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Redo NETS and Good Jobs First merge using distance instead of state",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/90"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgaBNnc",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "target completion date": "2025-05-23",
          "title": "Redo NETS and Good Jobs First merge using distance instead of state"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Firm Level",
          "content": {
            "body": "**Goal:** find parent company country in each year for each company in NETS\n\n**Ingredients:**\n- Match of 25 million establishments in NETS to their Orbis counterpart.\n- All parent company countries for all firms in Orbis.\n\n**Difficulties:**\n- Orbis data are from 2007--2022\n- There are gaps in parent company information\n\n**Solutions:**\n- Forward fill observations missing observations in the middle\n- Backfill data from 2007\n- Forward fill if last observation year is before 2020",
            "number": 92,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Merge in Orbis ultimate parent info for all matches",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/92"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgaSQeY",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/97"
          ],
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "target completion date": "2025-05-24",
          "title": "Merge in Orbis ultimate parent info for all matches"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Constituents",
          "content": {
            "body": "@spkim1228 I realized that this is more appropriate as a separate issue. I think there are 2 goals:\n1. To find the pre-2010 and post-2017 shapefiles for congressional districts\n2. To assure that we have all the data in the same format such that it works with your function\n\nThis link is a promising place to start: https://www2.census.gov/geo/tiger/\n\nHere is what GPT says about this:\nHere is a concise table of the Congressional District shapefile directory links on the Census TIGER site:\n\n| Year / Source                           | URL                                                                                                    |\n| --------------------------------------- | ------------------------------------------------------------------------------------------------------ |\n| **2000 Redistricting**                  | [https://www2.census.gov/geo/tiger/rd\\_2ktiger/](https://www2.census.gov/geo/tiger/rd_2ktiger/)        |\n| **108th Congress (2003 redistricting)** | [https://www2.census.gov/geo/tiger/tgrcd108/](https://www2.census.gov/geo/tiger/tgrcd108/)             |\n| **2002 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2002/CD/](https://www2.census.gov/geo/tiger/TIGER2002/CD/)     |\n| **2003 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2003/CD/](https://www2.census.gov/geo/tiger/TIGER2003/CD/)     |\n| **2007 Final Edition**                  | [https://www2.census.gov/geo/tiger/TIGER2007FE/CD/](https://www2.census.gov/geo/tiger/TIGER2007FE/CD/) |\n| **2008 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2008/CD/](https://www2.census.gov/geo/tiger/TIGER2008/CD/)     |\n| **2009 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2009/CD/](https://www2.census.gov/geo/tiger/TIGER2009/CD/)     |\n| **2010 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2010/CD/](https://www2.census.gov/geo/tiger/TIGER2010/CD/)     |\n| **2011 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2011/CD/](https://www2.census.gov/geo/tiger/TIGER2011/CD/)     |\n| **2012 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2012/CD/](https://www2.census.gov/geo/tiger/TIGER2012/CD/)     |\n| **2013 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2013/CD/](https://www2.census.gov/geo/tiger/TIGER2013/CD/)     |\n| **2014 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2014/CD/](https://www2.census.gov/geo/tiger/TIGER2014/CD/)     |\n| **2015 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2015/CD/](https://www2.census.gov/geo/tiger/TIGER2015/CD/)     |\n| **2016 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2016/CD/](https://www2.census.gov/geo/tiger/TIGER2016/CD/)     |\n| **2017 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2017/CD/](https://www2.census.gov/geo/tiger/TIGER2017/CD/)     |\n| **2018 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2018/CD/](https://www2.census.gov/geo/tiger/TIGER2018/CD/)     |\n| **2019 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2019/CD/](https://www2.census.gov/geo/tiger/TIGER2019/CD/)     |\n| **2020 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2020/CD/](https://www2.census.gov/geo/tiger/TIGER2020/CD/)     |\n| **2021 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2021/CD/](https://www2.census.gov/geo/tiger/TIGER2021/CD/)     |\n| **2022 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2022/CD/](https://www2.census.gov/geo/tiger/TIGER2022/CD/)     |\n| **2023 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2023/CD/](https://www2.census.gov/geo/tiger/TIGER2023/CD/)     |\n| **2024 Tiger/Line**                     | [https://www2.census.gov/geo/tiger/TIGER2024/CD/](https://www2.census.gov/geo/tiger/TIGER2024/CD/)     |\n\nSources:\n\u2013 Directory index for all TIGER/Line and redistricting folders ([[Census Bureau](https://www2.census.gov/geo/tiger/)][1])\n\u2013 2000 redistricting (rd\\_2ktiger) folder listing ([[Census Bureau](https://www2.census.gov/geo/tiger/rd_2ktiger/)][2])\n\u2013 108th Congress CD folder listing (tgrcd108) ([[Census Bureau](https://www2.census.gov/geo/tiger/tgrcd108/)][3])\n\n[1]: https://www2.census.gov/geo/tiger/ \"Index of /geo/tiger\"\n[2]: https://www2.census.gov/geo/tiger/rd_2ktiger/ \"Index of /geo/tiger/rd_2ktiger\"\n[3]: https://www2.census.gov/geo/tiger/tgrcd108/ \"Index of /geo/tiger/tgrcd108\"",
            "number": 93,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Find missing congressional district shapefiles",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/93"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgaTamY",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "target completion date": "2025-05-13",
          "title": "Find missing congressional district shapefiles"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Firm Level",
          "content": {
            "body": "@MarcoGrotteria can you please find the data I need to replicate the measures used in Spenkuch, Teso, and Xu? In particular, I need the columns that relate to:\n1. Cost overruns;\n2. Delays.\n\nIf we need new data, that's totally fine! I just want to use the same stuff they do for efficiency measures.",
            "number": 94,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Find contracts delays and overruns columns",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/94"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgaWKIc",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "target completion date": "2025-07-14",
          "title": "Find contracts delays and overruns columns"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "@spkim1228 the easiest way to do this is likely to create slightly cleaned files that are all in the same format and then run the function on the slightly cleaned data.",
            "number": 95,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Modify geocode function for new shapefile data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/95"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgaZe08",
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "target completion date": "2025-05-21",
          "title": "Modify geocode function for new shapefile data"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Firm Level",
          "content": {
            "body": "**Contracts analysis data requirements:** Merge contracts with\n- [x] NETS -- employment, NAICS, congressional district\n- [x] Aggregate to HQCompany\n- [x] Orbis parent country\n- [x] FARA meetings\n- [x] LDA\n\n**Notes:**\n- Orbis parent info will need to be merged with HQCompany\n- Need to think about getting ultimate parent company for LDA and merging on that\n\n### Aggregating to headquarter level\n\nThis is likely the hardest step. Ideally, we will have a dataset that is at the year-firm-congressional district level. This means that We need to merge in \"ultimate parent company\" information to each establishment. This can be done in two ways:\n1. By merging HQDUNS information for each DUNS\n2. By using the ultimate parent company information in our ultimate parents file in Orbis.\n    - Ideally, we can find all NETS companies that share either a, HQDUNS or ultimate parent in Orbis.",
            "number": 99,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Create contracts analysis data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/99"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgaop-g",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "target completion date": "2025-06-18",
          "title": "Create contracts analysis data"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Firm Level",
          "content": {
            "body": "## Issue description\n\n**Goal:** @spkim1228 please, create a NETS congressional district file using location information.\n\n**Difficulties:**\n- Establishments sometimes move location, and this will need to be accounted for. You can do this using the Move and MoveSummary files.\n- When the lat-long data is missing, please use our mapping of zipcodes to congressional districts.\n- This can be found in `datastore/raw/geography/district/orig`.\n- Note, there is not an exact mapping from zip to congressional district. They have weight that will help you split among various districts. I will instruct on how to do this. Please reach out when you get here.\n\n**Output:**\n- A pickle file with duns number of congressional district where each column represents a year from 1990-2020.",
            "number": 100,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Add congressional district to NETS location",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/100"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgapveI",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/103"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "target completion date": "2025-05-29",
          "title": "Add congressional district to NETS location"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Firm Level",
          "content": {
            "body": "",
            "number": 101,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "issue 90 redo nets gjf merge using distance instead of state",
            "type": "PullRequest",
            "url": "https://github.com/MaxMillerLab/foreign_influence/pull/101"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgap60o",
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "reviewers": [
            "MaxMillerLab"
          ],
          "status": "Done",
          "target completion date": "2025-05-23",
          "title": "issue 90 redo nets gjf merge using distance instead of state"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Firm Level",
          "content": {
            "body": "Currently, we are missing the years 1995-1997 and 2005-2007 for congressional district data. The following will be checked:\n\n- Whether the ancestry data gives adequate data on congressional district/year/population information\n- If this can be used to assist in merging the nets data to congressional districts in issue #100 ",
            "number": 102,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Check IPUMS Ancestry Data",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/102"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzga77ho",
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Check IPUMS Ancestry Data"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "category": "Legislator",
          "content": {
            "body": "The district senate dataset for FARA meetings has 10x more rows than the state senate dataset. Investigate to see how this is the case.",
            "number": 105,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Check difference between state and district senate datasets",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/105"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgbcVlw",
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Check difference between state and district senate datasets"
        },
        {
          "content": {
            "body": "Clean up the script currently called `addresses_to_coords.py` located in `source/derived/geography` so that\n\n- There is a general script in user_modules that creates coordinates from cleaned addresses \n- Each geocded dataset has its own script labeled `create_XX_lat_long.py` in its own directory.",
            "number": 108,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Split geocode addresses script",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/108"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgbjGhA",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "target completion date": "2025-06-17",
          "title": "Split geocode addresses script"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "spkim1228"
          ],
          "content": {
            "body": "The goal is the consolidate all the datasets in firm-level into a single SQL database. This includes\n- The files in `derived/firm_level`\n- Some files in `raw/firm_level/XXX/data` after also determining whether we are still using those files\n- Contracts datasets\n\nWe should have a script that can handle doing this automatically for the files that exist in the set, with each folder having its own schema in the SQL database (i.e, nets, orbis, etc.). Unfortunately, I don't think subschemas exist natively in SQL but we can get around it via creating naming conventions for schemas (i.e, something like orbis.usa_firms) or using metadata tags for the schemas. Let me know if you have a preference!",
            "number": 112,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Consolidate firm-level SQL database",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/112"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgbu4FA",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/123"
          ],
          "priority": "Low",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Consolidate firm-level SQL database"
        },
        {
          "assignees": [
            "MaxMillerLab",
            "spkim1228"
          ],
          "content": {
            "body": "Currently, there are a number of no longer used scripts and datasets that are floating around in the repository. We are looking to identify them and move them to archival folders within each folder.",
            "number": 113,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Clean up repository of unused scripts and datasets",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/113"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgbu4SY",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/120"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Clean up repository of unused scripts and datasets"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "content": {
            "body": "# Analyze which industries lobby most heavily in each congressional district\n\n## Objective\nUnderstand which industries lobby most heavily in each congressional district using two complementary approaches that leverage the Orbis-NETS merged data.\n\n## Background\nThis analysis will combine firm-level data from multiple sources to map lobbying activity to congressional districts. We have already completed the Orbis-NETS merge and have congressional district mappings for establishments.\n\n## Required Data Sources\n\n### Firm-level data\n- **Orbis ownership data**: \n  - `datastore/derived/firm_level/merging/orbis_nets/duns_parent_country_map.dta` (Duns numbers and their parent country from Orbis)\n\n- **NETS establishment data**:\n  - `datastore/raw/firm_level/nets/data/parquet/` (all NETS info)\n  - `datastore/raw/firm_level/nets/data/NETS2020_CongressionalDistricts.pkl`\n  \n- **Orbis-NETS merge**:\n  - `datastore/derived/firm_level/merging/orbis_nets/` (various match files)\n\n### Lobbying data\n- **LDA data**:\n  - `datastore/raw/lda/orig/` (client and report level datasets)\n  \n- **FARA data**:\n  - `datastore/derived/fara/meetings/legislator` (meetings data for legislators)\n\n## Implementation Steps\n\n### Phase 1: Match LDA to Orbis and then NETS\n1. Match LDA registrants to BvD IDs\n2. Link BvD IDs to NETS establishments through Orbis-NETS merge\n3. Merge employment information from NETS\n4. Aggregate lobbying amounts by:\n   - Industry (using NAICS/SIC codes)\n   - Congressional district\n   - Year\n   - Employment\n   - Number of establishments\n\n### Phase 2: Find foreign, connected establishments\n1. Use `datastore/derived/firm_level/merging/orbis_nets/duns_parent_country_map.dta` to identify all foreign establishments\n2. Use `datastore/derived/fara/meetings/district/senate/district_senate_annual.dta` and `datastore/derived/fara/meetings/district/house/district_house_annual.dta` to locate connections\n3. Merge in NETS industry, congressional district, and employment information\n4. Aggregate foreign-connected firm presence by:\n   - Parent country\n   - Industry\n   - Congressional district\n5. Create influence metrics (e.g., employment, number of establishments)\n\n### Phase 3: Integration and Output\n1. Combine LDA and FARA analyses\n2. Create industry-district-year-level data showing:\n   - Fraction of lobbying establishments\n   - Fraction of employment from lobbying firms\n",
            "number": 116,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Analyze which industries lobby most heavily in each congressional district",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/116"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgcSX4s",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/124"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Analyze which industries lobby most heavily in each congressional district"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "There are 2 steps for reformatting the NETS lat-long dataset\n\n\n- [x] Splitting the dataset into three different datasets based on the origin of the dataset (HQCompany, MoveOrig, and MoveDest)\n- [x] Formatting it into a wide dataset with `movesummary.pq` for lat-long over years",
            "number": 117,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Reformat NETS lat long dataset",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/117"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgcXigw",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/131"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "start date": "2025-07-08",
          "status": "Done",
          "target completion date": "2025-07-30",
          "title": "Reformat NETS lat long dataset"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Reformat the NETS congressional district dataset to handle the multiple congressional districts for each year and handle the missing years per congressional districts.",
            "number": 118,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Reformat NETS congressional district dataset",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/118"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgcY_x0",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/121"
          ],
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "title": "Reformat NETS congressional district dataset"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "Using the `ecta200569-sup-0002-dataandprograms.zip` file located in `datastore/raw/firm_level/contracts/orig`, merge it into the contracts data. We must examine the contents of the zipfile to determine how to do so, whether we should append datasets or merge them together.",
            "number": 122,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Calculate Contracts Costs, Overruns, and Delays",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/122"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgcb8dE",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/129"
          ],
          "priority": "High",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "target completion date": "2025-07-22",
          "title": "Calculate Contracts Costs, Overruns, and Delays"
        },
        {
          "assignees": [
            "MaxMillerLab"
          ],
          "category": "Country Level",
          "content": {
            "body": "@MaxMillerLab  can you please check whether tariffs are related in the cross-sedction with meetings?  We would like to extend the unidentified results in Section 2.5 \\subsection{Motivating evidence} to tariffs.",
            "number": 125,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Motivating evidence tariffs",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/125"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgcmkVo",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "start date": "2025-07-14",
          "status": "Ready",
          "target completion date": "2025-07-29",
          "title": "Motivating evidence tariffs"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "For the following scripts, update to use DuckDB parquet file functionality and rerun. Additionally, verify that the outputs of the scripts match the original.\n\n- [x] Subsidy address matching\n- [x] Orbis address matching",
            "number": 126,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Use DuckDB for Location Matching",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/126"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgcnsJo",
          "linked pull requests": [
            "https://github.com/MaxMillerLab/foreign_influence/pull/128"
          ],
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "target completion date": "2025-07-20",
          "title": "Use DuckDB for Location Matching"
        },
        {
          "assignees": [
            "spkim1228"
          ],
          "content": {
            "body": "There are a lot of duplicate files floating around in the datastore. Currently, there are a lot of .csv/.dta/.pkl files for files we have switched to .pq for. I am planning on deleting these duplicates. I don't anticipate writing a script for this, instead just deleting the files when I come across them.",
            "number": 127,
            "repository": "MaxMillerLab/foreign_influence",
            "title": "Clean datastore",
            "type": "Issue",
            "url": "https://github.com/MaxMillerLab/foreign_influence/issues/127"
          },
          "id": "PVTI_lAHOAkGERM4AjpRhzgcnslo",
          "priority": "Medium",
          "repository": "https://github.com/MaxMillerLab/foreign_influence",
          "status": "Done",
          "target completion date": "2025-07-18",
          "title": "Clean datastore"
        }
      ]
    }
  }
}